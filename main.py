import argparse
import base64
import json
import logging
import os
import platform
import re
import shutil
import time
from io import BytesIO
from itertools import combinations
from typing import List, Tuple

import cohere
import gradio as gr
import oci
import oracledb
import pandas as pd
import requests
from PIL import Image
from dotenv import load_dotenv, find_dotenv, set_key, get_key
from gradio.themes import GoogleFont
from langchain_community.document_loaders import TextLoader
from langchain_community.embeddings import OCIGenAIEmbeddings
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI, AzureChatOpenAI
from langfuse.callback import CallbackHandler
from oracledb import DatabaseError
from unstructured.partition.auto import partition

from markitdown import MarkItDown
# from langchain_community.chat_models import ChatOCIGenAI
from my_langchain_community.chat_models import ChatOCIGenAI
from utils.chunk_util import RecursiveCharacterTextSplitter
from utils.common_util import get_dict_value
from utils.css_gradio import custom_css
from utils.generator_util import generate_unique_id
from utils.prompts import (
    get_sub_query_prompt, get_rag_fusion_prompt, get_hyde_prompt, get_step_back_prompt,
    get_langgpt_rag_prompt, get_llm_evaluation_system_message, get_chat_system_message,
    get_markitdown_llm_prompt, get_query_generation_prompt, update_langgpt_rag_prompt,
    get_image_qa_prompt, update_image_qa_prompt
)

# read local .env file
load_dotenv(find_dotenv())

DEFAULT_COLLECTION_NAME = os.environ["DEFAULT_COLLECTION_NAME"]

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(__name__)


def compress_image_for_display(image_url, quality=85, max_width=800, max_height=1200):
    """
    ç”»åƒURLã‚’åœ§ç¸®ã—ã¦è¡¨ç¤ºç”¨ã®æ–°ã—ã„URLã‚’ç”Ÿæˆã™ã‚‹

    Args:
        image_url: å…ƒã®ç”»åƒURLï¼ˆdata:image/...;base64,... å½¢å¼ï¼‰
        quality: JPEGåœ§ç¸®å“è³ª (1-100)
        max_width: æœ€å¤§å¹…
        max_height: æœ€å¤§é«˜ã•

    Returns:
        str: åœ§ç¸®ã•ã‚ŒãŸç”»åƒã®data URL
    """
    output_buffer = None
    try:
        # data URLã‹ã‚‰base64ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        if not image_url.startswith('data:image/'):
            return image_url

        # base64ãƒ‡ãƒ¼ã‚¿éƒ¨åˆ†ã‚’å–å¾—
        header, base64_data = image_url.split(',', 1)

        # base64ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        image_data = base64.b64decode(base64_data)

        # PILã§ç”»åƒã‚’é–‹ã
        with Image.open(BytesIO(image_data)) as img:
            # RGBãƒ¢ãƒ¼ãƒ‰ã«å¤‰æ›ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
            if img.mode in ('RGBA', 'LA', 'P'):
                img = img.convert('RGB')

            # ç”»åƒã‚µã‚¤ã‚ºã‚’èª¿æ•´ï¼ˆã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã‚’ä¿æŒï¼‰
            original_width, original_height = img.size

            # ç¸®å°ãŒå¿…è¦ã‹ãƒã‚§ãƒƒã‚¯
            if original_width > max_width or original_height > max_height:
                # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã‚’ä¿æŒã—ã¦ç¸®å°
                ratio = min(max_width / original_width, max_height / original_height)
                new_width = int(original_width * ratio)
                new_height = int(original_height * ratio)
                img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)
                print(f"ç”»åƒã‚µã‚¤ã‚ºã‚’åœ§ç¸®: {original_width}x{original_height} -> {new_width}x{new_height}")

            # åœ§ç¸®ã•ã‚ŒãŸç”»åƒã‚’ãƒã‚¤ãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã«ä¿å­˜
            output_buffer = BytesIO()
            img.save(output_buffer, format='JPEG', quality=quality, optimize=True)
            compressed_data = output_buffer.getvalue()

            # base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            compressed_base64 = base64.b64encode(compressed_data).decode('utf-8')

            # æ–°ã—ã„data URLã‚’ç”Ÿæˆ
            compressed_url = f"data:image/jpeg;base64,{compressed_base64}"

            # åœ§ç¸®ç‡ã‚’è¨ˆç®—ã—ã¦ãƒ­ã‚°å‡ºåŠ›
            original_size = len(base64_data)
            compressed_size = len(compressed_base64)
            compression_ratio = (1 - compressed_size / original_size) * 100
            print(f"ç”»åƒåœ§ç¸®å®Œäº†: {original_size} -> {compressed_size} bytes ({compression_ratio:.1f}% å‰Šæ¸›)")

            return compressed_url

    except Exception as e:
        print(f"ç”»åƒåœ§ç¸®ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return image_url  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒã®ç”»åƒURLã‚’è¿”ã™
    finally:
        # BytesIOãƒãƒƒãƒ•ã‚¡ã‚’æ˜ç¤ºçš„ã«é–‰ã˜ã‚‹
        if output_buffer is not None:
            try:
                output_buffer.close()
            except Exception:
                pass  # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–


def cleanup_llm_client(llm_client):
    """
    LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ãƒªã‚½ãƒ¼ã‚¹ã‚’å®‰å…¨ã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã™ã‚‹ï¼ˆåŒæœŸç‰ˆï¼‰

    Args:
        llm_client: ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã™ã‚‹LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
    """
    if llm_client is None:
        return

    try:
        # OpenAIç³»ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®å ´åˆ
        if hasattr(llm_client, 'client'):
            if hasattr(llm_client.client, 'close'):
                # åŒæœŸçš„ã«ã‚¯ãƒ­ãƒ¼ã‚º
                if hasattr(llm_client.client.close, '__call__'):
                    try:
                        llm_client.client.close()
                    except Exception as e:
                        print(f"OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åŒæœŸã‚¯ãƒ­ãƒ¼ã‚ºä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

        # _clientã‚¢ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ãƒˆã‚’æŒã¤å ´åˆ
        elif hasattr(llm_client, '_client'):
            if hasattr(llm_client._client, 'close'):
                try:
                    llm_client._client.close()
                except Exception as e:
                    print(f"_client ã‚¯ãƒ­ãƒ¼ã‚ºä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

            # OCI GenAIç³»ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚»ãƒƒã‚·ãƒ§ãƒ³å‡¦ç†
            if hasattr(llm_client._client, '_session'):
                if hasattr(llm_client._client._session, 'close'):
                    try:
                        llm_client._client._session.close()
                    except Exception as e:
                        print(f"OCI ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¯ãƒ­ãƒ¼ã‚ºä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

        print(f"LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ {type(llm_client).__name__} ã®ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸ")

    except Exception as cleanup_error:
        print(f"LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­ã«äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {cleanup_error}")


async def cleanup_llm_client_async(llm_client):
    """
    LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ãƒªã‚½ãƒ¼ã‚¹ã‚’å®‰å…¨ã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã™ã‚‹ï¼ˆéåŒæœŸç‰ˆï¼‰

    Args:
        llm_client: ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã™ã‚‹LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
    """
    if llm_client is None:
        return

    try:
        # OpenAIç³»ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®å ´åˆï¼ˆéåŒæœŸã‚¯ãƒ­ãƒ¼ã‚ºï¼‰
        if hasattr(llm_client, 'client'):
            # HTTPXã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®å ´åˆ
            if hasattr(llm_client.client, 'aclose'):
                try:
                    await llm_client.client.aclose()
                    print(f"OpenAI HTTPXã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®éåŒæœŸã‚¯ãƒ­ãƒ¼ã‚ºãŒå®Œäº†ã—ã¾ã—ãŸ")
                except Exception as e:
                    # OpenAI API type ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
                    if "Ambiguous use of module client" not in str(e):
                        print(f"OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®éåŒæœŸã‚¯ãƒ­ãƒ¼ã‚ºä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            elif hasattr(llm_client.client, 'close'):
                try:
                    llm_client.client.close()
                    print(f"OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åŒæœŸã‚¯ãƒ­ãƒ¼ã‚ºãŒå®Œäº†ã—ã¾ã—ãŸ")
                except Exception as e:
                    # OpenAI API type ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
                    if "Ambiguous use of module client" not in str(e):
                        print(f"OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åŒæœŸã‚¯ãƒ­ãƒ¼ã‚ºä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

            # è¿½åŠ ã®HTTPæ¥ç¶šãƒ—ãƒ¼ãƒ«æ¸…ç†
            if hasattr(llm_client.client, '_client'):
                if hasattr(llm_client.client._client, 'aclose'):
                    try:
                        await llm_client.client._client.aclose()
                        print(f"OpenAI å†…éƒ¨HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®éåŒæœŸã‚¯ãƒ­ãƒ¼ã‚ºãŒå®Œäº†ã—ã¾ã—ãŸ")
                    except Exception as e:
                        if "Ambiguous use of module client" not in str(e):
                            print(f"OpenAI å†…éƒ¨HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¯ãƒ­ãƒ¼ã‚ºä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

        # _clientã‚¢ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ãƒˆã‚’æŒã¤å ´åˆï¼ˆOCI GenAIç­‰ï¼‰
        elif hasattr(llm_client, '_client'):
            if hasattr(llm_client._client, 'aclose'):
                try:
                    await llm_client._client.aclose()
                    print(f"OCI _client ã®éåŒæœŸã‚¯ãƒ­ãƒ¼ã‚ºãŒå®Œäº†ã—ã¾ã—ãŸ")
                except Exception as e:
                    print(f"_client éåŒæœŸã‚¯ãƒ­ãƒ¼ã‚ºä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            elif hasattr(llm_client._client, 'close'):
                try:
                    llm_client._client.close()
                    print(f"OCI _client ã®åŒæœŸã‚¯ãƒ­ãƒ¼ã‚ºãŒå®Œäº†ã—ã¾ã—ãŸ")
                except Exception as e:
                    print(f"_client ã‚¯ãƒ­ãƒ¼ã‚ºä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

            # OCI GenAIç³»ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚»ãƒƒã‚·ãƒ§ãƒ³å‡¦ç†
            if hasattr(llm_client._client, '_session'):
                if hasattr(llm_client._client._session, 'aclose'):
                    try:
                        await llm_client._client._session.aclose()
                        print(f"OCI ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®éåŒæœŸã‚¯ãƒ­ãƒ¼ã‚ºãŒå®Œäº†ã—ã¾ã—ãŸ")
                    except Exception as e:
                        print(f"OCI ã‚»ãƒƒã‚·ãƒ§ãƒ³éåŒæœŸã‚¯ãƒ­ãƒ¼ã‚ºä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                elif hasattr(llm_client._client._session, 'close'):
                    try:
                        llm_client._client._session.close()
                        print(f"OCI ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®åŒæœŸã‚¯ãƒ­ãƒ¼ã‚ºãŒå®Œäº†ã—ã¾ã—ãŸ")
                    except Exception as e:
                        print(f"OCI ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¯ãƒ­ãƒ¼ã‚ºä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

        # è¿½åŠ ã®ãƒªã‚½ãƒ¼ã‚¹æ¸…ç†ï¼šHTTPã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ¼ãƒ«ã®å¼·åˆ¶ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆè»½é‡ç‰ˆï¼‰
        try:
            await force_cleanup_http_connections(llm_client)
        except Exception as force_cleanup_error:
            # å¼·åˆ¶ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã®ã‚¨ãƒ©ãƒ¼ã¯è©³ç´°ã‚’è¡¨ç¤ºã—ãªã„
            if "Ambiguous use of module client" not in str(force_cleanup_error):
                print(f"HTTPæ¥ç¶šãƒ—ãƒ¼ãƒ«å¼·åˆ¶ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {force_cleanup_error}")

        print(f"LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ {type(llm_client).__name__} ã®éåŒæœŸãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸ")

    except Exception as cleanup_error:
        # OpenAI API type ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
        if "Ambiguous use of module client" not in str(cleanup_error):
            print(f"LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®éåŒæœŸã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­ã«äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {cleanup_error}")


async def force_cleanup_http_connections(llm_client):
    """
    HTTPã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ¼ãƒ«ã‚’å¼·åˆ¶çš„ã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã™ã‚‹

    Args:
        llm_client: LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
    """
    try:
        # httpxãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ¼ãƒ«ã‚’æ¢ã—ã¦ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        import httpx
        import gc

        # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã—ã¦ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’åé›†
        gc.collect()

        # httpxã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’æ¢ã—ã¦ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆæœ€å¤§3ã¤ã¾ã§ï¼‰
        cleaned_count = 0
        for obj in gc.get_objects():
            if cleaned_count >= 3:  # å‡¦ç†æ•°ã‚’åˆ¶é™
                break

            if isinstance(obj, httpx.AsyncClient):
                try:
                    if not obj.is_closed:
                        await obj.aclose()
                        cleaned_count += 1
                        print(f"æœªé–‰é–ã®HTTPXã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ #{cleaned_count} ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸ")
                except Exception as e:
                    # OpenAI API type ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
                    if "Ambiguous use of module client" not in str(e):
                        print(f"HTTPXã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            elif isinstance(obj, httpx.Client):
                try:
                    if not obj.is_closed:
                        obj.close()
                        cleaned_count += 1
                        print(f"æœªé–‰é–ã®åŒæœŸHTTPXã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ #{cleaned_count} ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸ")
                except Exception as e:
                    # OpenAI API type ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
                    if "Ambiguous use of module client" not in str(e):
                        print(f"åŒæœŸHTTPXã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

    except ImportError:
        # httpxãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„å ´åˆã¯ç„¡è¦–
        pass
    except Exception as e:
        # OpenAI API type ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
        if "Ambiguous use of module client" not in str(e):
            print(f"HTTPæ¥ç¶šãƒ—ãƒ¼ãƒ«å¼·åˆ¶ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")


async def cleanup_all_http_connections():
    """
    ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®HTTPæ¥ç¶šãƒ—ãƒ¼ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã™ã‚‹
    """
    try:
        import httpx
        import aiohttp
        import gc

        print("ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®HTTPæ¥ç¶šãƒ—ãƒ¼ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’é–‹å§‹...")

        # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ
        gc.collect()

        cleaned_count = 0

        # ã™ã¹ã¦ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’æ¢ã™
        for obj in gc.get_objects():
            try:
                # httpxã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
                if isinstance(obj, httpx.AsyncClient):
                    if not obj.is_closed:
                        await obj.aclose()
                        cleaned_count += 1
                        print(f"HTTPXã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ #{cleaned_count} ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸ")
                elif isinstance(obj, httpx.Client):
                    if not obj.is_closed:
                        obj.close()
                        cleaned_count += 1
                        print(f"åŒæœŸHTTPXã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ #{cleaned_count} ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸ")

                # aiohttpã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
                elif hasattr(obj, '__class__') and 'aiohttp' in str(obj.__class__):
                    if hasattr(obj, 'close') and not getattr(obj, 'closed', True):
                        try:
                            await obj.close()
                            cleaned_count += 1
                            print(f"aiohttpã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ #{cleaned_count} ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸ")
                        except Exception:
                            pass

            except Exception as e:
                # å€‹åˆ¥ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
                pass

        # æœ€çµ‚ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
        gc.collect()

        print(f"HTTPæ¥ç¶šãƒ—ãƒ¼ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†: {cleaned_count}å€‹ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å‡¦ç†ã—ã¾ã—ãŸ")

    except ImportError:
        print("HTTPæ¥ç¶šãƒ—ãƒ¼ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—: å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    except Exception as e:
        print(f"ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®HTTPæ¥ç¶šãƒ—ãƒ¼ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")


async def lightweight_cleanup():
    """
    è»½é‡ãªãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆç”»åƒå‡¦ç†å¾Œã«ä½¿ç”¨ï¼‰
    """
    try:
        import gc

        # è»½é‡ãªã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
        collected = gc.collect()
        if collected > 0:
            print(f"è»½é‡ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—: {collected} ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å›åã—ã¾ã—ãŸ")

        # åŸºæœ¬çš„ãªHTTPæ¥ç¶šãƒã‚§ãƒƒã‚¯ï¼ˆé‡ã„å‡¦ç†ã¯é¿ã‘ã‚‹ï¼‰
        try:
            import httpx
            # æ˜ã‚‰ã‹ã«é–‰ã˜ã‚‰ã‚Œã¦ã„ãªã„ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã¿ãƒã‚§ãƒƒã‚¯
            cleaned_count = 0
            for obj in gc.get_objects():
                if isinstance(obj, httpx.AsyncClient) and hasattr(obj, 'is_closed'):
                    if not obj.is_closed:
                        try:
                            await obj.aclose()
                            cleaned_count += 1
                            print(f"æœªé–‰é–ã®HTTPXã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ #{cleaned_count} ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸ")
                        except Exception as close_error:
                            # OpenAI API type ã‚¨ãƒ©ãƒ¼ãªã©ã¯ç„¡è¦–
                            if "Ambiguous use of module client" not in str(close_error):
                                print(f"HTTPXã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¯ãƒ­ãƒ¼ã‚ºä¸­ã«ã‚¨ãƒ©ãƒ¼: {close_error}")
                        if cleaned_count >= 3:  # æœ€å¤§3ã¤ã¾ã§å‡¦ç†
                            break
        except ImportError:
            pass
        except Exception as http_error:
            # HTTPé–¢é€£ã®ã‚¨ãƒ©ãƒ¼ã¯è©³ç´°ã‚’è¡¨ç¤ºã—ãªã„
            print(f"HTTPæ¥ç¶šãƒã‚§ãƒƒã‚¯ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼ˆç„¡è¦–ã•ã‚Œã¾ã™ï¼‰")

    except Exception as e:
        print(f"è»½é‡ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")


def enable_resource_warnings():
    """
    ãƒªã‚½ãƒ¼ã‚¹è­¦å‘Šã‚’æœ‰åŠ¹ã«ã—ã¦è©³ç´°ãªæƒ…å ±ã‚’å–å¾—ã™ã‚‹
    """
    import warnings
    import tracemalloc

    # ResourceWarningã‚’æœ‰åŠ¹ã«ã™ã‚‹
    warnings.filterwarnings("always", category=ResourceWarning)

    # tracemallocã‚’æœ‰åŠ¹ã«ã—ã¦ãƒ¡ãƒ¢ãƒªè¿½è·¡ã‚’é–‹å§‹
    if not tracemalloc.is_tracing():
        tracemalloc.start()
        print("ãƒªã‚½ãƒ¼ã‚¹è¿½è·¡ãŒæœ‰åŠ¹ã«ãªã‚Šã¾ã—ãŸ")

    # ã‚«ã‚¹ã‚¿ãƒ warningå‡¦ç†ã‚’è¨­å®š
    def custom_warning_handler(message, category, filename, lineno, file=None, line=None):
        if category == ResourceWarning:
            print(f"ğŸš¨ ãƒªã‚½ãƒ¼ã‚¹è­¦å‘Š: {message}")
            print(f"   ãƒ•ã‚¡ã‚¤ãƒ«: {filename}:{lineno}")
            if tracemalloc.is_tracing():
                # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®çµ±è¨ˆã‚’è¡¨ç¤º
                current, peak = tracemalloc.get_traced_memory()
                print(f"   ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: ç¾åœ¨={current / 1024 / 1024:.1f}MB, ãƒ”ãƒ¼ã‚¯={peak / 1024 / 1024:.1f}MB")

    warnings.showwarning = custom_warning_handler


async def final_resource_cleanup():
    """
    ãƒ—ãƒ­ã‚°ãƒ©ãƒ çµ‚äº†æ™‚ã®æœ€çµ‚ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    """
    print("\n=== æœ€çµ‚ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’é–‹å§‹ ===")

    try:
        # HTTPæ¥ç¶šãƒ—ãƒ¼ãƒ«ã®å…¨ä½“ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        await cleanup_all_http_connections()

        # è¿½åŠ ã®ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        import gc
        import asyncio

        # ç¾åœ¨ã®ã‚¿ã‚¹ã‚¯ã‚’å–å¾—ï¼ˆè‡ªåˆ†è‡ªèº«ã¯é™¤å¤–ï¼‰
        current_task = asyncio.current_task()
        tasks = [task for task in asyncio.all_tasks()
                 if not task.done() and task != current_task]

        if tasks:
            print(f"å®Ÿè¡Œä¸­ã®ã‚¿ã‚¹ã‚¯ {len(tasks)} å€‹ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã™...")
            # ã‚¿ã‚¹ã‚¯ã‚’å®‰å…¨ã«ã‚­ãƒ£ãƒ³ã‚»ãƒ«
            for task in tasks:
                try:
                    if not task.done() and not task.cancelled():
                        task.cancel()
                except Exception as cancel_error:
                    print(f"ã‚¿ã‚¹ã‚¯ã‚­ãƒ£ãƒ³ã‚»ãƒ«ä¸­ã«ã‚¨ãƒ©ãƒ¼: {cancel_error}")

            # ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ã®å®Œäº†ã‚’å¾…ã¤ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
            try:
                await asyncio.wait_for(
                    asyncio.gather(*tasks, return_exceptions=True),
                    timeout=5.0
                )
                print("ã™ã¹ã¦ã®ã‚¿ã‚¹ã‚¯ãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")
            except asyncio.TimeoutError:
                print("ã‚¿ã‚¹ã‚¯ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸï¼ˆä¸€éƒ¨ã®ã‚¿ã‚¹ã‚¯ãŒæ®‹ã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼‰")
            except Exception as gather_error:
                print(f"ã‚¿ã‚¹ã‚¯å¾…æ©Ÿä¸­ã«ã‚¨ãƒ©ãƒ¼: {gather_error}")

        # å¼·åˆ¶ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
        for i in range(3):
            collected = gc.collect()
            if collected > 0:
                print(f"ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ {i + 1}: {collected} ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å›åã—ã¾ã—ãŸ")

        # ãƒ¡ãƒ¢ãƒªçµ±è¨ˆã‚’è¡¨ç¤º
        import tracemalloc
        if tracemalloc.is_tracing():
            current, peak = tracemalloc.get_traced_memory()
            print(f"æœ€çµ‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: ç¾åœ¨={current / 1024 / 1024:.1f}MB, ãƒ”ãƒ¼ã‚¯={peak / 1024 / 1024:.1f}MB")
            tracemalloc.stop()

        print("=== æœ€çµ‚ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº† ===\n")

    except Exception as e:
        print(f"æœ€çµ‚ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()


def check_langfuse_availability():
    """
    Langfuse ã‚µãƒ¼ãƒ“ã‚¹ã®å¯ç”¨æ€§ã‚’äº‹å‰ã«ç¢ºèªã™ã‚‹

    Returns:
        bool: Langfuse ã‚µãƒ¼ãƒ“ã‚¹ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯ Trueã€ãã†ã§ãªã‘ã‚Œã° False
    """
    try:
        # ç’°å¢ƒå¤‰æ•°ã®å­˜åœ¨ç¢ºèª
        required_env_vars = ["LANGFUSE_SECRET_KEY", "LANGFUSE_PUBLIC_KEY", "LANGFUSE_HOST"]
        for var in required_env_vars:
            if not os.environ.get(var):
                logger.warning(f"Langfuse ç’°å¢ƒå¤‰æ•° {var} ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return False

        # Langfuse ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ
        import requests
        from urllib.parse import urljoin

        host = os.environ["LANGFUSE_HOST"].rstrip('/')

        # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’è©¦è¡Œ
        health_url = urljoin(host, "/api/public/health")

        try:
            response = requests.get(health_url, timeout=5)
            if response.status_code == 200:
                logger.info("Langfuse ã‚µãƒ¼ãƒ“ã‚¹ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
                return True
            else:
                logger.warning(f"Langfuse ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯å¤±æ•—: HTTP {response.status_code}")
                return False
        except requests.exceptions.RequestException as e:
            logger.warning(f"Langfuse ã‚µãƒ¼ãƒ“ã‚¹ã¸ã®æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return False

    except Exception as e:
        logger.warning(f"Langfuse å¯ç”¨æ€§ãƒã‚§ãƒƒã‚¯ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return False


def create_safe_langfuse_handler():
    """
    å®‰å…¨ãªlangfuse handlerã‚’ä½œæˆã™ã‚‹
    ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚streamå‡¦ç†ã‚’ä¸­æ–­ã—ãªã„ã‚ˆã†ã«ã™ã‚‹

    Returns:
        CallbackHandler or None: æ­£å¸¸ã«ä½œæˆã§ããŸå ´åˆã¯CallbackHandlerã€ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯None
    """
    try:
        # äº‹å‰ã«Langfuseã‚µãƒ¼ãƒ“ã‚¹ã®å¯ç”¨æ€§ã‚’ãƒã‚§ãƒƒã‚¯
        if not check_langfuse_availability():
            logger.warning("Langfuse ã‚µãƒ¼ãƒ“ã‚¹ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€callback ã‚’ç„¡åŠ¹ã«ã—ã¾ã™")
            return None

        return CallbackHandler(
            secret_key=os.environ["LANGFUSE_SECRET_KEY"],
            public_key=os.environ["LANGFUSE_PUBLIC_KEY"],
            host=os.environ["LANGFUSE_HOST"],
        )
    except Exception as e:
        logger.warning(f"Langfuse handlerã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None


def get_safe_stream_config(model_name=None):
    """
    å®‰å…¨ãªstreamè¨­å®šã‚’å–å¾—ã™ã‚‹
    langfuse handlerãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ç©ºã®è¨­å®šã‚’è¿”ã™

    Args:
        model_name: ãƒ¢ãƒ‡ãƒ«åï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç”¨ï¼‰

    Returns:
        dict: streamè¨­å®š
    """
    try:
        langfuse_handler = create_safe_langfuse_handler()
        if langfuse_handler is None:
            logger.info(f"Langfuse ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€{model_name} ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã¯ callback ãªã—ã§å®Ÿè¡Œã•ã‚Œã¾ã™")
            return {}

        config = {"callbacks": [langfuse_handler]}
        if model_name:
            config["metadata"] = {"ls_model_name": model_name}
        logger.info(f"Langfuse callback ãŒæœ‰åŠ¹ã«ãªã‚Šã¾ã—ãŸ: {model_name}")
        return config
    except Exception as e:
        logger.warning(f"Streamè¨­å®šã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return {}


if platform.system() == 'Linux':
    oracledb.init_oracle_client(lib_dir=os.environ["ORACLE_CLIENT_LIB_DIR"])

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ—ãƒ¼ãƒ«ã‚’åˆæœŸåŒ–ï¼ˆãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ã‚’é¿ã‘ã‚‹ãŸã‚æ¥ç¶šæ•°ã‚’å¢—åŠ ï¼‰
pool = oracledb.create_pool(
    dsn=os.environ["ORACLE_23AI_CONNECTION_STRING"],
    min=5,
    max=20,
    increment=2,
    timeout=30,  # æ¥ç¶šã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ30ç§’
    getmode=oracledb.POOL_GETMODE_WAIT  # åˆ©ç”¨å¯èƒ½ãªæ¥ç¶šã‚’å¾…æ©Ÿ
)


def do_auth(username, password):
    dsn = os.environ["ORACLE_23AI_CONNECTION_STRING"]
    pattern = r"^([^/]+)/([^@]+)@"
    match = re.match(pattern, dsn)

    if match:
        if username.lower() == match.group(1).lower() and password == match.group(2):
            return True
    return False


def get_region():
    oci_config_path = find_dotenv("/root/.oci/config")
    region = get_key(oci_config_path, "region")
    return region


def check_database_pool_health():
    """
    ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ—ãƒ¼ãƒ«ã®å¥åº·çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹

    Returns:
        bool: ãƒ—ãƒ¼ãƒ«ãŒæ­£å¸¸ãªå ´åˆTrueã€å•é¡ŒãŒã‚ã‚‹å ´åˆFalse
    """
    try:
        with pool.acquire() as conn:
            with conn.cursor() as cursor:
                cursor.execute("SELECT 1 FROM DUAL")
                result = cursor.fetchone()
                return result is not None
    except Exception as e:
        logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ—ãƒ¼ãƒ«ã®å¥åº·ãƒã‚§ãƒƒã‚¯ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return False


def generate_embedding_response(inputs: List[str]):
    config = oci.config.from_file('/root/.oci/config', "DEFAULT")
    region = get_region()
    generative_ai_inference_client = oci.generative_ai_inference.GenerativeAiInferenceClient(
        config=config,
        service_endpoint=f"https://inference.generativeai.{region}.oci.oraclecloud.com",
        retry_strategy=oci.retry.NoneRetryStrategy(),
        timeout=(10, 240))
    batch_size = 96
    all_embeddings = []

    for i in range(0, len(inputs), batch_size):
        batch = inputs[i:i + batch_size]

        embed_text_detail = oci.generative_ai_inference.models.EmbedTextDetails()
        embed_text_detail.serving_mode = oci.generative_ai_inference.models.OnDemandServingMode(
            model_id=os.environ["OCI_COHERE_EMBED_MODEL"]
        )
        embed_text_detail.inputs = batch
        embed_text_detail.truncate = "NONE"
        embed_text_detail.compartment_id = os.environ["OCI_COMPARTMENT_OCID"]

        max_retries = 3
        retry_count = 0
        while retry_count < max_retries:
            try:
                embed_text_response = generative_ai_inference_client.embed_text(embed_text_detail)
                print(f"Processed batch {i // batch_size + 1} of {(len(inputs) - 1) // batch_size + 1}")
                all_embeddings.extend(embed_text_response.data.embeddings)
                break
            except Exception as e:
                print(f"Exception: {e}")
                retry_count += 1
                print(f"Error embedding text: {e}. Retrying ({retry_count}/{max_retries})...")
                time.sleep(10 * retry_count)
                if retry_count == max_retries:
                    gr.Warning("ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãã—ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
                    all_embeddings = []
                    return all_embeddings

        time.sleep(1)

    return all_embeddings


def rerank_documents_response(input_text, inputs: List[str], rerank_model):
    all_document_ranks = []
    batch_size = 200

    if rerank_model in ["cohere/rerank-multilingual-v3.1", "cohere/rerank-english-v3.1"]:
        rerank_model = rerank_model.replace("/", ".")
        config = oci.config.from_file('/root/.oci/config', "DEFAULT")
        region = get_region()
        rerank_generative_ai_inference_client = oci.generative_ai_inference.GenerativeAiInferenceClient(
            config=config,
            service_endpoint=f"https://inference.generativeai.{region}.oci.oraclecloud.com",
            retry_strategy=oci.retry.NoneRetryStrategy(),
            timeout=(10, 240))

        for i in range(0, len(inputs), batch_size):
            batch = inputs[i:i + batch_size]

            rerank_text_detail = oci.generative_ai_inference.models.RerankTextDetails()
            rerank_text_detail.input = input_text
            rerank_text_detail.documents = batch
            rerank_text_detail.serving_mode = oci.generative_ai_inference.models.OnDemandServingMode(
                serving_type="ON_DEMAND",
                model_id=rerank_model
            )
            rerank_text_detail.compartment_id = os.environ["OCI_COMPARTMENT_OCID"]
            rerank_response = rerank_generative_ai_inference_client.rerank_text(rerank_text_detail)
            print(f"Processed batch {i // batch_size + 1} of {(len(inputs) - 1) // batch_size + 1}")
            adjusted_results = []
            for rank in rerank_response.data.document_ranks:
                adjusted_result = {
                    "document": rank.document,
                    "index": i + rank.index,
                    "relevance_score": rank.relevance_score
                }
                adjusted_results.append(adjusted_result)
            all_document_ranks.extend(adjusted_results)
    else:
        print(f"{os.environ['COHERE_API_KEY']=}")
        cohere_reranker = rerank_model.split('/')[1]
        cohere_client = cohere.Client(api_key=os.environ["COHERE_API_KEY"])
        for i in range(0, len(inputs), batch_size):
            batch = inputs[i:i + batch_size]

            rerank_response = cohere_client.rerank(
                query=input_text,
                documents=batch,
                top_n=len(batch),
                model=cohere_reranker
            )
            print(f"{rerank_response=}")
            adjusted_results = []
            for rank in rerank_response.results:
                adjusted_result = {
                    "document": rank.document,
                    "index": i + rank.index,
                    "relevance_score": rank.relevance_score
                }
                adjusted_results.append(adjusted_result)
            all_document_ranks.extend(adjusted_results)

    return all_document_ranks


def get_doc_list() -> List[Tuple[str, str]]:
    with pool.acquire() as conn:
        with conn.cursor() as cursor:
            try:
                cursor.execute(f"""
SELECT
    json_value(cmetadata, '$.file_name') name,
    id
FROM
    {DEFAULT_COLLECTION_NAME}_collection
ORDER BY name """)
                return [(f"{row[0]}", row[1]) for row in cursor.fetchall()]
            except DatabaseError as de:
                return []


def refresh_doc_list():
    doc_list = get_doc_list()
    return (
        gr.Radio(choices=doc_list, value=None),
        gr.CheckboxGroup(choices=doc_list, value=[]),
        gr.CheckboxGroup(choices=doc_list, value=[])
    )


def get_server_path(doc_id: str) -> str:
    with pool.acquire() as conn:
        with conn.cursor() as cursor:
            cursor.execute(f"""
SELECT json_value(cmetadata, '$.server_path') AS server_path
FROM {DEFAULT_COLLECTION_NAME}_collection
WHERE id = :doc_id """, doc_id=doc_id)
            return cursor.fetchone()[0]


def process_text_chunks(unstructured_chunks):
    chunks = []
    chunk_id = 1
    start_offset = 1

    for chunk in unstructured_chunks:
        # chunk_length = len(chunk.text)
        chunk_length = len(chunk)
        if chunk_length == 0:
            continue
        chunks.append({
            'CHUNK_ID': chunk_id,
            'CHUNK_OFFSET': start_offset,
            'CHUNK_LENGTH': chunk_length,
            # 'CHUNK_DATA': chunk.text
            'CHUNK_DATA': chunk
        })

        # IDã¨ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’æ›´æ–°
        chunk_id += 1
        start_offset += chunk_length

    return chunks


async def command_a_task(system_text, query_text, command_a_checkbox):
    region = get_region()
    if command_a_checkbox:
        command_a = ChatOCIGenAI(
            model_id="cohere.command-a-03-2025",
            provider="cohere",
            service_endpoint=f"https://inference.generativeai.{region}.oci.oraclecloud.com",
            compartment_id=os.environ["OCI_COMPARTMENT_OCID"],
            model_kwargs={"temperature": 0.0, "top_p": 0.75, "seed": 42, "max_tokens": 3600},
        )
        if system_text:
            messages = [
                SystemMessage(content=system_text),
                HumanMessage(content=query_text),
            ]
        else:
            messages = [
                HumanMessage(content=query_text),
            ]
        start_time = time.time()
        print(f"{start_time=}")

        # å®‰å…¨ãªlangfuseè¨­å®šã‚’å–å¾—
        stream_config = get_safe_stream_config("cohere.command-a-03-2025")

        try:
            async for chunk in command_a.astream(messages, config=stream_config):
                yield chunk.content
        except Exception as e:
            logger.error(f"Command-A streamå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚streamå‡¦ç†ã‚’ç¶™ç¶šã™ã‚‹ãŸã‚ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’yield
            yield f"\n\nã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\n\n"
        end_time = time.time()
        print(f"{end_time=}")
        inference_time = end_time - start_time
        print(f"\n\næ¨è«–æ™‚é–“: {inference_time:.2f}ç§’")
        yield f"\n\næ¨è«–æ™‚é–“: {inference_time:.2f}ç§’"
        yield "TASK_DONE"
    else:
        yield "TASK_DONE"


async def xai_grok_3_task(system_text, query_text, xai_grok_3_checkbox):
    region = get_region()
    if xai_grok_3_checkbox:
        xai_grok_3 = ChatOCIGenAI(
            model_id="xai.grok-3",
            provider="xai",
            service_endpoint=f"https://inference.generativeai.{region}.oci.oraclecloud.com",
            compartment_id=os.environ["OCI_COMPARTMENT_OCID"],
            model_kwargs={"temperature": 0.0, "top_p": 0.75, "seed": 42, "max_tokens": 3600},
        )
        if system_text:
            messages = [
                SystemMessage(content=system_text),
                HumanMessage(content=query_text),
            ]
        else:
            messages = [
                HumanMessage(content=query_text),
            ]
        start_time = time.time()
        print(f"{start_time=}")

        # å®‰å…¨ãªlangfuseè¨­å®šã‚’å–å¾—
        stream_config = get_safe_stream_config("xai.grok-3")

        try:
            async for chunk in xai_grok_3.astream(messages, config=stream_config):
                yield chunk.content
        except Exception as e:
            logger.error(f"XAI Grok-3 ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†ã‚’ç¶™ç¶šã™ã‚‹ãŸã‚ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’yield
            yield f"\n\nã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\n\n"
        end_time = time.time()
        print(f"{end_time=}")
        inference_time = end_time - start_time
        print(f"\n\næ¨è«–æ™‚é–“: {inference_time:.2f}ç§’")
        yield f"\n\næ¨è«–æ™‚é–“: {inference_time:.2f}ç§’"
        yield "TASK_DONE"
    else:
        yield "TASK_DONE"


async def llama_3_3_70b_task(system_text, query_text, llama_3_3_70b_checkbox):
    region = get_region()
    if llama_3_3_70b_checkbox:
        llama_3_3_70b = ChatOCIGenAI(
            model_id="meta.llama-3.3-70b-instruct",
            provider="meta",
            service_endpoint=f"https://inference.generativeai.{region}.oci.oraclecloud.com",
            compartment_id=os.environ["OCI_COMPARTMENT_OCID"],
            model_kwargs={"temperature": 0.0, "top_p": 0.75, "seed": 42, "max_tokens": 3600},
        )
        if system_text:
            messages = [
                SystemMessage(content=system_text),
                HumanMessage(content=query_text),
            ]
        else:
            messages = [
                HumanMessage(content=query_text),
            ]
        start_time = time.time()
        print(f"{start_time=}")

        # å®‰å…¨ãªlangfuseè¨­å®šã‚’å–å¾—
        stream_config = get_safe_stream_config("meta.llama-3.3-70b-instruct")

        try:
            async for chunk in llama_3_3_70b.astream(messages, config=stream_config):
                yield chunk.content
        except Exception as e:
            logger.error(f"Llama-3.3-70B ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†ã‚’ç¶™ç¶šã™ã‚‹ãŸã‚ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’yield
            yield f"\n\nã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\n\n"
        end_time = time.time()
        print(f"{end_time=}")
        inference_time = end_time - start_time
        print(f"\n\næ¨è«–æ™‚é–“: {inference_time:.2f}ç§’")
        yield f"\n\næ¨è«–æ™‚é–“: {inference_time:.2f}ç§’"
        yield "TASK_DONE"
    else:
        yield "TASK_DONE"


async def llama_3_2_90b_vision_task(system_text, query_image, query_text, llama_3_2_90b_vision_checkbox):
    def encode_image(image_path):
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')

    region = get_region()
    if llama_3_2_90b_vision_checkbox:
        llama_3_2_90b_vision = ChatOCIGenAI(
            model_id="meta.llama-3.2-90b-vision-instruct",
            provider="meta",
            service_endpoint=f"https://inference.generativeai.{region}.oci.oraclecloud.com",
            compartment_id=os.environ["OCI_COMPARTMENT_OCID"],
            model_kwargs={"temperature": 0.0, "top_p": 0.75, "seed": 42, "max_tokens": 3600, "presence_penalty": 2,
                          "frequency_penalty": 2},
        )
        if query_image:
            base64_image = encode_image(query_image)
            human_message = HumanMessage(content=[
                {"type": "text", "text": query_text},
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"},
                },
            ], )
        else:
            human_message = HumanMessage(content=query_text)

        if system_text:
            messages = [
                SystemMessage(content=system_text),
                human_message,
            ]
        else:
            messages = [
                human_message,
            ]
        start_time = time.time()
        print(f"{start_time=}")

        # å®‰å…¨ãªlangfuseè¨­å®šã‚’å–å¾—
        stream_config = get_safe_stream_config("meta.llama-3.2-90b-vision-instruct")

        try:
            async for chunk in llama_3_2_90b_vision.astream(messages, config=stream_config):
                yield chunk.content
        except Exception as e:
            logger.error(f"Llama-3.2-90B-Vision ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†ã‚’ç¶™ç¶šã™ã‚‹ãŸã‚ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’yield
            yield f"\n\nã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\n\n"
        end_time = time.time()
        print(f"{end_time=}")
        inference_time = end_time - start_time
        print(f"\n\næ¨è«–æ™‚é–“: {inference_time:.2f}ç§’")
        yield f"\n\næ¨è«–æ™‚é–“: {inference_time:.2f}ç§’"
        yield "TASK_DONE"
    else:
        yield "TASK_DONE"


async def llama_4_maverick_task(system_text, query_image, query_text, llama_4_maverick_checkbox):
    def encode_image(image_path):
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')

    region = get_region()
    if llama_4_maverick_checkbox:
        llama_4_maverick = ChatOCIGenAI(
            model_id="meta.llama-4-maverick-17b-128e-instruct-fp8",
            provider="meta",
            service_endpoint=f"https://inference.generativeai.{region}.oci.oraclecloud.com",
            compartment_id=os.environ["OCI_COMPARTMENT_OCID"],
            model_kwargs={"temperature": 0.0, "top_p": 0.75, "seed": 42, "max_tokens": 3600},
        )
        if query_image:
            base64_image = encode_image(query_image)
            human_message = HumanMessage(content=[
                {"type": "text", "text": query_text},
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"},
                },
            ], )
        else:
            human_message = HumanMessage(content=query_text)

        if system_text:
            messages = [
                SystemMessage(content=system_text),
                human_message,
            ]
        else:
            messages = [
                human_message,
            ]
        start_time = time.time()
        print(f"{start_time=}")

        # å®‰å…¨ãªlangfuseè¨­å®šã‚’å–å¾—
        stream_config = get_safe_stream_config("meta.llama-4-maverick-17b-128e-instruct-fp8")

        try:
            async for chunk in llama_4_maverick.astream(messages, config=stream_config):
                yield chunk.content
        except Exception as e:
            logger.error(f"Llama-4-Maverick ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†ã‚’ç¶™ç¶šã™ã‚‹ãŸã‚ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’yield
            yield f"\n\nã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\n\n"
        end_time = time.time()
        print(f"{end_time=}")
        inference_time = end_time - start_time
        print(f"\n\næ¨è«–æ™‚é–“: {inference_time:.2f}ç§’")
        yield f"\n\næ¨è«–æ™‚é–“: {inference_time:.2f}ç§’"
        yield "TASK_DONE"
    else:
        yield "TASK_DONE"


async def llama_4_scout_task(system_text, query_image, query_text, llama_4_scout_checkbox):
    def encode_image(image_path):
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')

    region = get_region()
    if llama_4_scout_checkbox:
        llama_4_scout = ChatOCIGenAI(
            model_id="meta.llama-4-scout-17b-16e-instruct",
            provider="meta",
            service_endpoint=f"https://inference.generativeai.{region}.oci.oraclecloud.com",
            compartment_id=os.environ["OCI_COMPARTMENT_OCID"],
            model_kwargs={"temperature": 0.0, "top_p": 0.75, "seed": 42, "max_tokens": 3600},
        )
        if query_image:
            base64_image = encode_image(query_image)
            human_message = HumanMessage(content=[
                {"type": "text", "text": query_text},
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"},
                },
            ], )
        else:
            human_message = HumanMessage(content=query_text)

        if system_text:
            messages = [
                SystemMessage(content=system_text),
                human_message,
            ]
        else:
            messages = [
                human_message,
            ]
        start_time = time.time()
        print(f"{start_time=}")

        # å®‰å…¨ãªlangfuseè¨­å®šã‚’å–å¾—
        stream_config = get_safe_stream_config("meta.llama-4-scout-17b-16e-instruct")
        print(f"{stream_config=}")

        try:
            async for chunk in llama_4_scout.astream(messages, config=stream_config):
                yield chunk.content
        except Exception as e:
            logger.error(f"Llama-4-Scout ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†ã‚’ç¶™ç¶šã™ã‚‹ãŸã‚ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’yield
            yield f"\n\nã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\n\n"
        end_time = time.time()
        print(f"{end_time=}")
        inference_time = end_time - start_time
        print(f"\n\næ¨è«–æ™‚é–“: {inference_time:.2f}ç§’")
        yield f"\n\næ¨è«–æ™‚é–“: {inference_time:.2f}ç§’"
        yield "TASK_DONE"
    else:
        yield "TASK_DONE"


async def openai_gpt4o_task(system_text, query_text, openai_gpt4o_checkbox):
    if openai_gpt4o_checkbox:
        load_dotenv(find_dotenv())
        openai_gpt4o = ChatOpenAI(
            model="gpt-4o",
            temperature=0,
            top_p=0.75,
            seed=42,
            max_tokens=None,
            timeout=None,
            max_retries=2,
            api_key=os.environ["OPENAI_API_KEY"],
            base_url=os.environ["OPENAI_BASE_URL"],
        )
        if system_text:
            messages = [
                SystemMessage(content=system_text),
                HumanMessage(content=query_text),
            ]
        else:
            messages = [
                HumanMessage(content=query_text),
            ]
        start_time = time.time()
        print(f"{start_time=}")

        # å®‰å…¨ãªlangfuseè¨­å®šã‚’å–å¾—
        stream_config = get_safe_stream_config("gpt-4o")

        try:
            async for chunk in openai_gpt4o.astream(messages, config=stream_config):
                yield chunk.content
        except Exception as e:
            logger.error(f"OpenAI GPT-4o ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†ã‚’ç¶™ç¶šã™ã‚‹ãŸã‚ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’yield
            yield f"\n\nã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\n\n"
        end_time = time.time()
        print(f"{end_time=}")
        inference_time = end_time - start_time
        print(f"\n\næ¨è«–æ™‚é–“: {inference_time:.2f}ç§’")
        yield f"\n\næ¨è«–æ™‚é–“: {inference_time:.2f}ç§’"
        yield "TASK_DONE"
    else:
        yield "TASK_DONE"


async def openai_gpt4_task(system_text, query_text, openai_gpt4_checkbox):
    if openai_gpt4_checkbox:
        load_dotenv(find_dotenv())
        openai_gpt4 = ChatOpenAI(
            model="gpt-4",
            temperature=0,
            top_p=0.75,
            seed=42,
            max_tokens=None,
            timeout=None,
            max_retries=2,
            api_key=os.environ["OPENAI_API_KEY"],
            base_url=os.environ["OPENAI_BASE_URL"],
        )
        if system_text:
            messages = [
                SystemMessage(content=system_text),
                HumanMessage(content=query_text),
            ]
        else:
            messages = [
                HumanMessage(content=query_text),
            ]
        start_time = time.time()
        print(f"{start_time=}")

        # å®‰å…¨ãªlangfuseè¨­å®šã‚’å–å¾—
        stream_config = get_safe_stream_config("gpt-4")

        try:
            async for chunk in openai_gpt4.astream(messages, config=stream_config):
                yield chunk.content
        except Exception as e:
            logger.error(f"OpenAI GPT-4 ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†ã‚’ç¶™ç¶šã™ã‚‹ãŸã‚ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’yield
            yield f"\n\nã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\n\n"
        end_time = time.time()
        print(f"{end_time=}")
        inference_time = end_time - start_time
        print(f"\n\næ¨è«–æ™‚é–“: {inference_time:.2f}ç§’")
        yield f"\n\næ¨è«–æ™‚é–“: {inference_time:.2f}ç§’"
        yield "TASK_DONE"
    else:
        yield "TASK_DONE"


async def azure_openai_gpt4o_task(system_text, query_text, azure_openai_gpt4o_checkbox):
    if azure_openai_gpt4o_checkbox:
        load_dotenv(find_dotenv())
        azure_openai_gpt4o = AzureChatOpenAI(
            deployment_name="gpt-4o",
            temperature=0,
            top_p=0.75,
            seed=42,
            max_tokens=None,
            timeout=None,
            max_retries=2,
            azure_endpoint=os.environ["AZURE_OPENAI_ENDPOINT_GPT_4O"],
            openai_api_key=os.environ["AZURE_OPENAI_API_KEY"],
            openai_api_version=os.environ["AZURE_OPENAI_API_VERSION_GPT_4O"],
        )
        if system_text:
            messages = [
                SystemMessage(content=system_text),
                HumanMessage(content=query_text),
            ]
        else:
            messages = [
                HumanMessage(content=query_text),
            ]
        start_time = time.time()
        print(f"{start_time=}")

        # å®‰å…¨ãªlangfuseè¨­å®šã‚’å–å¾—
        stream_config = get_safe_stream_config("azure-gpt-4o")

        try:
            async for chunk in azure_openai_gpt4o.astream(messages, config=stream_config):
                yield chunk.content
        except Exception as e:
            logger.error(f"Azure OpenAI GPT-4o ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†ã‚’ç¶™ç¶šã™ã‚‹ãŸã‚ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’yield
            yield f"\n\nã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\n\n"
        end_time = time.time()
        print(f"{end_time=}")
        inference_time = end_time - start_time
        print(f"\n\næ¨è«–æ™‚é–“: {inference_time:.2f}ç§’")
        yield f"\n\næ¨è«–æ™‚é–“: {inference_time:.2f}ç§’"
        yield "TASK_DONE"
    else:
        yield "TASK_DONE"


async def azure_openai_gpt4_task(system_text, query_text, azure_openai_gpt4_checkbox):
    if azure_openai_gpt4_checkbox:
        load_dotenv(find_dotenv())
        azure_openai_gpt4 = AzureChatOpenAI(
            deployment_name="gpt-4",
            temperature=0,
            top_p=0.75,
            seed=42,
            max_tokens=None,
            timeout=None,
            max_retries=2,
            azure_endpoint=os.environ["AZURE_OPENAI_ENDPOINT_GPT_4"],
            openai_api_key=os.environ["AZURE_OPENAI_API_KEY"],
            openai_api_version=os.environ["AZURE_OPENAI_API_VERSION_GPT_4"],
        )
        if system_text:
            messages = [
                SystemMessage(content=system_text),
                HumanMessage(content=query_text),
            ]
        else:
            messages = [
                HumanMessage(content=query_text),
            ]
        start_time = time.time()
        print(f"{start_time=}")

        # å®‰å…¨ãªlangfuseè¨­å®šã‚’å–å¾—
        stream_config = get_safe_stream_config("azure-gpt-4")

        try:
            async for chunk in azure_openai_gpt4.astream(messages, config=stream_config):
                yield chunk.content
        except Exception as e:
            logger.error(f"Azure OpenAI GPT-4 ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†ã‚’ç¶™ç¶šã™ã‚‹ãŸã‚ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’yield
            yield f"\n\nã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\n\n"
        end_time = time.time()
        print(f"{end_time=}")
        inference_time = end_time - start_time
        print(f"\n\næ¨è«–æ™‚é–“: {inference_time:.2f}ç§’")
        yield f"\n\næ¨è«–æ™‚é–“: {inference_time:.2f}ç§’"
        yield "TASK_DONE"
    else:
        yield "TASK_DONE"


async def chat(
        system_text,
        xai_grok_3_user_text,
        command_a_user_text,
        llama_4_maverick_user_image,
        llama_4_maverick_user_text,
        llama_4_scout_user_image,
        llama_4_scout_user_text,
        llama_3_3_70b_user_text,
        llama_3_2_90b_vision_user_image,
        llama_3_2_90b_vision_user_text,
        openai_gpt4o_user_text,
        openai_gpt4_user_text,
        azure_openai_gpt4o_user_text,
        azure_openai_gpt4_user_text,
        xai_grok_3_checkbox,
        command_a_checkbox,
        llama_4_maverick_checkbox,
        llama_4_scout_checkbox,
        llama_3_3_70b_checkbox,
        llama_3_2_90b_vision_checkbox,
        openai_gpt4o_gen_checkbox,
        openai_gpt4_gen_checkbox,
        azure_openai_gpt4o_gen_checkbox,
        azure_openai_gpt4_gen_checkbox
):
    xai_grok_3_gen = xai_grok_3_task(system_text, xai_grok_3_user_text, xai_grok_3_checkbox)
    command_a_gen = command_a_task(system_text, command_a_user_text, command_a_checkbox)
    llama_4_maverick_gen = llama_4_maverick_task(system_text, llama_4_maverick_user_image,
                                                 llama_4_maverick_user_text, llama_4_maverick_checkbox)
    llama_4_scout_gen = llama_4_scout_task(system_text, llama_4_scout_user_image,
                                           llama_4_scout_user_text, llama_4_scout_checkbox)
    llama_3_3_70b_gen = llama_3_3_70b_task(system_text, llama_3_3_70b_user_text, llama_3_3_70b_checkbox)
    llama_3_2_90b_vision_gen = llama_3_2_90b_vision_task(system_text, llama_3_2_90b_vision_user_image,
                                                         llama_3_2_90b_vision_user_text,
                                                         llama_3_2_90b_vision_checkbox)
    openai_gpt4o_gen = openai_gpt4o_task(system_text, openai_gpt4o_user_text, openai_gpt4o_gen_checkbox)
    openai_gpt4_gen = openai_gpt4_task(system_text, openai_gpt4_user_text, openai_gpt4_gen_checkbox)
    azure_openai_gpt4o_gen = azure_openai_gpt4o_task(system_text, azure_openai_gpt4o_user_text,
                                                     azure_openai_gpt4o_gen_checkbox)
    azure_openai_gpt4_gen = azure_openai_gpt4_task(system_text, azure_openai_gpt4_user_text,
                                                   azure_openai_gpt4_gen_checkbox)

    responses_status = ["", "", "", "", "", "", "", "", "", ""]
    while True:
        responses = ["", "", "", "", "", "", "", "", "", ""]
        generators = [xai_grok_3_gen, command_a_gen,
                      llama_4_maverick_gen, llama_4_scout_gen,
                      llama_3_3_70b_gen, llama_3_2_90b_vision_gen,
                      openai_gpt4o_gen, openai_gpt4_gen,
                      azure_openai_gpt4o_gen, azure_openai_gpt4_gen]

        for i, gen in enumerate(generators):
            try:
                response = await anext(gen)
                if response:
                    if response == "TASK_DONE":
                        responses_status[i] = response
                    else:
                        responses[i] = response
            except StopAsyncIteration:
                pass

        yield tuple(responses)

        if all(response_status == "TASK_DONE" for response_status in responses_status):
            print("All tasks completed with DONE")
            break


def set_chat_llm_answer(llm_answer_checkbox):
    xai_grok_3_answer_visible = False
    command_a_answer_visible = False
    llama_4_maverick_answer_visible = False
    llama_4_scout_answer_visible = False
    llama_3_3_70b_answer_visible = False
    llama_3_2_90b_vision_answer_visible = False
    openai_gpt4o_answer_visible = False
    openai_gpt4_answer_visible = False
    azure_openai_gpt4o_answer_visible = False
    azure_openai_gpt4_answer_visible = False
    if "xai/grok-3" in llm_answer_checkbox:
        xai_grok_3_answer_visible = True
    if "cohere/command-a" in llm_answer_checkbox:
        command_a_answer_visible = True
    if "meta/llama-4-maverick-17b-128e-instruct-fp8" in llm_answer_checkbox:
        llama_4_maverick_answer_visible = True
    if "meta/llama-4-scout-17b-16e-instruct" in llm_answer_checkbox:
        llama_4_scout_answer_visible = True
    if "meta/llama-3-3-70b" in llm_answer_checkbox:
        llama_3_3_70b_answer_visible = True
    if "meta/llama-3-2-90b-vision" in llm_answer_checkbox:
        llama_3_2_90b_vision_answer_visible = True
    if "openai/gpt-4o" in llm_answer_checkbox:
        openai_gpt4o_answer_visible = True
    if "openai/gpt-4" in llm_answer_checkbox:
        openai_gpt4_answer_visible = True
    if "azure_openai/gpt-4o" in llm_answer_checkbox:
        azure_openai_gpt4o_answer_visible = True
    if "azure_openai/gpt-4" in llm_answer_checkbox:
        azure_openai_gpt4_answer_visible = True
    return (
        gr.Accordion(visible=xai_grok_3_answer_visible),
        gr.Accordion(visible=command_a_answer_visible),
        gr.Accordion(visible=llama_4_maverick_answer_visible),
        gr.Accordion(visible=llama_4_scout_answer_visible),
        gr.Accordion(visible=llama_3_3_70b_answer_visible),
        gr.Accordion(visible=llama_3_2_90b_vision_answer_visible),
        gr.Accordion(visible=openai_gpt4o_answer_visible),
        gr.Accordion(visible=openai_gpt4_answer_visible),
        gr.Accordion(visible=azure_openai_gpt4o_answer_visible),
        gr.Accordion(visible=azure_openai_gpt4_answer_visible)
    )


def set_chat_llm_evaluation(llm_evaluation_checkbox):
    xai_grok_3_evaluation_visible = False
    command_a_evaluation_visible = False
    llama_4_maverick_evaluation_visible = False
    llama_4_scout_evaluation_visible = False
    llama_3_3_70b_evaluation_visible = False
    llama_3_2_90b_vision_evaluation_visible = False
    openai_gpt4o_evaluation_visible = False
    openai_gpt4_evaluation_visible = False
    azure_openai_gpt4o_evaluation_visible = False
    azure_openai_gpt4_evaluation_visible = False
    if llm_evaluation_checkbox:
        xai_grok_3_evaluation_visible = True
        command_a_evaluation_visible = True
        llama_4_maverick_evaluation_visible = True
        llama_4_scout_evaluation_visible = True
        llama_3_3_70b_evaluation_visible = True
        llama_3_2_90b_vision_evaluation_visible = True
        openai_gpt4o_evaluation_visible = True
        openai_gpt4_evaluation_visible = True
        azure_openai_gpt4o_evaluation_visible = True
        azure_openai_gpt4_evaluation_visible = True
    return (
        gr.Accordion(visible=xai_grok_3_evaluation_visible),
        gr.Accordion(visible=command_a_evaluation_visible),
        gr.Accordion(visible=llama_4_maverick_evaluation_visible),
        gr.Accordion(visible=llama_4_scout_evaluation_visible),
        gr.Accordion(visible=llama_3_3_70b_evaluation_visible),
        gr.Accordion(visible=llama_3_2_90b_vision_evaluation_visible),
        gr.Accordion(visible=openai_gpt4o_evaluation_visible),
        gr.Accordion(visible=openai_gpt4_evaluation_visible),
        gr.Accordion(visible=azure_openai_gpt4o_evaluation_visible),
        gr.Accordion(visible=azure_openai_gpt4_evaluation_visible)
    )


def set_image_answer_visibility(llm_answer_checkbox, use_image):
    """
    ç”»åƒå›ç­”ã®å¯è¦–æ€§ã‚’åˆ¶å¾¡ã™ã‚‹é–¢æ•°
    é¸æŠã•ã‚ŒãŸLLMãƒ¢ãƒ‡ãƒ«ã¨ã€Œç”»åƒã‚’ä½¿ã£ã¦å›ç­”ã€ã®çŠ¶æ…‹ã«åŸºã¥ã„ã¦ã€
    å¯¾è±¡ã®ãƒ¢ãƒ‡ãƒ«ã®ç”»åƒå›ç­”Accordionã®å¯è¦–æ€§ã‚’æ±ºå®šã™ã‚‹
    """
    llama_4_maverick_image_visible = False
    llama_4_scout_image_visible = False
    llama_3_2_90b_vision_image_visible = False
    openai_gpt4o_image_visible = False
    azure_openai_gpt4o_image_visible = False

    # ç”»åƒã‚’ä½¿ã£ã¦å›ç­”ãŒã‚ªãƒ³ã§ã€ã‹ã¤å¯¾å¿œã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿è¡¨ç¤º
    if use_image:
        if "meta/llama-4-maverick-17b-128e-instruct-fp8" in llm_answer_checkbox:
            llama_4_maverick_image_visible = True
        if "meta/llama-4-scout-17b-16e-instruct" in llm_answer_checkbox:
            llama_4_scout_image_visible = True
        if "meta/llama-3-2-90b-vision" in llm_answer_checkbox:
            llama_3_2_90b_vision_image_visible = True
        if "openai/gpt-4o" in llm_answer_checkbox:
            openai_gpt4o_image_visible = True
        if "azure_openai/gpt-4o" in llm_answer_checkbox:
            azure_openai_gpt4o_image_visible = True

    return (
        gr.Accordion(visible=llama_4_maverick_image_visible),
        gr.Accordion(visible=llama_4_scout_image_visible),
        gr.Accordion(visible=llama_3_2_90b_vision_image_visible),
        gr.Accordion(visible=openai_gpt4o_image_visible),
        gr.Accordion(visible=azure_openai_gpt4o_image_visible)
    )


async def chat_stream(system_text, query_image, query_text, llm_answer_checkbox):
    has_error = False
    if not llm_answer_checkbox or len(llm_answer_checkbox) == 0:
        has_error = True
        gr.Warning("LLM ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„")
    if not query_text:
        has_error = True
        gr.Warning("ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ»ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

    if has_error:
        yield (
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        )
        return
    xai_grok_3_user_text = query_text
    command_a_user_text = query_text
    llama_4_maverick_user_image = query_image
    llama_4_maverick_user_text = query_text
    llama_4_scout_user_image = query_image
    llama_4_scout_user_text = query_text
    llama_3_3_70b_user_text = query_text
    llama_3_2_90b_vision_user_image = query_image
    llama_3_2_90b_vision_user_text = query_text
    openai_gpt4o_user_text = query_text
    openai_gpt4_user_text = query_text
    azure_openai_gpt4o_user_text = query_text
    azure_openai_gpt4_user_text = query_text
    xai_grok_3_checkbox = False
    command_a_checkbox = False
    llama_4_maverick_checkbox = False
    llama_4_scout_checkbox = False
    llama_3_3_70b_checkbox = False
    llama_3_2_90b_vision_checkbox = False
    openai_gpt4o_checkbox = False
    openai_gpt4_checkbox = False
    azure_openai_gpt4o_checkbox = False
    azure_openai_gpt4_checkbox = False
    if "xai/grok-3" in llm_answer_checkbox:
        xai_grok_3_checkbox = True
    if "cohere/command-a" in llm_answer_checkbox:
        command_a_checkbox = True
    if "meta/llama-4-maverick-17b-128e-instruct-fp8" in llm_answer_checkbox:
        llama_4_maverick_checkbox = True
    if "meta/llama-4-scout-17b-16e-instruct" in llm_answer_checkbox:
        llama_4_scout_checkbox = True
    if "meta/llama-3-3-70b" in llm_answer_checkbox:
        llama_3_3_70b_checkbox = True
    if "meta/llama-3-2-90b-vision" in llm_answer_checkbox:
        llama_3_2_90b_vision_checkbox = True
    if "openai/gpt-4o" in llm_answer_checkbox:
        openai_gpt4o_checkbox = True
    if "openai/gpt-4" in llm_answer_checkbox:
        openai_gpt4_checkbox = True
    if "azure_openai/gpt-4o" in llm_answer_checkbox:
        azure_openai_gpt4o_checkbox = True
    if "azure_openai/gpt-4" in llm_answer_checkbox:
        azure_openai_gpt4_checkbox = True
    # ChatOCIGenAI
    xai_grok_3_response = ""
    command_a_response = ""
    llama_4_maverick_response = ""
    llama_4_scout_response = ""
    llama_3_3_70b_response = ""
    llama_3_2_90b_vision_response = ""
    openai_gpt4o_response = ""
    openai_gpt4_response = ""
    azure_openai_gpt4o_response = ""
    azure_openai_gpt4_response = ""
    async for xai_grok_3, command_a, llama_4_maverick, llama_4_scout, llama_3_3_70b, llama_3_2_90b_vision, gpt4o, gpt4, azure_gpt4o, azure_gpt4 in chat(
            system_text,
            xai_grok_3_user_text,
            command_a_user_text,
            llama_4_maverick_user_image,
            llama_4_maverick_user_text,
            llama_4_scout_user_image,
            llama_4_scout_user_text,
            llama_3_3_70b_user_text,
            llama_3_2_90b_vision_user_image,
            llama_3_2_90b_vision_user_text,
            openai_gpt4o_user_text,
            openai_gpt4_user_text,
            azure_openai_gpt4o_user_text,
            azure_openai_gpt4_user_text,
            xai_grok_3_checkbox,
            command_a_checkbox,
            llama_4_maverick_checkbox,
            llama_4_scout_checkbox,
            llama_3_3_70b_checkbox,
            llama_3_2_90b_vision_checkbox,
            openai_gpt4o_checkbox,
            openai_gpt4_checkbox,
            azure_openai_gpt4o_checkbox,
            azure_openai_gpt4_checkbox
    ):
        xai_grok_3_response += xai_grok_3
        command_a_response += command_a
        llama_4_maverick_response += llama_4_maverick
        llama_4_scout_response += llama_4_scout
        llama_3_3_70b_response += llama_3_3_70b
        llama_3_2_90b_vision_response += llama_3_2_90b_vision
        openai_gpt4o_response += gpt4o
        openai_gpt4_response += gpt4
        azure_openai_gpt4o_response += azure_gpt4o
        azure_openai_gpt4_response += azure_gpt4
        yield (
            gr.Markdown(value=xai_grok_3_response),
            gr.Markdown(value=command_a_response),
            gr.Markdown(value=llama_4_maverick_response),
            gr.Markdown(value=llama_4_scout_response),
            gr.Markdown(value=llama_3_3_70b_response),
            gr.Markdown(value=llama_3_2_90b_vision_response),
            gr.Markdown(value=openai_gpt4o_response),
            gr.Markdown(value=openai_gpt4_response),
            gr.Markdown(value=azure_openai_gpt4o_response),
            gr.Markdown(value=azure_openai_gpt4_response)
        )


def reset_all_llm_messages():
    """
    ã™ã¹ã¦ã®LLMãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒªã‚»ãƒƒãƒˆã™ã‚‹
    """
    return (
        gr.Markdown(value=""),  # tab_chat_document_xai_grok_3_answer_text
        gr.Markdown(value=""),  # tab_chat_document_command_a_answer_text
        gr.Markdown(value=""),  # tab_chat_document_llama_4_maverick_answer_text
        gr.Markdown(value=""),  # tab_chat_document_llama_4_scout_answer_text
        gr.Markdown(value=""),  # tab_chat_document_llama_3_3_70b_answer_text
        gr.Markdown(value=""),  # tab_chat_document_llama_3_2_90b_vision_answer_text
        gr.Markdown(value=""),  # tab_chat_document_openai_gpt4o_answer_text
        gr.Markdown(value=""),  # tab_chat_document_openai_gpt4_answer_text
        gr.Markdown(value=""),  # tab_chat_document_azure_openai_gpt4o_answer_text
        gr.Markdown(value=""),  # tab_chat_document_azure_openai_gpt4_answer_text
    )


def reset_image_answers():
    """
    ç”»åƒå›ç­”ã‚’ãƒªã‚»ãƒƒãƒˆã™ã‚‹
    """
    return (
        gr.Markdown(value=""),  # tab_chat_document_llama_4_maverick_image_answer_text
        gr.Markdown(value=""),  # tab_chat_document_llama_4_scout_image_answer_text
        gr.Markdown(value=""),  # tab_chat_document_llama_3_2_90b_vision_image_answer_text
        gr.Markdown(value=""),  # tab_chat_document_openai_gpt4o_image_answer_text
        gr.Markdown(value=""),  # tab_chat_document_azure_openai_gpt4o_image_answer_text
    )


def reset_llm_evaluations():
    """
    LLMè©•ä¾¡ã‚’ãƒªã‚»ãƒƒãƒˆã™ã‚‹
    """
    return (
        gr.Markdown(value=""),  # tab_chat_document_xai_grok_3_evaluation_text
        gr.Markdown(value=""),  # tab_chat_document_command_a_evaluation_text
        gr.Markdown(value=""),  # tab_chat_document_llama_4_maverick_evaluation_text
        gr.Markdown(value=""),  # tab_chat_document_llama_4_scout_evaluation_text
        gr.Markdown(value=""),  # tab_chat_document_llama_3_3_70b_evaluation_text
        gr.Markdown(value=""),  # tab_chat_document_llama_3_2_90b_vision_evaluation_text
        gr.Markdown(value=""),  # tab_chat_document_openai_gpt4o_evaluation_text
        gr.Markdown(value=""),  # tab_chat_document_openai_gpt4_evaluation_text
        gr.Markdown(value=""),  # tab_chat_document_azure_openai_gpt4o_evaluation_text
        gr.Markdown(value=""),  # tab_chat_document_azure_openai_gpt4_evaluation_text
    )


def reset_eval_by_human_result():
    return (
        gr.Radio(value="good"),
        gr.Textbox(value=""),
        gr.Radio(value="good"),
        gr.Textbox(value=""),
        gr.Radio(value="good"),
        gr.Textbox(value=""),
        gr.Radio(value="good"),
        gr.Textbox(value=""),
        gr.Radio(value="good"),
        gr.Textbox(value=""),
        gr.Radio(value="good"),
        gr.Textbox(value=""),
        gr.Radio(value="good"),
        gr.Textbox(value=""),
        gr.Radio(value="good"),
        gr.Textbox(value=""),
        gr.Radio(value="good"),
        gr.Textbox(value=""),
        gr.Radio(value="good"),
        gr.Textbox(value=""),
    )


def eval_by_human(
        query_id,
        llm_name,
        human_evaluation_result,
        user_comment,
):
    print("eval_by_human() start...")
    with pool.acquire() as conn:
        with conn.cursor() as cursor:
            update_sql = """
                         UPDATE RAG_QA_FEEDBACK
                         SET human_evaluation_result = :1,
                                user_comment = :2
                         WHERE query_id = :3 AND llm_name = :4 \
                         """
            cursor.execute(
                update_sql,
                [
                    human_evaluation_result,
                    user_comment,
                    query_id,
                    llm_name
                ]
            )

            conn.commit()

    return (
        gr.Radio(),
        gr.Textbox(value=user_comment)
    )


def create_oci_cred(user_ocid, tenancy_ocid, fingerprint, private_key_file, region):
    def process_private_key(private_key_file_path):
        with open(private_key_file_path, 'r') as file:
            lines = file.readlines()

        processed_key = ''.join(line.strip() for line in lines if not line.startswith('--'))
        return processed_key

    has_error = False
    if not user_ocid:
        has_error = True
        gr.Warning("User OCIDã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if not tenancy_ocid:
        has_error = True
        gr.Warning("Tenancy OCIDã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if not fingerprint:
        has_error = True
        gr.Warning("Fingerprintã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if not private_key_file:
        has_error = True
        gr.Warning("Private Keyã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if not region:
        has_error = True
        gr.Warning("Regionã‚’é¸æŠã—ã¦ãã ã•ã„")

    if has_error:
        return gr.Accordion(), gr.Textbox()

    user_ocid = user_ocid.strip()
    tenancy_ocid = tenancy_ocid.strip()
    fingerprint = fingerprint.strip()
    region = region.strip()

    # set up OCI config
    if not os.path.exists("/root/.oci"):
        os.makedirs("/root/.oci")
    if not os.path.exists("/root/.oci/config"):
        shutil.copy("./.oci/config", "/root/.oci/config")
    oci_config_path = find_dotenv("/root/.oci/config")
    key_file_path = '/root/.oci/oci_api_key.pem'
    set_key(oci_config_path, "user", user_ocid, quote_mode="never")
    set_key(oci_config_path, "tenancy", tenancy_ocid, quote_mode="never")
    set_key(oci_config_path, "region", region, quote_mode="never")
    set_key(oci_config_path, "fingerprint", fingerprint, quote_mode="never")
    set_key(oci_config_path, "key_file", key_file_path, quote_mode="never")
    shutil.copy(private_key_file.name, key_file_path)
    load_dotenv(oci_config_path)

    # set up OCI Credential on database
    private_key = process_private_key(private_key_file.name)

    with pool.acquire() as conn:
        with conn.cursor() as cursor:
            try:
                # Define the PL/SQL statement
                append_acl_sql = """
BEGIN
  DBMS_NETWORK_ACL_ADMIN.APPEND_HOST_ACE(
    host => '*',
    ace => xs$ace_type(privilege_list => xs$name_list('connect'),
                       principal_name => 'admin',
                       principal_type => xs_acl.ptype_db));
END;
                """

                # Execute the PL/SQL statement
                cursor.execute(append_acl_sql)
            except DatabaseError as de:
                print(f"DatabaseError={de}")

            try:
                drop_oci_cred_sql = "BEGIN dbms_vector.drop_credential('OCI_CRED'); END;"
                cursor.execute(drop_oci_cred_sql)
            except DatabaseError as de:
                print(f"DatabaseError={de}")

            oci_cred = {
                'user_ocid': user_ocid,
                'tenancy_ocid': tenancy_ocid,
                'compartment_ocid': os.environ["OCI_COMPARTMENT_OCID"],
                'private_key': private_key.strip(),
                'fingerprint': fingerprint
            }

            create_oci_cred_sql = """
BEGIN
   dbms_vector.create_credential(
       credential_name => 'OCI_CRED',
       params => json(:json_params)
   );
END; """

            cursor.execute(create_oci_cred_sql, json_params=json.dumps(oci_cred))
            conn.commit()

    create_oci_cred_sql = f"""
-- Append Host ACE
BEGIN
  DBMS_NETWORK_ACL_ADMIN.APPEND_HOST_ACE(
    host => '*',
    ace => xs$ace_type(privilege_list => xs$name_list('connect'),
                       principal_name => 'admin',
                       principal_type => xs_acl.ptype_db));
END;

-- Drop Existing OCI Credential
BEGIN dbms_vector.drop_credential('OCI_CRED'); END;

-- Create New OCI Credential
BEGIN
    dbms_vector.create_credential(
        credential_name => 'OCI_CRED',
        params => json('{json.dumps(oci_cred)}')
    );
END;
"""
    gr.Info("OCI API Keyã®è¨­å®šãŒå®Œäº†ã—ã¾ã—ãŸ")
    return gr.Accordion(), gr.Textbox(value=create_oci_cred_sql.strip())


def create_cohere_cred(cohere_cred_api_key):
    has_error = False
    if not cohere_cred_api_key:
        has_error = True
        gr.Warning("Cohere API Keyã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if has_error:
        return gr.Textbox()
    cohere_cred_api_key = cohere_cred_api_key.strip()
    env_path = find_dotenv()
    os.environ["COHERE_API_KEY"] = cohere_cred_api_key
    set_key(env_path, "COHERE_API_KEY", cohere_cred_api_key, quote_mode="never")
    load_dotenv(env_path)
    gr.Info("Cohere API Keyã®è¨­å®šãŒå®Œäº†ã—ã¾ã—ãŸ")
    return gr.Textbox(value=cohere_cred_api_key)


def create_openai_cred(openai_cred_base_url, openai_cred_api_key):
    has_error = False
    if not openai_cred_base_url:
        has_error = True
        gr.Warning("OpenAI Base URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if not openai_cred_api_key:
        has_error = True
        gr.Warning("OpenAI API Keyã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if has_error:
        return gr.Textbox(), gr.Textbox()

    openai_cred_base_url = openai_cred_base_url.strip()
    openai_cred_api_key = openai_cred_api_key.strip()
    env_path = find_dotenv()
    os.environ["OPENAI_BASE_URL"] = openai_cred_base_url
    os.environ["OPENAI_API_KEY"] = openai_cred_api_key
    set_key(env_path, "OPENAI_BASE_URL", openai_cred_base_url, quote_mode="never")
    set_key(env_path, "OPENAI_API_KEY", openai_cred_api_key, quote_mode="never")
    load_dotenv(find_dotenv())
    gr.Info("OpenAI API Keyã®è¨­å®šãŒå®Œäº†ã—ã¾ã—ãŸ")
    return gr.Textbox(value=openai_cred_base_url), gr.Textbox(value=openai_cred_api_key)


def create_azure_openai_cred(
        azure_openai_cred_api_key,
        azure_openai_cred_endpoint_gpt_4o,
        azure_openai_cred_endpoint_gpt_4,
):
    has_error = False
    if not azure_openai_cred_api_key:
        has_error = True
        gr.Warning("Azure OpenAI API Keyã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if not azure_openai_cred_endpoint_gpt_4o:
        has_error = True
        gr.Warning("Azure OpenAI GPT-4O Endpointã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if 'api-version=' not in azure_openai_cred_endpoint_gpt_4o:
        has_error = True
        gr.Warning("Azure OpenAI GPT-4O Endpointã«ã¯api-versionã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if azure_openai_cred_endpoint_gpt_4 and 'api-version=' not in azure_openai_cred_endpoint_gpt_4:
        has_error = True
        gr.Warning("Azure OpenAI GPT-4 Endpointã«ã¯api-versionã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if has_error:
        return gr.Textbox(), gr.Textbox(), gr.Textbox()

    azure_openai_cred_api_key = azure_openai_cred_api_key.strip()
    azure_openai_cred_endpoint_gpt_4o = azure_openai_cred_endpoint_gpt_4o.strip() if azure_openai_cred_endpoint_gpt_4o else ""
    azure_openai_cred_api_version_gpt_4o = re.search(r"api-version=([^&]+)", azure_openai_cred_endpoint_gpt_4o).group(
        1).strip()
    # azure_openai_cred_api_version_gpt_4o = azure_openai_cred_api_version_gpt_4o.strip() if azure_openai_cred_api_version_gpt_4o else ""
    azure_openai_cred_endpoint_gpt_4 = azure_openai_cred_endpoint_gpt_4.strip() if azure_openai_cred_endpoint_gpt_4 else ""
    azure_openai_cred_api_version_gpt_4 = ""
    if azure_openai_cred_endpoint_gpt_4:
        azure_openai_cred_api_version_gpt_4 = re.search(r"api-version=([^&]+)", azure_openai_cred_endpoint_gpt_4).group(
            1).strip()

    env_path = find_dotenv()

    os.environ["AZURE_OPENAI_API_KEY"] = azure_openai_cred_api_key
    os.environ["AZURE_OPENAI_ENDPOINT_GPT_4O"] = azure_openai_cred_endpoint_gpt_4o
    os.environ["AZURE_OPENAI_API_VERSION_GPT_4O"] = azure_openai_cred_api_version_gpt_4o
    os.environ["AZURE_OPENAI_ENDPOINT_GPT_4"] = azure_openai_cred_endpoint_gpt_4
    os.environ["AZURE_OPENAI_API_VERSION_GPT_4"] = azure_openai_cred_api_version_gpt_4

    set_key(env_path, "AZURE_OPENAI_API_KEY", azure_openai_cred_api_key, quote_mode="never")
    set_key(env_path, "AZURE_OPENAI_ENDPOINT_GPT_4O", azure_openai_cred_endpoint_gpt_4o, quote_mode="never")
    set_key(env_path, "AZURE_OPENAI_API_VERSION_GPT_4O", azure_openai_cred_api_version_gpt_4o, quote_mode="never")
    set_key(env_path, "AZURE_OPENAI_ENDPOINT_GPT_4", azure_openai_cred_endpoint_gpt_4, quote_mode="never")
    set_key(env_path, "AZURE_OPENAI_API_VERSION_GPT_4", azure_openai_cred_api_version_gpt_4, quote_mode="never")

    load_dotenv(find_dotenv())

    gr.Info("Azure OpenAI API Keyã®è¨­å®šãŒå®Œäº†ã—ã¾ã—ãŸ")
    return gr.Textbox(value=azure_openai_cred_api_key), \
        gr.Textbox(value=azure_openai_cred_endpoint_gpt_4o), \
        gr.Textbox(value=azure_openai_cred_endpoint_gpt_4)


def create_langfuse_cred(langfuse_cred_secret_key, langfuse_cred_public_key, langfuse_cred_host):
    has_error = False
    if not langfuse_cred_secret_key:
        has_error = True
        gr.Warning("Langfuse Secret Keyã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if not langfuse_cred_public_key:
        has_error = True
        gr.Warning("Langfuse Public Keyã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if not langfuse_cred_host:
        has_error = True
        gr.Warning("Langfuse Hostã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if has_error:
        return gr.Textbox(), gr.Textbox(), gr.Textbox()

    langfuse_cred_secret_key = langfuse_cred_secret_key.strip()
    langfuse_cred_public_key = langfuse_cred_public_key.strip()
    langfuse_cred_host = langfuse_cred_host.strip()
    env_path = find_dotenv()
    os.environ["LANGFUSE_SECRET_KEY"] = langfuse_cred_secret_key
    os.environ["LANGFUSE_PUBLIC_KEY"] = langfuse_cred_public_key
    os.environ["LANGFUSE_HOST"] = langfuse_cred_host
    set_key(env_path, "LANGFUSE_SECRET_KEY", langfuse_cred_secret_key, quote_mode="never")
    set_key(env_path, "LANGFUSE_PUBLIC_KEY", langfuse_cred_public_key, quote_mode="never")
    set_key(env_path, "LANGFUSE_HOST", langfuse_cred_host, quote_mode="never")
    load_dotenv(env_path)
    gr.Info("LangFuseã®è¨­å®šãŒå®Œäº†ã—ã¾ã—ãŸ")
    return (
        gr.Textbox(value=langfuse_cred_secret_key),
        gr.Textbox(value=langfuse_cred_public_key),
        gr.Textbox(value=langfuse_cred_host)
    )


def create_table():
    # Drop the preference if it exists
    check_preference_sql = """
                           SELECT PRE_NAME
                           FROM CTX_PREFERENCES
                           WHERE PRE_NAME = 'WORLD_LEXER'
                             AND PRE_OWNER = USER \
                           """

    drop_preference_plsql = """
                            -- Drop Preference
                            BEGIN
  CTX_DDL.DROP_PREFERENCE
                            ('world_lexer');
                            END; \
                            """

    create_preference_plsql = """
                              -- Create Preference
                              BEGIN
  CTX_DDL.CREATE_PREFERENCE
                              ('world_lexer','WORLD_LEXER');
                              END; \
                              """

    # Drop the index if it exists
    check_index_sql = f"""
SELECT INDEX_NAME FROM USER_INDEXES WHERE INDEX_NAME = '{DEFAULT_COLLECTION_NAME.upper()}_EMBED_DATA_IDX'
"""

    check_image_index_sql = f"""
SELECT INDEX_NAME FROM USER_INDEXES WHERE INDEX_NAME = '{DEFAULT_COLLECTION_NAME.upper()}_IMAGE_EMBED_DATA_IDX'
"""

    drop_index_sql = f"""
-- Drop Index
DROP INDEX {DEFAULT_COLLECTION_NAME.upper()}_EMBED_DATA_IDX
"""

    drop_image_index_sql = f"""
-- Drop Image Index
DROP INDEX {DEFAULT_COLLECTION_NAME.upper()}_IMAGE_EMBED_DATA_IDX
"""

    create_index_sql = f"""
-- Create Index
-- CREATE INDEX {DEFAULT_COLLECTION_NAME}_embed_data_idx ON {DEFAULT_COLLECTION_NAME}_embedding(embed_data) INDEXTYPE IS CTXSYS.CONTEXT PARAMETERS ('LEXER world_lexer sync (on commit)')
CREATE INDEX {DEFAULT_COLLECTION_NAME}_embed_data_idx ON {DEFAULT_COLLECTION_NAME}_embedding(embed_data) INDEXTYPE IS CTXSYS.CONTEXT PARAMETERS ('LEXER world_lexer sync (every "freq=minutely; interval=1")')
"""

    create_image_index_sql = f"""
-- Create Image Index
CREATE INDEX {DEFAULT_COLLECTION_NAME}_image_embed_data_idx ON {DEFAULT_COLLECTION_NAME}_image_embedding(embed_data) INDEXTYPE IS CTXSYS.CONTEXT PARAMETERS ('LEXER world_lexer sync (every "freq=minutely; interval=1")')
"""

    output_sql_text = f"""
-- Create Collection Table
CREATE TABLE IF NOT EXISTS {DEFAULT_COLLECTION_NAME}_collection (
    id VARCHAR2(200),
    data BLOB,
    cmetadata CLOB
);
"""

    # Get embedding function and dimension first
    region = get_region()
    embed = OCIGenAIEmbeddings(
        model_id=os.environ["OCI_COHERE_EMBED_MODEL"],
        service_endpoint=f"https://inference.generativeai.{region}.oci.oraclecloud.com",
        compartment_id=os.environ["OCI_COMPARTMENT_OCID"]
    )

    # Get embedding dimension by creating a test embedding
    test_embedding = embed.embed_query("test")
    embedding_dim = len(test_embedding)

    output_sql_text += f"""
-- Create Embedding Table
CREATE TABLE IF NOT EXISTS {DEFAULT_COLLECTION_NAME}_embedding (
    doc_id VARCHAR2(200),
    embed_id NUMBER,
    embed_data VARCHAR2(4000),
    embed_vector VECTOR({embedding_dim}, FLOAT32),
    cmetadata CLOB
);
"""

    output_sql_text += f"""
-- Create Image Table
CREATE TABLE IF NOT EXISTS {DEFAULT_COLLECTION_NAME}_image (
    doc_id VARCHAR2(200),
    img_id NUMBER,
    text_data CLOB,
    vlm_data CLOB,
    base64_data CLOB
);
"""

    output_sql_text += f"""
-- Create Image Embedding Table
CREATE TABLE IF NOT EXISTS {DEFAULT_COLLECTION_NAME}_image_embedding (
    doc_id VARCHAR2(200),
    embed_id NUMBER,
    embed_data VARCHAR2(4000),
    embed_vector VECTOR({embedding_dim}, FLOAT32),
    cmetadata CLOB,
    img_id NUMBER
);
"""

    drop_rag_qa_result_sql = """DROP TABLE IF EXISTS RAG_QA_RESULT"""

    create_rag_qa_result_sql = """CREATE TABLE IF NOT EXISTS RAG_QA_RESULT
    (
        id
        NUMBER
        GENERATED
        ALWAYS AS
        IDENTITY
        PRIMARY
        KEY,
        query_id
        VARCHAR2
                                  (
        100
                                  ),
        query VARCHAR2
                                  (
                                      4000
                                  ),
        standard_answer VARCHAR2
                                  (
                                      30000
                                  ),
        sql CLOB,
        created_date TIMESTAMP DEFAULT TO_TIMESTAMP
                                  (
                                      TO_CHAR
                                  (
                                      SYSTIMESTAMP,
                                      'YYYY-MM-DD HH24:MI:SS'
                                  ), 'YYYY-MM-DD HH24:MI:SS')
        )"""

    drop_rag_qa_feedback_sql = """DROP TABLE IF EXISTS RAG_QA_FEEDBACK"""

    create_rag_qa_feedback_sql = """CREATE TABLE IF NOT EXISTS RAG_QA_FEEDBACK
    (
        id
        NUMBER
        GENERATED
        ALWAYS AS
        IDENTITY
        PRIMARY
        KEY,
        query_id
        VARCHAR2
                                    (
        100
                                    ),
        llm_name VARCHAR2
                                    (
                                        100
                                    ),
        llm_answer CLOB,
        ragas_evaluation_result CLOB,
        human_evaluation_result VARCHAR2
                                    (
                                        20
                                    ),
        user_comment VARCHAR2
                                    (
                                        30000
                                    ),
        created_date TIMESTAMP DEFAULT TO_TIMESTAMP
                                    (
                                        TO_CHAR
                                    (
                                        SYSTIMESTAMP,
                                        'YYYY-MM-DD HH24:MI:SS'
                                    ), 'YYYY-MM-DD HH24:MI:SS')
        )"""

    output_sql_text += "\n" + create_preference_plsql.strip() + "\n"
    output_sql_text += "\n" + drop_rag_qa_result_sql.strip() + ";"
    output_sql_text += "\n" + drop_rag_qa_feedback_sql.strip() + ";"
    output_sql_text += f"\n-- Drop Indexes\nDROP INDEX IF EXISTS {DEFAULT_COLLECTION_NAME}_embed_data_idx;"
    output_sql_text += f"\nDROP INDEX IF EXISTS {DEFAULT_COLLECTION_NAME}_image_embed_data_idx;"
    output_sql_text += "\n" + create_index_sql.strip() + ";"
    output_sql_text += "\n" + create_image_index_sql.strip() + ";"
    output_sql_text += "\n" + create_rag_qa_result_sql.strip() + ";"
    output_sql_text += "\n" + create_rag_qa_feedback_sql.strip() + ";"

    # Drop and Create table SQLs for image and image_embedding tables
    drop_image_table_sql = f"DROP TABLE {DEFAULT_COLLECTION_NAME}_image PURGE"
    drop_image_embedding_table_sql = f"DROP TABLE {DEFAULT_COLLECTION_NAME}_image_embedding PURGE"

    create_image_table_sql = f"""CREATE TABLE {DEFAULT_COLLECTION_NAME}_image (
    doc_id VARCHAR2(200),
    img_id NUMBER,
    text_data CLOB,
    vlm_data CLOB,
    base64_data CLOB
)"""

    create_image_embedding_table_sql = f"""CREATE TABLE {DEFAULT_COLLECTION_NAME}_image_embedding (
    doc_id VARCHAR2(200),
    embed_id NUMBER,
    embed_data VARCHAR2(4000),
    embed_vector VECTOR({embedding_dim}, FLOAT32),
    cmetadata CLOB,
    img_id NUMBER
)"""

    # Add drop and create statements to output SQL text
    output_sql_text += f"\n-- Drop Image Tables\n{drop_image_table_sql};"
    output_sql_text += f"\n{drop_image_embedding_table_sql};"
    output_sql_text += f"\n-- Create Image Tables\n{create_image_table_sql};"
    output_sql_text += f"\n{create_image_embedding_table_sql};"

    with pool.acquire() as conn:
        with conn.cursor() as cursor:
            cursor.execute(check_preference_sql)
            if cursor.fetchone():
                cursor.execute(drop_preference_plsql)
            else:
                print("Preference 'WORLD_LEXER' does not exist.")
            cursor.execute(create_preference_plsql)

            # Drop and recreate image tables with existence check
            # Check and drop image table
            try:
                cursor.execute(f"SELECT COUNT(*) FROM {DEFAULT_COLLECTION_NAME}_image")
                # Table exists, drop it
                cursor.execute(drop_image_table_sql)
                print(f"ãƒ†ãƒ¼ãƒ–ãƒ« {DEFAULT_COLLECTION_NAME}_image ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
            except DatabaseError as e:
                if e.args[0].code == 942:  # Table or view does not exist
                    print(f"ãƒ†ãƒ¼ãƒ–ãƒ« {DEFAULT_COLLECTION_NAME}_image ã¯å­˜åœ¨ã—ã¾ã›ã‚“")
                else:
                    print(f"ãƒ†ãƒ¼ãƒ–ãƒ« {DEFAULT_COLLECTION_NAME}_image ã®å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")

            # Check and drop image_embedding table
            try:
                cursor.execute(f"SELECT COUNT(*) FROM {DEFAULT_COLLECTION_NAME}_image_embedding")
                # Table exists, drop it
                cursor.execute(drop_image_embedding_table_sql)
                print(f"ãƒ†ãƒ¼ãƒ–ãƒ« {DEFAULT_COLLECTION_NAME}_image_embedding ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
            except DatabaseError as e:
                if e.args[0].code == 942:  # Table or view does not exist
                    print(f"ãƒ†ãƒ¼ãƒ–ãƒ« {DEFAULT_COLLECTION_NAME}_image_embedding ã¯å­˜åœ¨ã—ã¾ã›ã‚“")
                else:
                    print(f"ãƒ†ãƒ¼ãƒ–ãƒ« {DEFAULT_COLLECTION_NAME}_image_embedding ã®å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")

            # Create image table
            try:
                cursor.execute(create_image_table_sql)
                print(f"ãƒ†ãƒ¼ãƒ–ãƒ« {DEFAULT_COLLECTION_NAME}_image ã‚’ä½œæˆã—ã¾ã—ãŸ")
            except DatabaseError as e:
                print(f"ãƒ†ãƒ¼ãƒ–ãƒ« {DEFAULT_COLLECTION_NAME}_image ã®ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")

            # Create image_embedding table
            try:
                cursor.execute(create_image_embedding_table_sql)
                print(f"ãƒ†ãƒ¼ãƒ–ãƒ« {DEFAULT_COLLECTION_NAME}_image_embedding ã‚’ä½œæˆã—ã¾ã—ãŸ")
            except DatabaseError as e:
                print(f"ãƒ†ãƒ¼ãƒ–ãƒ« {DEFAULT_COLLECTION_NAME}_image_embedding ã®ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")

            # Drop and create indexes with existence check
            try:
                cursor.execute(
                    f"SELECT COUNT(*) FROM USER_INDEXES WHERE INDEX_NAME = '{DEFAULT_COLLECTION_NAME.upper()}_EMBED_DATA_IDX'")
                if cursor.fetchone()[0] > 0:
                    cursor.execute(f"DROP INDEX {DEFAULT_COLLECTION_NAME}_embed_data_idx")
                    print(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {DEFAULT_COLLECTION_NAME}_embed_data_idx ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
            except DatabaseError as e:
                print(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {DEFAULT_COLLECTION_NAME}_embed_data_idx ã®å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")

            try:
                cursor.execute(
                    f"SELECT COUNT(*) FROM USER_INDEXES WHERE INDEX_NAME = '{DEFAULT_COLLECTION_NAME.upper()}_IMAGE_EMBED_DATA_IDX'")
                if cursor.fetchone()[0] > 0:
                    cursor.execute(f"DROP INDEX {DEFAULT_COLLECTION_NAME}_image_embed_data_idx")
                    print(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {DEFAULT_COLLECTION_NAME}_image_embed_data_idx ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
            except DatabaseError as e:
                print(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {DEFAULT_COLLECTION_NAME}_image_embed_data_idx ã®å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")

            # Create indexes
            try:
                cursor.execute(create_index_sql)
                print(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {DEFAULT_COLLECTION_NAME}_embed_data_idx ã‚’ä½œæˆã—ã¾ã—ãŸ")
            except DatabaseError as e:
                print(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {DEFAULT_COLLECTION_NAME}_embed_data_idx ã®ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")

            try:
                cursor.execute(create_image_index_sql)
                print(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {DEFAULT_COLLECTION_NAME}_image_embed_data_idx ã‚’ä½œæˆã—ã¾ã—ãŸ")
            except DatabaseError as e:
                print(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {DEFAULT_COLLECTION_NAME}_image_embed_data_idx ã®ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")

            try:
                cursor.execute(drop_rag_qa_result_sql)
            except DatabaseError as e:
                print(f"{e}")

            try:
                cursor.execute(create_rag_qa_result_sql)
            except DatabaseError as e:
                print(f"{e}")

            try:
                cursor.execute(drop_rag_qa_feedback_sql)
            except DatabaseError as e:
                print(f"{e}")

            try:
                cursor.execute(create_rag_qa_feedback_sql)
            except DatabaseError as e:
                print(f"{e}")

            conn.commit()

    gr.Info("ãƒ†ãƒ¼ãƒ–ãƒ«ã®ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸ")
    return gr.Accordion(), gr.Textbox(value=output_sql_text.strip())


def test_oci_cred(test_query_text):
    test_query_vector = ""
    region = get_region()
    embed_genai_params = {
        "provider": "ocigenai",
        "credential_name": "OCI_CRED",
        "url": f"https://inference.generativeai.{region}.oci.oraclecloud.com/20231130/actions/embedText",
        "model": "cohere.embed-multilingual-v3.0"
    }

    with pool.acquire() as conn:
        with conn.cursor() as cursor:
            plsql = """
DECLARE
    l_embed_genai_params CLOB := :embed_genai_params;
    l_result SYS_REFCURSOR;
BEGIN
    OPEN l_result FOR
        SELECT et.*
        FROM dbms_vector_chain.utl_to_embeddings(:text_to_embed, JSON(l_embed_genai_params)) et;
    :result := l_result;
END; """

            result_cursor = cursor.var(oracledb.CURSOR)

            cursor.execute(plsql,
                           embed_genai_params=json.dumps(embed_genai_params),
                           text_to_embed=test_query_text,
                           result=result_cursor)

            # Fetch the results from the ref cursor
            with result_cursor.getvalue() as ref_cursor:
                result_rows = ref_cursor.fetchall()
                for row in result_rows:
                    if isinstance(row, tuple):
                        if isinstance(row[0], oracledb.LOB):
                            lob_content = row[0].read()
                            lob_json = json.loads(lob_content)
                            test_query_vector += str(lob_json["embed_vector"]) + "\n"

    return gr.Textbox(value=test_query_vector)


def convert_excel_to_text_document(file_path):
    has_error = False
    if not file_path:
        has_error = True
        gr.Warning("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„")
    if has_error:
        return gr.File(value=None)

    output_file_path = file_path.name + '.txt'
    df = pd.read_excel(file_path.name)
    data = df.to_dict('records')
    with open(output_file_path, 'w', encoding='utf-8') as f:
        for row in data:
            # Process each row to convert Timestamps to strings
            processed_row = {}
            for key, value in row.items():
                if isinstance(value, pd.Timestamp):
                    processed_row[key] = str(value)
                else:
                    processed_row[key] = value
            json_line = json.dumps(processed_row, ensure_ascii=False)
            f.write(json_line + ' <FIXED_DELIMITER>\n')
    return (
        gr.File(),
        gr.File(value=output_file_path)
    )


def convert_to_markdown_document(file_path, use_llm, llm_prompt):
    has_error = False
    if not file_path:
        has_error = True
        gr.Warning("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„")
    if has_error:
        return gr.File(value=None)

    output_file_path = file_path.name + '.md'
    md = MarkItDown()

    file_extension = os.path.splitext(file_path.name)[-1].lower()
    if file_extension in ['.jpg', '.jpeg', '.png', '.ppt', '.pptx'] and use_llm:
        region = get_region()
        model_id = "meta.llama-3.2-90b-vision-instruct"
        if region == "us-chicago-1":
            model_id = "meta.llama-4-scout-17b-16e-instruct"
        client = ChatOCIGenAI(
            model_id="meta.llama-3.2-90b-vision-instruct",
            provider="meta",
            service_endpoint=f"https://inference.generativeai.{region}.oci.oraclecloud.com",
            compartment_id=os.environ["OCI_COMPARTMENT_OCID"],
            model_kwargs={"temperature": 0.0, "top_p": 0.75, "seed": 42, "max_tokens": 3600},
        )
        md = MarkItDown(llm_client=client, llm_model="meta.llama-3.2-90b-vision-instruct")
        result = md.convert(
            file_path.name,
            llm_prompt=llm_prompt,
        )
        print(f"{result.text_content=}")
    else:
        result = md.convert(
            file_path.name,
        )
    with open(output_file_path, 'w', encoding='utf-8') as f:
        f.write(result.text_content)
    return (
        gr.File(),
        gr.File(value=output_file_path)
    )


def load_document(file_path, server_directory, document_metadata):
    print("in load_document() start...")
    has_error = False
    if not file_path:
        has_error = True
        gr.Warning("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„")
    if document_metadata:
        document_metadata = document_metadata.strip()
        if "=" not in document_metadata or '"' in document_metadata or "'" in document_metadata or '\\' in document_metadata:
            has_error = True
            gr.Warning("ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚key1=value1,key2=value2,... ã®å½¢å¼ã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        else:
            metadatas = document_metadata.split(",")
            for metadata in metadatas:
                if "=" not in metadata:
                    has_error = True
                    gr.Warning(
                        "ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚key1=value1,key2=value2,... ã®å½¢å¼ã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
                    break
    if has_error:
        return gr.Textbox(value=""), gr.Textbox(value=""), gr.Textbox(value=""), gr.Textbox(value="")

    if not os.path.exists(server_directory):
        os.makedirs(server_directory)
    doc_id = generate_unique_id("doc_")
    file_name = os.path.basename(file_path.name)
    file_extension = os.path.splitext(file_name)
    if isinstance(file_extension, tuple):
        file_extension = file_extension[1]
    server_path = os.path.join(server_directory, f"{doc_id}_{file_name}")
    shutil.copy(file_path.name, server_path)

    collection_cmeta = {}
    if file_extension == ".md":
        loader = TextLoader(server_path)
        documents = loader.load()
        original_contents = "".join(doc.page_content for doc in documents)
        pages_count = len(documents)
    else:
        # https://docs.unstructured.io/open-source/core-functionality/overview
        pages_count = 1
        elements = partition(filename=server_path, strategy='fast',
                             languages=["jpn", "eng", "chi_sim"],
                             extract_image_block_types=["Table"],
                             extract_image_block_to_payload=False,
                             skip_infer_table_types=["pdf", "jpg", "png", "heic", "doc", "docx"])
        for el in elements:
            print(f"{el=}")
        original_contents = " \n".join(el.text.replace('\x0b', '\n') for el in elements)
        print(f"{original_contents=}")

    collection_cmeta['file_name'] = file_name
    collection_cmeta['source'] = server_path
    collection_cmeta['server_path'] = server_path
    if document_metadata:
        metadatas = document_metadata.split(",")
        for metadata in metadatas:
            key, value = metadata.split("=")
            collection_cmeta[key] = value

    with pool.acquire() as conn:
        with conn.cursor() as cursor:
            cursor.setinputsizes(**{"data": oracledb.BLOB})
            load_document_sql = f"""
 -- (Only for Reference) Insert to table {DEFAULT_COLLECTION_NAME}_collection
 INSERT INTO {DEFAULT_COLLECTION_NAME}_collection(id, data, cmetadata)
 VALUES (:id, to_blob(:data), :cmetadata) """
            output_sql_text = load_document_sql.replace(":id", "'" + str(doc_id) + "'")
            output_sql_text = output_sql_text.replace(":data", "'...'")
            output_sql_text = output_sql_text.replace(":cmetadata", "'" + json.dumps(collection_cmeta) + "'") + ";"
            cursor.execute(load_document_sql, {
                'id': doc_id,
                'data': original_contents,
                'cmetadata': json.dumps(collection_cmeta)
            })
            conn.commit()

    return (
        gr.Textbox(value=output_sql_text.strip()),
        gr.Textbox(value=doc_id),
        gr.Textbox(value=str(pages_count)),
        gr.Textbox(value=original_contents)
    )


# def split_document_by_oracle(doc_id, chunks_by, chunks_max_size,
#                              chunks_overlap_size,
#                              chunks_split_by, chunks_split_by_custom,
#                              chunks_language, chunks_normalize,
#                              chunks_normalize_options):
#     # print(f"{chunks_normalize_options=}")
#     with pool.acquire() as conn:
#         with conn.cursor() as cursor:
#             parameter_str = '{"by": "' + chunks_by + '","max": "' + str(
#                 chunks_max_size) + '", "overlap": "' + str(
#                 int(int(
#                     chunks_max_size) * chunks_overlap_size / 100)) + '", "split": "' + chunks_split_by + '", '
#             if chunks_split_by == "CUSTOM":
#                 parameter_str += '"custom_list": [' + chunks_split_by_custom + '], '
#             parameter_str += '"language": "' + chunks_language + '", '
#             if chunks_normalize == "NONE" or chunks_normalize == "ALL":
#                 parameter_str += '"normalize": "' + chunks_normalize + '"}'
#             else:
#                 parameter_str += '"normalize": "' + chunks_normalize + '", "norm_options": [' + ", ".join(
#                     ['"' + s + '"' for s in chunks_normalize_options]) + ']}'
#
#             # print(f"{parameter_str}")
#             select_chunks_sql = f"""
# -- Select chunks
# SELECT
#     json_value(ct.column_value, '$.chunk_id')     AS chunk_id,
#     json_value(ct.column_value, '$.chunk_offset') AS chunk_offset,
#     json_value(ct.column_value, '$.chunk_length') AS chunk_length,
#     json_value(ct.column_value, '$.chunk_data')   AS chunk_data
# FROM
#     {DEFAULT_COLLECTION_NAME}_collection dt,
#     dbms_vector_chain.utl_to_chunks(
#         dbms_vector_chain.utl_to_text(dt.data),
#         JSON(
#             :parameter_str
#         )
#     ) ct
#     CROSS JOIN
#         JSON_TABLE ( ct.column_value, '$[*]'
#             COLUMNS (
#                 column_value VARCHAR2 ( 4000 ) PATH '$'
#             )
#         )
# WHERE
#     dt.id = :doc_id """
#             # cursor.setinputsizes(oracledb.DB_TYPE_VECTOR)
#             utl_to_chunks_sql_output = "\n" + select_chunks_sql.replace(':parameter_str',
#                                                                         "'" + parameter_str + "'").replace(
#                 ':doc_id', "'" + str(doc_id) + "'") + ";"
#             # print(f"{utl_to_chunks_sql_output=}")
#             cursor.execute(select_chunks_sql,
#                            [parameter_str, doc_id])
#             chunks = []
#             for row in cursor:
#                 chunks.append({'CHUNK_ID': row[0],
#                                'CHUNK_OFFSET': row[1], 'CHUNK_LENGTH': row[2], 'CHUNK_DATA': row[3]})
#
#             conn.commit()
#
#     chunks_dataframe = pd.DataFrame(chunks)
#     return (
#         gr.Textbox(utl_to_chunks_sql_output.strip()),
#         gr.Textbox(value=str(len(chunks_dataframe))),
#         gr.Dataframe(value=chunks_dataframe)
#     )


def reset_document_chunks_result_dataframe():
    return (
        gr.Dataframe(value=None, row_count=(1, "fixed"))
    )


def process_image_blocks(doc_id, doc_data, chunk_size=None, chunk_overlap=None):
    """
    ç”»åƒãƒ–ãƒ­ãƒƒã‚¯ã‚’å‡¦ç†ã—ã¦{DEFAULT_COLLECTION_NAME}_imageãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜ã—ã€
    text_splitterã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ã—{DEFAULT_COLLECTION_NAME}_image_embeddingãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜ã™ã‚‹

    Args:
        doc_id: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆID
        doc_data: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿
        chunk_size: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
        chunk_overlap: ãƒãƒ£ãƒ³ã‚¯ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—
    """
    # ç”»åƒãƒ–ãƒ­ãƒƒã‚¯ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
    image_blocks = re.findall(r'<!-- image_begin -->(.*?)<!-- image_end -->', doc_data, re.DOTALL)

    if not image_blocks:
        return

    with pool.acquire() as conn:
        with conn.cursor() as cursor:
            # æ—¢å­˜ã®ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤
            delete_image_sql = f"""
DELETE FROM {DEFAULT_COLLECTION_NAME}_image WHERE doc_id = :doc_id
"""
            cursor.execute(delete_image_sql, [doc_id])

            # æ—¢å­˜ã®ç”»åƒembedding ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤
            delete_image_embedding_sql = f"""
DELETE FROM {DEFAULT_COLLECTION_NAME}_image_embedding WHERE doc_id = :doc_id
"""
            cursor.execute(delete_image_embedding_sql, [doc_id])

            # ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥ã™ã‚‹SQL
            insert_image_sql = f"""
INSERT INTO {DEFAULT_COLLECTION_NAME}_image (
    doc_id,
    img_id,
    text_data,
    vlm_data,
    base64_data
) VALUES (:doc_id, :img_id, :text_data, :vlm_data, :base64_data)
"""

            img_id = 1
            for image_block in image_blocks:
                # OCRã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
                text_data_match = re.search(r'<!-- image_ocr_content_begin -->(.*?)<!-- image_ocr_content_end -->',
                                            image_block, re.DOTALL)
                text_data = text_data_match.group(1).strip() if text_data_match else ""

                # VLMèª¬æ˜ã‚’æŠ½å‡º
                vlm_data_match = re.search(
                    r'<!-- image_vlm_description_begin -->(.*?)<!-- image_vlm_description_end -->', image_block,
                    re.DOTALL)
                vlm_data = vlm_data_match.group(1).strip() if vlm_data_match else ""

                # Base64ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                base64_data_match = re.search(r'<!-- image_base64_begin -->(.*?)<!-- image_base64_end -->', image_block,
                                              re.DOTALL)
                base64_data = base64_data_match.group(1).strip() if base64_data_match else ""

                # base64_dataã‹ã‚‰å‰å¾Œã®HTMLã‚³ãƒ¡ãƒ³ãƒˆè¨˜å·ã‚’é™¤å»
                if base64_data:
                    # å‰å¾Œã® <!-- ã¨ --> ã‚’é™¤å»
                    base64_data = re.sub(r'^<!--\s*', '', base64_data)
                    base64_data = re.sub(r'\s*-->$', '', base64_data)
                    base64_data = base64_data.strip()

                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æŒ¿å…¥
                cursor.execute(insert_image_sql, {
                    'doc_id': doc_id,
                    'img_id': img_id,
                    'text_data': text_data,
                    'vlm_data': vlm_data,
                    'base64_data': base64_data
                })

                print(f"ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: doc_id={doc_id}, img_id={img_id}")
                img_id += 1

            conn.commit()
            print(f"åˆè¨ˆ {len(image_blocks)} å€‹ã®ç”»åƒãƒ–ãƒ­ãƒƒã‚¯ã‚’å‡¦ç†ã—ã¾ã—ãŸ")

            # ç”»åƒãƒ‡ãƒ¼ã‚¿ã®splitå‡¦ç†ã‚’å®Ÿè¡Œ
            _process_image_data_splitting(doc_id, cursor, chunk_size, chunk_overlap)
            conn.commit()


def _process_image_data_splitting(doc_id, cursor, chunk_size=None, chunk_overlap=None):
    """
    {DEFAULT_COLLECTION_NAME}_imageãƒ†ãƒ¼ãƒ–ãƒ«ã®text_dataã¨vlm_dataã‚’å€‹åˆ¥ã«text_splitterã§åˆ†å‰²ã—ã€
    {DEFAULT_COLLECTION_NAME}_image_embeddingãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜ã™ã‚‹

    Args:
        doc_id: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆID
        cursor: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚«ãƒ¼ã‚½ãƒ«
        chunk_size: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
        chunk_overlap: ãƒãƒ£ãƒ³ã‚¯ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—
    """
    # ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    select_image_sql = f"""
SELECT img_id, text_data, vlm_data FROM {DEFAULT_COLLECTION_NAME}_image
WHERE doc_id = :doc_id ORDER BY img_id
"""
    cursor.execute(select_image_sql, [doc_id])
    image_records = cursor.fetchall()

    if not image_records:
        print(f"ç”»åƒãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: doc_id={doc_id}")
        return

    # text_splitterã‚’åˆæœŸåŒ–
    if chunk_size is not None and chunk_overlap is not None:
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=256,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®chunk_size
            chunk_overlap=25  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®overlapï¼ˆç´„10%ï¼‰
        )

    # ç”»åƒembedding ãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥ã™ã‚‹SQL
    insert_embedding_sql = f"""
INSERT INTO {DEFAULT_COLLECTION_NAME}_image_embedding (
    doc_id,
    embed_id,
    embed_data,
    embed_vector,
    cmetadata,
    img_id
) VALUES (:doc_id, :embed_id, :embed_data, :embed_vector, :cmetadata, :img_id)
"""

    # text_dataã¨vlm_dataã§ç‹¬ç«‹ã—ãŸembed_idã‚’ä½¿ç”¨
    text_embed_id = 1
    vlm_embed_id = 1
    total_chunks = 0

    for img_id, text_data, vlm_data in image_records:
        # LOBã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ–‡å­—åˆ—ã«å¤‰æ›
        text_data_str = text_data.read() if text_data else ""
        vlm_data_str = vlm_data.read() if vlm_data else ""

        # text_dataã‚’å€‹åˆ¥ã«å‡¦ç†
        if text_data_str and text_data_str.strip():
            text_chunks = text_splitter.split_text(text_data_str)

            if text_chunks:
                # text_dataã®chunkã«å¯¾ã—ã¦embeddingã‚’ç”Ÿæˆã—ã¦ä¿å­˜
                text_chunk_texts = [chunk for chunk in text_chunks if chunk.strip()]
                if text_chunk_texts:
                    text_embed_vectors = generate_embedding_response(text_chunk_texts)

                    for i, (chunk_text, embed_vector) in enumerate(zip(text_chunk_texts, text_embed_vectors)):
                        # text_dataç”¨ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
                        cmetadata = json.dumps({
                            "img_id": img_id,
                            "chunk_index": i,
                            "source": "image_processing",
                            "data_type": "text_data",
                            "original_img_id": img_id
                        }, ensure_ascii=False)

                        cursor.setinputsizes(embed_vector=oracledb.DB_TYPE_VECTOR)
                        cursor.execute(insert_embedding_sql, {
                            'doc_id': doc_id,
                            'embed_id': text_embed_id,
                            'embed_data': chunk_text,
                            'embed_vector': embed_vector,
                            'cmetadata': cmetadata,
                            'img_id': img_id
                        })

                        print(
                            f"ç”»åƒtext_data embeddingãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: doc_id={doc_id}, img_id={img_id}, embed_id={text_embed_id}")
                        text_embed_id += 1
                        total_chunks += 1

        # vlm_dataã‚’å€‹åˆ¥ã«å‡¦ç†
        if vlm_data_str and vlm_data_str.strip():
            vlm_chunks = text_splitter.split_text(vlm_data_str)

            if vlm_chunks:
                # vlm_dataã®chunkã«å¯¾ã—ã¦embeddingã‚’ç”Ÿæˆã—ã¦ä¿å­˜
                vlm_chunk_texts = [chunk for chunk in vlm_chunks if chunk.strip()]
                if vlm_chunk_texts:
                    vlm_embed_vectors = generate_embedding_response(vlm_chunk_texts)

                    for i, (chunk_text, embed_vector) in enumerate(zip(vlm_chunk_texts, vlm_embed_vectors)):
                        # vlm_dataç”¨ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
                        cmetadata = json.dumps({
                            "img_id": img_id,
                            "chunk_index": i,
                            "source": "image_processing",
                            "data_type": "vlm_data",
                            "original_img_id": img_id
                        }, ensure_ascii=False)

                        cursor.setinputsizes(embed_vector=oracledb.DB_TYPE_VECTOR)
                        cursor.execute(insert_embedding_sql, {
                            'doc_id': doc_id,
                            'embed_id': vlm_embed_id,
                            'embed_data': chunk_text,
                            'embed_vector': embed_vector,
                            'cmetadata': cmetadata,
                            'img_id': img_id
                        })

                        print(
                            f"ç”»åƒvlm_data embeddingãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: doc_id={doc_id}, img_id={img_id}, embed_id={vlm_embed_id}")
                        vlm_embed_id += 1
                        total_chunks += 1

        # ä¸¡æ–¹ã®ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®å ´åˆã®è­¦å‘Š
        if (not text_data_str or not text_data_str.strip()) and (not vlm_data_str or not vlm_data_str.strip()):
            print(f"ç”»åƒ {img_id} ã«text_dataã¨vlm_dataã®ä¸¡æ–¹ãŒç©ºã§ã™")

    print(f"ç”»åƒãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ: åˆè¨ˆ {total_chunks} å€‹ã®chunkã‚’ç”Ÿæˆ")


def reset_document_chunks_result_detail():
    return (
        gr.Textbox(value=""),
        gr.Textbox(value="")
    )


def split_document_by_unstructured(doc_id, chunks_by, chunks_max_size,
                                   chunks_overlap_size,
                                   chunks_split_by, chunks_split_by_custom,
                                   chunks_language, chunks_normalize,
                                   chunks_normalize_options):
    has_error = False
    if not doc_id:
        has_error = True
        gr.Warning("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’é¸æŠã—ã¦ãã ã•ã„")
    if has_error:
        return (
            gr.Textbox(value=""),
            gr.Textbox(value=""),
            gr.Dataframe(value=None, row_count=(1, "fixed"))
        )
    # print(f"{chunks_normalize_options=}")
    output_sql = ""
    server_path = get_server_path(doc_id)

    chunks_overlap = int(float(chunks_max_size) * (float(chunks_overlap_size) / 100))
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunks_max_size - chunks_overlap,
        chunk_overlap=chunks_overlap
    )

    # Check if it's a .md file and get content from database instead of reading the file
    doc_data = ""
    if server_path.lower().endswith('.md'):
        loader = TextLoader(server_path)
        documents = loader.load()
        doc_data = "\n".join(doc.page_content for doc in documents)

        # Check if doc_data contains image blocks and extract OCR content if needed
        if re.search(r'<!-- image_begin -->.*?<!-- image_end -->', doc_data, re.DOTALL):
            # Extract all OCR content blocks
            text_contexts = re.findall(r'<!-- image_ocr_content_begin -->(.*?)<!-- image_ocr_content_end -->', doc_data,
                                       re.DOTALL)
            doc_data = "\n".join(ocr.strip() for ocr in text_contexts if ocr.strip())
    else:
        # If we can't get data from database, fall back to reading the file
        elements = partition(filename=server_path, strategy='fast',
                             languages=["jpn", "eng", "chi_sim"],
                             extract_image_block_types=["Table"],
                             extract_image_block_to_payload=False,
                             skip_infer_table_types=["pdf", "jpg", "png", "heic", "doc", "docx"])
        # Use claude to get table data
        # page_table_documents = parser_pdf(server_path)
        page_table_documents = {}
        # elements = partition(filename=server_path,
        #                      strategy='hi_res',
        #                      languages=["jpn", "eng", "chi_sim"]
        #                      )
        # for issue: https://github.com/Unstructured-IO/unstructured/issues/3396
        elements = partition(filename=server_path, strategy='fast',
                             languages=["jpn", "eng", "chi_sim"],
                             extract_image_block_types=["Table"],
                             extract_image_block_to_payload=False,
                             # skip_infer_table_types=["pdf", "ppt", "pptx", "doc", "docx", "xls", "xlsx"])
                             skip_infer_table_types=["pdf", "jpg", "png", "heic", "doc", "docx"])
        prev_page_number = 0
        table_idx = 1
        for el in elements:
            # print(f"{el.category=}")
            # print(f"{el.text=}")
            # print(f"{el.metadata.page_number=}")
            page_number = el.metadata.page_number
            if prev_page_number != page_number:
                prev_page_number = page_number
                table_idx = 1
            # print(f"{type(el.category)=}")
            if el.category == "Table":
                table_id = f"page_{page_number}_table_{table_idx}"
                print(f"{table_id=}")
                print(f"{page_table_documents=}")
                if page_table_documents:
                    page_table_document = get_dict_value(page_table_documents, table_id)
                    print(f"{page_table_document=}")
                    if page_table_document:
                        page_content = get_dict_value(page_table_document, "page_content")
                        table_to_markdown = get_dict_value(page_table_document, "table_to_markdown")
                        if page_content and table_to_markdown:
                            print(f"Before: {el.text=}")
                            el.text = page_content + "\n" + table_to_markdown + "\n"
                            print(f"After: {el.text=}")
                table_idx += 1
            for element in elements:
                # Convert element.text to string to avoid TypeError with LOB objects
                element.text = str(element.text).replace('\x0b', '\n')
            doc_data = " \n".join([str(element.text) for element in elements])

    unstructured_chunks = text_splitter.split_text(doc_data)

    chunks = process_text_chunks(unstructured_chunks)
    chunks_dataframe = pd.DataFrame(chunks)

    with pool.acquire() as conn:
        with conn.cursor() as cursor:
            delete_sql = f"""
-- Delete chunks
DELETE FROM {DEFAULT_COLLECTION_NAME}_embedding WHERE doc_id = :doc_id """
            cursor.execute(delete_sql,
                           [doc_id])
            output_sql += delete_sql.replace(':doc_id', "'" + str(doc_id) + "'").lstrip() + ";"

            save_chunks_sql = f"""
-- (Only for Reference) Insert chunks
INSERT INTO {DEFAULT_COLLECTION_NAME}_embedding (
doc_id,
embed_id,
embed_data
)
VALUES (:doc_id, :embed_id, :embed_data) """
            output_sql += "\n" + save_chunks_sql.replace(':doc_id', "'" + str(doc_id) + "'"
                                                         ).replace(':embed_id', "'...'"
                                                                   ).replace(':embed_data', "'...'"
                                                                             ) + ";"
            print(f"{output_sql=}")
            # ãƒãƒƒãƒæŒ¿å…¥ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
            data_to_insert = [(doc_id, chunk['CHUNK_ID'], chunk['CHUNK_DATA']) for chunk in chunks]

            # ãƒãƒƒãƒæŒ¿å…¥ã‚’å®Ÿè¡Œ
            cursor.executemany(save_chunks_sql, data_to_insert)
            conn.commit()

    return (
        gr.Textbox(value=output_sql),
        gr.Textbox(value=str(len(chunks_dataframe))),
        gr.Dataframe(value=chunks_dataframe, row_count=(len(chunks_dataframe), "fixed"))
    )


def on_select_split_document_chunks_result(evt: gr.SelectData, df: pd.DataFrame):
    print("on_select_split_document_chunks_result() start...")
    selected_index = evt.index[0]  # é¸æŠã•ã‚ŒãŸè¡Œã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
    selected_row = df.iloc[selected_index]  # é¸æŠã•ã‚ŒãŸè¡Œã®ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    return selected_row['CHUNK_ID'], \
        selected_row['CHUNK_DATA']


def update_document_chunks_result_detail(doc_id, df: pd.DataFrame, chunk_id, chunk_data):
    print("in update_document_chunks_result_detail() start...")
    has_error = False
    if not doc_id:
        has_error = True
        gr.Warning("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’é¸æŠã—ã¦ãã ã•ã„")
    if not chunk_data or chunk_data.strip() == "":
        has_error = True
        gr.Warning("CHUNK_DATAã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if has_error:
        return (
            gr.Dataframe(),
            gr.Textbox(),
            gr.Textbox(),
        )

    chunk_data = chunk_data.strip()

    with pool.acquire() as conn:
        with conn.cursor() as cursor:
            update_sql = f"""
UPDATE {DEFAULT_COLLECTION_NAME}_embedding
SET embed_data = :embed_data, embed_vector = :embed_vector
WHERE doc_id = :doc_id and embed_id = :embed_id
"""
            embed_vector = generate_embedding_response([chunk_data])[0]
            cursor.setinputsizes(embed_vector=oracledb.DB_TYPE_VECTOR)
            cursor.execute(update_sql,
                           {'doc_id': doc_id, 'embed_id': chunk_id, 'embed_data': chunk_data,
                            'embed_vector': embed_vector})
            conn.commit()

    print(f"{chunk_id=}")
    print(f"{chunk_data=}")

    updated_df = df.copy()
    mask = updated_df['CHUNK_ID'] == int(chunk_id)
    updated_df.loc[mask, 'CHUNK_DATA'] = chunk_data
    updated_df.loc[mask, 'CHUNK_LENGTH'] = len(chunk_data)
    # print(f"{mask.sum()} rows updated")
    print(f"{updated_df=}")

    return (
        gr.Dataframe(value=updated_df),
        gr.Textbox(),
        gr.Textbox(value=chunk_data),
    )


def embed_save_document_by_unstructured(doc_id, chunks_by, chunks_max_size,
                                        chunks_overlap_size,
                                        chunks_split_by, chunks_split_by_custom,
                                        chunks_language, chunks_normalize,
                                        chunks_normalize_options):
    has_error = False
    if not doc_id:
        has_error = True
        gr.Warning("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’é¸æŠã—ã¦ãã ã•ã„")
    if has_error:
        return (
            gr.Textbox(value=""),
            gr.Textbox(value=""),
            gr.Dataframe(value=None, row_count=(1, "fixed"))
        )

    # ç”»åƒãƒ–ãƒ­ãƒƒã‚¯ã‚’å‡¦ç†ã—ã¦{DEFAULT_COLLECTION_NAME}_imageãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜
    server_path = get_server_path(doc_id)
    chunks_overlap = int(float(chunks_max_size) * (float(chunks_overlap_size) / 100))

    # Check if it's a .md file and get content to process image blocks
    if server_path.lower().endswith('.md'):
        loader = TextLoader(server_path)
        documents = loader.load()
        doc_data = "\n".join(doc.page_content for doc in documents)

        # Check if doc_data contains image blocks and process them
        if re.search(r'<!-- image_begin -->.*?<!-- image_end -->', doc_data, re.DOTALL):
            process_image_blocks(doc_id, doc_data,
                                 chunk_size=chunks_max_size - chunks_overlap,
                                 chunk_overlap=chunks_overlap)

    output_sql = ""
    with pool.acquire() as conn:
        with conn.cursor() as cursor:
            select_sql = f"""
SELECT doc_id, embed_id, embed_data FROM {DEFAULT_COLLECTION_NAME}_embedding  WHERE doc_id = :doc_id
"""
            cursor.execute(select_sql, doc_id=doc_id)
            records = cursor.fetchall()
            embed_datas = [record[2] for record in records]
            embed_vectors = generate_embedding_response(embed_datas)
            update_sql = f"""
UPDATE {DEFAULT_COLLECTION_NAME}_embedding
SET embed_vector = :embed_vector
WHERE doc_id = :doc_id and embed_id = :embed_id
"""

            # update_sql = f"""
            # -- Update chunks
            # UPDATE {DEFAULT_COLLECTION_NAME}_embedding
            # SET embed_vector = dbms_vector.utl_to_embedding(embed_data, json('{{"provider": "ocigenai", "credential_name": "OCI_CRED", "url": "https://inference.generativeai.us-chicago-1.oci.oraclecloud.com/20231130/actions/embedText", "model": "cohere.embed-multilingual-v3.0"}}'))
            # WHERE doc_id = :doc_id """
            # cursor.execute(update_sql,
            #                [doc_id])
            cursor.setinputsizes(embed_vector=oracledb.DB_TYPE_VECTOR)
            output_sql += update_sql.replace(':doc_id', "'" + str(doc_id) + "'"
                                             ).replace(':embed_id', "'" + str('...') + "'"
                                                       ).replace(':embed_vector', "'" + str('...') + "'").strip() + ";"
            print(f"{output_sql=}")
            cursor.executemany(update_sql,
                               [{'doc_id': record[0], 'embed_id': record[1], 'embed_vector': embed_vector}
                                for record, embed_vector in zip(records, embed_vectors)])
            conn.commit()
    return (
        gr.Textbox(output_sql),
        gr.Textbox(),
        gr.Dataframe()
    )


def generate_query(query_text, generate_query_radio):
    has_error = False
    if not query_text:
        has_error = True
        gr.Warning("ã‚¯ã‚¨ãƒªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if has_error:
        return gr.Textbox(value=""), gr.Textbox(value=""), gr.Textbox(value="")

    generate_query1 = ""
    generate_query2 = ""
    generate_query3 = ""

    if generate_query_radio == "None":
        return gr.Textbox(value=generate_query1), gr.Textbox(value=generate_query2), gr.Textbox(value=generate_query3)

    region = get_region()
    if region == "us-chicago-1":
        chat_llm = ChatOCIGenAI(
            model_id="xai.grok-3",
            provider="xai",
            service_endpoint=f"https://inference.generativeai.{region}.oci.oraclecloud.com",
            compartment_id=os.environ["OCI_COMPARTMENT_OCID"],
            model_kwargs={"temperature": 0.0, "top_p": 0.75, "seed": 42, "max_tokens": 600},
        )
    else:
        chat_llm = ChatOCIGenAI(
            model_id="cohere.command-a-03-2025",
            provider="cohere",
            service_endpoint=f"https://inference.generativeai.{region}.oci.oraclecloud.com",
            compartment_id=os.environ["OCI_COMPARTMENT_OCID"],
            model_kwargs={"temperature": 0.0, "top_p": 0.75, "seed": 42, "max_tokens": 600},
        )

    # RAG-Fusion
    if generate_query_radio == "Sub-Query":
        # v1
        # sub_query_prompt = ChatPromptTemplate.from_messages([
        #     ("system",
        #      """
        #      You are an advanced assistant that specializes in breaking down complex, multifaceted input queries into more manageable sub-queries.
        #      This approach allows for each aspect of the query to be explored in depth, facilitating a comprehensive and nuanced response.
        #      Your task is to dissect the given query into its constituent elements and generate targeted sub-queries that can each be researched or answered individually,
        #      ensuring that the final response holistically addresses all components of the original query.
        #      """),
        #     ("user",
        #      "Decompose the following query into targeted sub-queries that can be individually explored: {original_query} \n OUTPUT (2 queries): )")
        # ])
        # v2
        sub_query_prompt = ChatPromptTemplate.from_messages([
            ("system", get_sub_query_prompt()),
            ("user", get_query_generation_prompt("Sub-Query", "{original_query}"))
        ])

        generate_sub_queries_chain = (
                sub_query_prompt | chat_llm | StrOutputParser() | (lambda x: x.split("\n"))
        )
        sub_queries = generate_sub_queries_chain.invoke({"original_query": query_text})
        print(f"{sub_queries=}")

        if isinstance(sub_queries, list):
            generate_query1 = re.sub(r'^1\. ', '', sub_queries[0])
            generate_query2 = re.sub(r'^2\. ', '', sub_queries[1])
            generate_query3 = re.sub(r'^3\. ', '', sub_queries[2])
    elif generate_query_radio == "RAG-Fusion":
        # v1
        # rag_fusion_prompt = ChatPromptTemplate.from_messages([
        #     ("system",
        #      "You are a helpful assistant that generates multiple similary search queries based on a single input query."),
        #     ("user", "Generate multiple search queries related to: {original_query} \n OUTPUT (2 queries):")
        # ])
        # v2
        rag_fusion_prompt = ChatPromptTemplate.from_messages([
            ("system", get_rag_fusion_prompt()),
            ("user", get_query_generation_prompt("RAG-Fusion", "{original_query}"))
        ])

        generate_rag_fusion_queries_chain = (
                rag_fusion_prompt | chat_llm | StrOutputParser() | (lambda x: x.split("\n"))
        )
        rag_fusion_queries = generate_rag_fusion_queries_chain.invoke({"original_query": query_text})
        print(f"{rag_fusion_queries=}")

        if isinstance(rag_fusion_queries, list):
            generate_query1 = re.sub(r'^1\. ', '', rag_fusion_queries[0])
            generate_query2 = re.sub(r'^2\. ', '', rag_fusion_queries[1])
            generate_query3 = re.sub(r'^3\. ', '', rag_fusion_queries[2])
    elif generate_query_radio == "HyDE":
        hyde_prompt = ChatPromptTemplate.from_messages([
            ("system", get_hyde_prompt()),
            ("user", get_query_generation_prompt("HyDE", "{original_query}"))
        ])

        generate_hyde_answers_chain = (
                hyde_prompt | chat_llm | StrOutputParser() | (lambda x: x.split("\n"))
        )
        hyde_answers = generate_hyde_answers_chain.invoke({"original_query": query_text})
        print(f"{hyde_answers=}")

        if isinstance(hyde_answers, list):
            generate_query1 = re.sub(r'^1\. ', '', hyde_answers[0])
            generate_query2 = re.sub(r'^2\. ', '', hyde_answers[1])
            generate_query3 = re.sub(r'^3\. ', '', hyde_answers[2])
    elif generate_query_radio == "Step-Back-Prompting":
        step_back_prompt = ChatPromptTemplate.from_messages([
            ("system", get_step_back_prompt()),
            ("user", get_query_generation_prompt("Step-Back-Prompting", "{original_query}"))
        ])

        generate_step_back_queries_chain = (
                step_back_prompt | chat_llm | StrOutputParser() | (lambda x: x.split("\n"))
        )
        step_back_queries = generate_step_back_queries_chain.invoke({"original_query": query_text})
        print(f"{step_back_queries=}")

        if isinstance(step_back_queries, list):
            generate_query1 = re.sub(r'^1\. ', '', step_back_queries[0])
            generate_query2 = re.sub(r'^2\. ', '', step_back_queries[1])
            generate_query3 = re.sub(r'^3\. ', '', step_back_queries[2])
    elif generate_query_radio == "Customized-Multi-Step-Query":
        region = get_region()
        select_multi_step_query_sql = f"""
                SELECT json_value(dc.cmetadata, '$.file_name') name, de.embed_id embed_id, de.embed_data embed_data, de.doc_id doc_id
                FROM {DEFAULT_COLLECTION_NAME}_embedding de, {DEFAULT_COLLECTION_NAME}_collection dc
                WHERE de.doc_id = dc.id
                ORDER BY vector_distance(de.embed_vector , (
                        SELECT to_vector(et.embed_vector) embed_vector
                        FROM
                            dbms_vector_chain.utl_to_embeddings(:query_text, JSON('{{"provider": "ocigenai", "credential_name": "OCI_CRED", "url": "https://inference.generativeai.{region}.oci.oraclecloud.com/20231130/actions/embedText", "model": "cohere.embed-multilingual-v3.0"}}')) t,
                            JSON_TABLE ( t.column_value, '$[*]'
                                    COLUMNS (
                                        embed_id NUMBER PATH '$.embed_id',
                                        embed_data VARCHAR2 ( 4000 ) PATH '$.embed_data',
                                        embed_vector CLOB PATH '$.embed_vector'
                                    )
                                )
                            et), COSINE)
            """
        select_multi_step_query_sql += "FETCH FIRST 3 ROWS ONLY"
        # Prepare parameters for SQL execution
        multi_step_query_params = {
            "query_text": query_text
        }

        # For debugging: Print the final SQL command.
        # Assuming complete_sql is your SQL string with placeholders like :extend_around_chunk_size, :doc_ids, etc.
        query_sql_output = select_multi_step_query_sql

        # Manually replace placeholders with parameter values for debugging
        for key, value in multi_step_query_params.items():
            placeholder = f":{key}"
            # For the purpose of display, ensure the value is properly quoted if it's a string
            display_value = f"'{value}'" if isinstance(value, str) else str(value)
            query_sql_output = query_sql_output.replace(placeholder, display_value)

        # Now query_sql_output contains the SQL command with parameter values inserted
        print(f"\nQUERY_SQL_OUTPUT:\n{query_sql_output}")
        with pool.acquire() as conn:
            with conn.cursor() as cursor:
                cursor.execute(select_multi_step_query_sql, multi_step_query_params)
                multi_step_queries = []
                for row in cursor:
                    # print(f"row: {row}")
                    multi_step_queries.append(row[2])
                print(f"{multi_step_queries=}")

        if isinstance(multi_step_queries, list):
            generate_query1 = multi_step_queries[0]
            generate_query2 = multi_step_queries[1]
            generate_query3 = multi_step_queries[2]

    return (
        gr.Textbox(value=generate_query1),
        gr.Textbox(value=generate_query2),
        gr.Textbox(value=generate_query3)
    )


def search_document(
        reranker_model_radio_input,
        reranker_top_k_slider_input,
        reranker_threshold_slider_input,
        threshold_value_slider_input,
        top_k_slider_input,
        doc_id_all_checkbox_input,
        doc_id_checkbox_group_input,
        text_search_checkbox_input,
        text_search_k_slider_input,
        document_metadata_text_input,
        query_text_input,
        sub_query1_text_input,
        sub_query2_text_input,
        sub_query3_text_input,
        partition_by_k_slider_input,
        answer_by_one_checkbox_input,
        extend_first_chunk_size_input,
        extend_around_chunk_size_input,
        use_image
):
    """
    Retrieve relevant splits for any question using similarity search.
    This is simply "top K" retrieval where we select documents based on embedding similarity to the query.
    """
    # ç”»åƒã‚’ä½¿ã£ã¦å›ç­”ãŒã‚ªãƒ³ã®å ´åˆã€ç‰¹å®šã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å€¤ã‚’å¼·åˆ¶ä½¿ç”¨
    if use_image:
        answer_by_one_checkbox_input = False
        extend_first_chunk_size_input = 0
        extend_around_chunk_size_input = 0
        print(
            f"ç”»åƒå›ç­”ãƒ¢ãƒ¼ãƒ‰: answer_by_one_checkbox={answer_by_one_checkbox_input}, extend_first_chunk_size={extend_first_chunk_size_input}, extend_around_chunk_size={extend_around_chunk_size_input}")

    has_error = False
    if not query_text_input:
        has_error = True
        # gr.Warning("ã‚¯ã‚¨ãƒªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if not doc_id_all_checkbox_input and (not doc_id_checkbox_group_input or doc_id_checkbox_group_input == [""]):
        has_error = True
        gr.Warning("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’é¸æŠã—ã¦ãã ã•ã„")
    if document_metadata_text_input:
        document_metadata_text_input = document_metadata_text_input.strip()
        if "=" not in document_metadata_text_input or '"' in document_metadata_text_input or "'" in document_metadata_text_input or '\\' in document_metadata_text_input:
            has_error = True
            gr.Warning("ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚key1=value1,key2=value2,... ã®å½¢å¼ã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        else:
            metadatas = document_metadata_text_input.split(",")
            for metadata in metadatas:
                if "=" not in metadata:
                    has_error = True
                    gr.Warning(
                        "ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚key1=value1,key2=value2,... ã®å½¢å¼ã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
                    break
    if has_error:
        return (
            gr.Textbox(value=""),
            gr.Markdown(
                "**æ¤œç´¢çµæœæ•°**: 0   |   **æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: (0)[]",
                visible=True
            ),
            gr.Dataframe(
                value=None,
                row_count=(1, "fixed")
            )
        )

    def cut_lists(lists, limit=10):
        if not lists or len(lists) == 0:
            return lists
        if len(lists) > limit:
            # æ–‡å­—åˆ—ã®é•·ã•ã§ã‚½ãƒ¼ãƒˆã€é•·ã„æ–‡å­—åˆ—ã‚’å„ªå…ˆçš„ã«å‰Šé™¤
            sorted_indices = sorted(range(len(lists)), key=lambda i: len(lists[i]))
            lists = [lists[i] for i in sorted_indices[:limit]]

        return lists

    def generate_combinations(words_list):
        sampled_list = []
        n = len(words_list)
        threshold = 0.75
        if n < 3:
            sampled_list = words_list
            return sampled_list
        min_required = int(n * threshold)
        print(f"{n=}")
        print(f"{min_required=}")
        sampled_list += list(combinations(words_list, min_required))
        return sampled_list

    def process_lists(lists):
        contains_sql_list = []
        for lst in lists:
            if isinstance(lst, str):
                contains_sql_list.append(f"contains(de.embed_data, '{lst}') > 0")
            else:
                temp = " and ".join(str(item) for item in lst)
                if temp and temp not in contains_sql_list:
                    contains_sql_list.append(f"contains(de.embed_data, '{temp}') > 0")
        return ' OR '.join(contains_sql_list)

    def format_keywords(x):
        if len(x) > 0:
            formatted = '[' + ', '.join(x) + ']'
            return f"({len(x)}){formatted}"
        else:
            return "(0)[No Match]"

    def replace_newlines(text):
        if '\n\n' in text:
            text = text.replace('\n\n', '\n')
            return replace_newlines(text)
        else:
            return text

    region = get_region()

    query_text_input = query_text_input.strip()
    sub_query1_text_input = sub_query1_text_input.strip()
    sub_query2_text_input = sub_query2_text_input.strip()
    sub_query3_text_input = sub_query3_text_input.strip()

    # Use OracleAIVector
    unranked_docs = []
    threshold_value = threshold_value_slider_input
    top_k = top_k_slider_input
    doc_ids_str = "'" + "','".join([str(doc_id) for doc_id in doc_id_checkbox_group_input if doc_id]) + "'"
    print(f"{doc_ids_str=}")
    with_sql = """
               -- Select data
               WITH offsets AS (SELECT level - (:extend_around_chunk_size / 2 + 1) AS offset
                                FROM dual
               CONNECT BY level <= (:extend_around_chunk_size + 1)
                   )
                        , selected_embed_ids AS
                        ( \
               """
    where_sql = """
                    WHERE 1 = 1
                    AND de.doc_id = dc.id """
    where_metadata_sql = ""
    if document_metadata_text_input:
        metadata_conditions = []
        metadatas = document_metadata_text_input.split(",")
        for i, metadata in enumerate(metadatas):
            if "=" not in metadata:
                continue
            key, value = metadata.split("=", 1)
            # ä½¿ç”¨æ­£ç¡®çš„JSONè·¯å¾„è¯­æ³•å’Œå‚æ•°ç»‘å®š
            metadata_conditions.append(f"json_value(dc.cmetadata, '$.\"{key}\"') = '{value}'")

        if metadata_conditions:
            where_metadata_sql = " AND (" + " AND ".join(metadata_conditions) + ") "
        print(f"{where_metadata_sql=}")

    # æ³¨æ„ï¼šwhere_threshold_sqlå’Œwhere_sqlç°åœ¨åœ¨base_sqlä¸­ç›´æ¥æ„å»ºï¼Œå› ä¸ºéœ€è¦æ ¹æ®use_imageä½¿ç”¨ä¸åŒçš„è¡¨åˆ«å
    # v4
    region = get_region()

    # ç”»åƒã‚’ä½¿ã£ã¦å›ç­”ãŒã‚ªãƒ³ã®å ´åˆã€image_embeddingãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½¿ç”¨ã€ã‚ªãƒ•ã®å ´åˆã¯embeddingãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½¿ç”¨
    if use_image:
        # ç”»åƒã‚’ä½¿ã£ã¦å›ç­”ãŒã‚ªãƒ³ã®å ´åˆã€image_embeddingãƒ†ãƒ¼ãƒ–ãƒ«ã®ã¿ã‚’ä½¿ç”¨
        base_sql = f"""
                        SELECT ie.doc_id doc_id, ie.embed_id embed_id, vector_distance(ie.embed_vector, (
                                SELECT to_vector(et.embed_vector) embed_vector
                                FROM
                                    dbms_vector_chain.utl_to_embeddings(
                                            :query_text,
                                            JSON('{{"provider": "ocigenai", "credential_name": "OCI_CRED", "url": "https://inference.generativeai.{region}.oci.oraclecloud.com/20231130/actions/embedText", "model": "cohere.embed-multilingual-v3.0"}}')) t,
                                    JSON_TABLE ( t.column_value, '$[*]'
                                            COLUMNS (
                                                embed_id NUMBER PATH '$.embed_id',
                                                embed_data VARCHAR2 ( 4000 ) PATH '$.embed_data',
                                                embed_vector CLOB PATH '$.embed_vector'
                                            )
                                        )
                                    et), COSINE
                                ) vector_distance
                        FROM {DEFAULT_COLLECTION_NAME}_image_embedding ie, {DEFAULT_COLLECTION_NAME}_collection dc
                        WHERE 1 = 1
                        AND ie.doc_id = dc.id """ + ("""
                        AND ie.doc_id IN (
                            SELECT TRIM(BOTH '''' FROM REGEXP_SUBSTR(:doc_ids, '''[^'']+''', 1, LEVEL)) AS doc_id
                            FROM DUAL
                            CONNECT BY REGEXP_SUBSTR(:doc_ids, '''[^'']+''', 1, LEVEL) IS NOT NULL
                        ) """ if not doc_id_all_checkbox_input else "") + where_metadata_sql + f"""
                        AND vector_distance(ie.embed_vector, (
                                SELECT to_vector(et.embed_vector) embed_vector
                                FROM
                                    dbms_vector_chain.utl_to_embeddings(
                                            :query_text,
                                            JSON('{{"provider": "ocigenai", "credential_name": "OCI_CRED", "url": "https://inference.generativeai.{region}.oci.oraclecloud.com/20231130/actions/embedText", "model": "cohere.embed-multilingual-v3.0"}}')) t,
                                    JSON_TABLE ( t.column_value, '$[*]'
                                            COLUMNS (
                                                embed_id NUMBER PATH '$.embed_id',
                                                embed_data VARCHAR2 ( 4000 ) PATH '$.embed_data',
                                                embed_vector CLOB PATH '$.embed_vector'
                                            )
                                        )
                                    et), COSINE
                                ) <= :threshold_value
                        ORDER BY vector_distance """
    else:
        # ç”»åƒã‚’ä½¿ã£ã¦å›ç­”ãŒã‚ªãƒ•ã®å ´åˆã€embeddingãƒ†ãƒ¼ãƒ–ãƒ«ã®ã¿ã‚’ä½¿ç”¨
        base_sql = f"""
                        SELECT de.doc_id doc_id, de.embed_id embed_id, vector_distance(de.embed_vector, (
                                SELECT to_vector(et.embed_vector) embed_vector
                                FROM
                                    dbms_vector_chain.utl_to_embeddings(
                                            :query_text,
                                            JSON('{{"provider": "ocigenai", "credential_name": "OCI_CRED", "url": "https://inference.generativeai.{region}.oci.oraclecloud.com/20231130/actions/embedText", "model": "cohere.embed-multilingual-v3.0"}}')) t,
                                    JSON_TABLE ( t.column_value, '$[*]'
                                            COLUMNS (
                                                embed_id NUMBER PATH '$.embed_id',
                                                embed_data VARCHAR2 ( 4000 ) PATH '$.embed_data',
                                                embed_vector CLOB PATH '$.embed_vector'
                                            )
                                        )
                                    et), COSINE
                                ) vector_distance
                        FROM {DEFAULT_COLLECTION_NAME}_embedding de, {DEFAULT_COLLECTION_NAME}_collection dc
                        WHERE 1 = 1
                        AND de.doc_id = dc.id """ + ("""
                        AND de.doc_id IN (
                            SELECT TRIM(BOTH '''' FROM REGEXP_SUBSTR(:doc_ids, '''[^'']+''', 1, LEVEL)) AS doc_id
                            FROM DUAL
                            CONNECT BY REGEXP_SUBSTR(:doc_ids, '''[^'']+''', 1, LEVEL) IS NOT NULL
                        ) """ if not doc_id_all_checkbox_input else "") + where_metadata_sql + f"""
                        AND vector_distance(de.embed_vector, (
                                SELECT to_vector(et.embed_vector) embed_vector
                                FROM
                                    dbms_vector_chain.utl_to_embeddings(
                                            :query_text,
                                            JSON('{{"provider": "ocigenai", "credential_name": "OCI_CRED", "url": "https://inference.generativeai.{region}.oci.oraclecloud.com/20231130/actions/embedText", "model": "cohere.embed-multilingual-v3.0"}}')) t,
                                    JSON_TABLE ( t.column_value, '$[*]'
                                            COLUMNS (
                                                embed_id NUMBER PATH '$.embed_id',
                                                embed_data VARCHAR2 ( 4000 ) PATH '$.embed_data',
                                                embed_vector CLOB PATH '$.embed_vector'
                                            )
                                        )
                                    et), COSINE
                                ) <= :threshold_value
                        ORDER BY vector_distance """
    base_sql += """
                    FETCH FIRST :partition_by PARTITIONS BY doc_id, :top_k ROWS ONLY
    """ if partition_by_k_slider_input > 0 else """
                    FETCH FIRST :top_k ROWS ONLY
    """
    select_sql = f"""
    ),
    selected_embed_id_doc_ids AS
    (
            SELECT DISTINCT s.embed_id + o.offset adjusted_embed_id, s.doc_id doc_id
            FROM selected_embed_ids s
            CROSS JOIN offsets o
    ),
    selected_results AS
    (
            SELECT s1.adjusted_embed_id adjusted_embed_id, s1.doc_id doc_id, NVL(s2.vector_distance, 999999.0) vector_distance
            FROM selected_embed_id_doc_ids s1
            LEFT JOIN selected_embed_ids s2
            ON s1.adjusted_embed_id = s2.embed_id AND s1.doc_id = s2.doc_id
    ),"""
    if use_image:
        # ç”»åƒã‚’ä½¿ã£ã¦å›ç­”ãŒã‚ªãƒ³ã®å ´åˆã€image_embeddingãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½¿ç”¨
        select_sql += f"""
    aggregated_results AS
    (
            SELECT json_value(dc.cmetadata, '$.file_name') name, ie.embed_id embed_id, ie.embed_data embed_data, ie.doc_id doc_id, MIN(s.vector_distance) vector_distance
            FROM selected_results s, {DEFAULT_COLLECTION_NAME}_image_embedding ie, {DEFAULT_COLLECTION_NAME}_collection dc
            WHERE s.adjusted_embed_id = ie.embed_id AND s.doc_id = dc.id and ie.doc_id = dc.id
            GROUP BY ie.doc_id, name, ie.embed_id, ie.embed_data"""
    else:
        # ç”»åƒã‚’ä½¿ã£ã¦å›ç­”ãŒã‚ªãƒ•ã®å ´åˆã€embeddingãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½¿ç”¨
        select_sql += f"""
    aggregated_results AS
    (
            SELECT json_value(dc.cmetadata, '$.file_name') name, de.embed_id embed_id, de.embed_data embed_data, de.doc_id doc_id, MIN(s.vector_distance) vector_distance
            FROM selected_results s, {DEFAULT_COLLECTION_NAME}_embedding de, {DEFAULT_COLLECTION_NAME}_collection dc
            WHERE s.adjusted_embed_id = de.embed_id AND s.doc_id = dc.id and de.doc_id = dc.id
            GROUP BY de.doc_id, name, de.embed_id, de.embed_data"""

    select_sql += """
            ORDER BY vector_distance
    ),
    ranked_data AS (
        SELECT
            ar.embed_data,
            ar.doc_id,
            ar.embed_id,
            ar.vector_distance,
            ar.name,
            LAG(ar.embed_id) OVER (PARTITION BY ar.doc_id ORDER BY ar.embed_id) AS prev_embed_id,
            LEAD(ar.embed_id) OVER (PARTITION BY ar.doc_id ORDER BY ar.embed_id) AS next_embed_id
        FROM
            aggregated_results ar
    ),
    grouped_data AS (
        SELECT
            rd.embed_data,
            rd.doc_id,
            rd.embed_id,
            rd.vector_distance,
            rd.name,
            CASE
                WHEN rd.prev_embed_id IS NULL OR rd.embed_id - rd.prev_embed_id > 1 THEN 1
                ELSE 0
            END AS new_group
        FROM
            ranked_data rd
    ),
    groups_marked AS (
        SELECT
            gd.embed_data,
            gd.doc_id,
            gd.embed_id,
            gd.vector_distance,
            gd.name,
            SUM(gd.new_group) OVER (PARTITION BY gd.doc_id ORDER BY gd.embed_id) AS group_id
        FROM
            grouped_data gd
    ),
    aggregated_data AS (
        SELECT
            RTRIM(XMLCAST(XMLAGG(XMLELEMENT(e, ad.embed_data || ',') ORDER BY ad.embed_id) AS CLOB), ',') AS combined_embed_data,
            ad.doc_id,
            MIN(ad.embed_id) AS min_embed_id,
            MIN(ad.vector_distance) AS min_vector_distance,
            ad.name
        FROM
            groups_marked ad
        GROUP BY
            ad.doc_id, ad.group_id, ad.name
    )
    SELECT
        ad.name,
        ad.min_embed_id AS embed_id,
        ad.combined_embed_data,
        ad.doc_id,
        ad.min_vector_distance AS vector_distance
    FROM
        aggregated_data ad
    ORDER BY
        ad.min_vector_distance
    """

    query_texts = [":query_text"]
    if sub_query1_text_input:
        query_texts.append(":sub_query1")
    if sub_query2_text_input:
        query_texts.append(":sub_query2")
    if sub_query3_text_input:
        query_texts.append(":sub_query3")
    print(f"{query_texts=}")

    search_texts = []
    if text_search_checkbox_input:
        # Generate the combined SQL based on available query texts
        search_texts = requests.post(os.environ["GINZA_API_ENDPOINT"],
                                     json={'query_text': query_text_input, 'language': 'ja'}).json()
        search_text = ""
        if search_texts and len(search_texts) > 0:
            search_texts = cut_lists(search_texts, text_search_k_slider_input)
            generated_combinations = generate_combinations(search_texts)
            search_text = process_lists(generated_combinations)
            # search_texts = cut_lists(search_texts, text_search_k_slider_input)
            # search_text = process_lists(search_texts)
        if len(search_text) > 0:
            # where_sql += """
            #             AND (""" + search_text + """) """
            # æ³¨æ„ï¼šã“ã“ã®where_sqlã¯ç•°ãªã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¯ã‚¨ãƒªã§ä½¿ç”¨ã•ã‚Œã‚‹ãŸã‚ã€ãƒ†ãƒ¼ãƒ–ãƒ«åˆ¥åã‚’ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã§ãã¾ã›ã‚“
            # ã“ã®éƒ¨åˆ†ã®ãƒ­ã‚¸ãƒƒã‚¯ã¯full_text_search_sqlã§å‡¦ç†ã•ã‚Œã¾ã™
            region = get_region()
            if use_image:
                # ç”»åƒã‚’ä½¿ã£ã¦å›ç­”ãŒã‚ªãƒ³ã®å ´åˆã€image_embeddingãƒ†ãƒ¼ãƒ–ãƒ«ã®ã¿ã‚’ä½¿ç”¨
                full_text_search_sql = f"""
                            SELECT ie.doc_id doc_id, ie.embed_id embed_id, vector_distance(ie.embed_vector, (
                                    SELECT to_vector(et.embed_vector) embed_vector
                                    FROM
                                        dbms_vector_chain.utl_to_embeddings(
                                                :query_text,
                                                JSON('{{"provider": "ocigenai", "credential_name": "OCI_CRED", "url": "https://inference.generativeai.{region}.oci.oraclecloud.com/20231130/actions/embedText", "model": "cohere.embed-multilingual-v3.0"}}')) t,
                                        JSON_TABLE ( t.column_value, '$[*]'
                                                COLUMNS (
                                                    embed_id NUMBER PATH '$.embed_id',
                                                    embed_data VARCHAR2 ( 4000 ) PATH '$.embed_data',
                                                    embed_vector CLOB PATH '$.embed_vector'
                                                )
                                            )
                                        et), COSINE
                                    ) vector_distance
                            FROM {DEFAULT_COLLECTION_NAME}_image_embedding ie, {DEFAULT_COLLECTION_NAME}_collection dc
                            WHERE 1 = 1
                            AND ie.doc_id = dc.id
                            AND CONTAINS(ie.embed_data, :search_texts, 1) > 0 """ + ("""
                            AND ie.doc_id IN (
                                SELECT TRIM(BOTH '''' FROM REGEXP_SUBSTR(:doc_ids, '''[^'']+''', 1, LEVEL)) AS doc_id
                                FROM DUAL
                                CONNECT BY REGEXP_SUBSTR(:doc_ids, '''[^'']+''', 1, LEVEL) IS NOT NULL
                            ) """ if not doc_id_all_checkbox_input else "") + where_metadata_sql + """
                            ORDER BY SCORE(1) DESC FETCH FIRST :top_k ROWS ONLY """
            else:
                # ç”»åƒã‚’ä½¿ã£ã¦å›ç­”ãŒã‚ªãƒ•ã®å ´åˆã€embeddingãƒ†ãƒ¼ãƒ–ãƒ«ã®ã¿ã‚’ä½¿ç”¨
                full_text_search_sql = f"""
                            SELECT de.doc_id doc_id, de.embed_id embed_id, vector_distance(de.embed_vector, (
                                    SELECT to_vector(et.embed_vector) embed_vector
                                    FROM
                                        dbms_vector_chain.utl_to_embeddings(
                                                :query_text,
                                                JSON('{{"provider": "ocigenai", "credential_name": "OCI_CRED", "url": "https://inference.generativeai.{region}.oci.oraclecloud.com/20231130/actions/embedText", "model": "cohere.embed-multilingual-v3.0"}}')) t,
                                        JSON_TABLE ( t.column_value, '$[*]'
                                                COLUMNS (
                                                    embed_id NUMBER PATH '$.embed_id',
                                                    embed_data VARCHAR2 ( 4000 ) PATH '$.embed_data',
                                                    embed_vector CLOB PATH '$.embed_vector'
                                                )
                                            )
                                        et), COSINE
                                    ) vector_distance
                            FROM {DEFAULT_COLLECTION_NAME}_embedding de, {DEFAULT_COLLECTION_NAME}_collection dc
                            WHERE 1 = 1
                            AND de.doc_id = dc.id
                            AND CONTAINS(de.embed_data, :search_texts, 1) > 0 """ + ("""
                            AND de.doc_id IN (
                                SELECT TRIM(BOTH '''' FROM REGEXP_SUBSTR(:doc_ids, '''[^'']+''', 1, LEVEL)) AS doc_id
                                FROM DUAL
                                CONNECT BY REGEXP_SUBSTR(:doc_ids, '''[^'']+''', 1, LEVEL) IS NOT NULL
                            ) """ if not doc_id_all_checkbox_input else "") + where_metadata_sql + """
                            ORDER BY SCORE(1) DESC FETCH FIRST :top_k ROWS ONLY """
            complete_sql = (with_sql + """
                UNION
        """.join(
                f"        ({base_sql.replace(':query_text', one_query_text)}        )" for one_query_text in
                query_texts) + """
                UNION
                ( """
                            + full_text_search_sql + """
                ) """
                            + select_sql)
    else:
        print(f"{query_texts=}")
        complete_sql = with_sql + """
            UNION
    """.join(
            f"        ({base_sql.replace(':query_text', one_query_text)}        )" for one_query_text in
            query_texts) + select_sql
        print(f"{complete_sql=}")

    # Prepare parameters for SQL execution
    params = {
        "extend_around_chunk_size": extend_around_chunk_size_input,
        "query_text": query_text_input,
        "threshold_value": threshold_value,
        "top_k": top_k
    }
    if not doc_id_all_checkbox_input:
        params["doc_ids"] = doc_ids_str
    if partition_by_k_slider_input > 0:
        params["partition_by"] = partition_by_k_slider_input
    if text_search_checkbox_input and search_texts:
        params["search_texts"] = " ACCUM ".join(search_texts)

    # Add sub-query texts if they exist
    if sub_query1_text_input:
        params["sub_query1"] = sub_query1_text_input
    if sub_query2_text_input:
        params["sub_query2"] = sub_query2_text_input
    if sub_query3_text_input:
        params["sub_query3"] = sub_query3_text_input

    # For debugging: Print the final SQL command.
    # Assuming complete_sql is your SQL string with placeholders like :extend_around_chunk_size, :doc_ids, etc.
    query_sql_output = complete_sql

    # Manually replace placeholders with parameter values for debugging
    for key, value in params.items():
        if key == "doc_ids":
            value = "''" + "'',''".join([str(doc_id) for doc_id in doc_id_checkbox_group_input if doc_id]) + "''"
        placeholder = f":{key}"
        # For the purpose of display, ensure the value is properly quoted if it's a string
        display_value = f"'{value}'" if isinstance(value, str) else str(value)
        query_sql_output = query_sql_output.replace(placeholder, display_value)
    query_sql_output = query_sql_output.strip() + ";"

    # Now query_sql_output contains the SQL command with parameter values inserted
    print(f"\nQUERY_SQL_OUTPUT:\n{query_sql_output}")

    with (pool.acquire() as conn):
        with conn.cursor() as cursor:
            max_retries = 3
            retries = 0
            while retries < max_retries:
                try:
                    cursor.execute(complete_sql, params)
                    break
                except Exception as e:
                    print(f"Exception: {e}")
                    retries += 1
                    print(f"Error executing SQL query: {e}. Retrying ({retries}/{max_retries})...")
                    time.sleep(10 * retries)
                    if retries == max_retries:
                        gr.Warning("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãã—ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
                        return (
                            gr.Textbox(value=""),
                            gr.Markdown(
                                "**æ¤œç´¢çµæœæ•°**: 0   |   **æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: (0)[]",
                                visible=True
                            ),
                            gr.Dataframe(
                                value=None,
                                row_count=(1, "fixed")
                            )
                        )
            for row in cursor:
                print(f"row: {row}")
                unranked_docs.append([row[0], row[1], row[2].read(), row[3], row[4]])

            if len(unranked_docs) == 0:
                docs_dataframe = pd.DataFrame(
                    columns=["NO", "CONTENT", "EMBED_ID", "SOURCE", "DISTANCE", "SCORE", "KEY_WORDS"])

                return (
                    gr.Textbox(value=query_sql_output.strip()),
                    gr.Markdown(
                        "**æ¤œç´¢çµæœæ•°**: " + str(len(docs_dataframe)) + "   |   **æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: (" + str(
                            len(search_texts)) + ")[" + ', '.join(search_texts) + "]",
                        visible=True),
                    gr.Dataframe(
                        value=docs_dataframe,
                        wrap=True,
                        headers=["NO", "CONTENT", "EMBED_ID", "SOURCE", "DISTANCE", "SCORE", "KEY_WORDS"],
                        column_widths=["4%", "50%", "6%", "8%", "6%", "8%"],
                        row_count=(1, "fixed"),
                    ),
                )

            # ToDo: In case of error
            if 'cohere/rerank' in reranker_model_radio_input:
                unranked = []
                for doc in unranked_docs:
                    unranked.append(doc[2])
                ranked_results = rerank_documents_response(query_text_input, unranked, reranker_model_radio_input)
                ranked_scores = [0.0] * len(unranked_docs)
                for result in ranked_results:
                    ranked_scores[result['index']] = result['relevance_score']
                docs_data = [{'CONTENT': doc[2],
                              'EMBED_ID': doc[1],
                              'SOURCE': str(doc[3]) + ":" + doc[0],
                              'DISTANCE': '-' if str(doc[4]) == '999999.0' else str(doc[4]),
                              'SCORE': ce_score} for doc, ce_score in zip(unranked_docs, ranked_scores)]
                docs_dataframe = pd.DataFrame(docs_data)
                docs_dataframe = docs_dataframe[
                    docs_dataframe['SCORE'] >= float(reranker_threshold_slider_input)
                    ].sort_values(by='SCORE', ascending=False).head(
                    reranker_top_k_slider_input)
            else:
                docs_data = [{'CONTENT': doc[2],
                              'EMBED_ID': doc[1],
                              'SOURCE': str(doc[3]) + ":" + doc[0],
                              'DISTANCE': '-' if str(doc[4]) == '999999.0' else str(doc[4]),
                              'SCORE': '-'} for doc in unranked_docs]
                docs_dataframe = pd.DataFrame(docs_data)

            print(f"{extend_first_chunk_size_input=}")
            if extend_first_chunk_size_input > 0 and len(docs_dataframe) > 0:
                filtered_doc_ids = "'" + "','".join(
                    [source.split(':')[0] for source in docs_dataframe['SOURCE'].tolist()]) + "'"
                print(f"{filtered_doc_ids=}")
                # ç”»åƒã‚’ä½¿ã£ã¦å›ç­”ã®è¨­å®šã«å¿œã˜ã¦é©åˆ‡ãªãƒ†ãƒ¼ãƒ–ãƒ«ã‚’é¸æŠ
                if use_image:
                    # ç”»åƒã‚’ä½¿ã£ã¦å›ç­”ãŒã‚ªãƒ³ã®å ´åˆã€image_embeddingãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½¿ç”¨
                    select_extend_first_chunk_sql = f"""
SELECT
        json_value(dc.cmetadata, '$.file_name') name,
        MIN(ie.embed_id) embed_id,
        RTRIM(XMLCAST(XMLAGG(XMLELEMENT(e, ie.embed_data || ',') ORDER BY ie.embed_id) AS CLOB), ',') AS embed_data,
        ie.doc_id doc_id,
        '999999.0' vector_distance
FROM
        {DEFAULT_COLLECTION_NAME}_image_embedding ie, {DEFAULT_COLLECTION_NAME}_collection dc
WHERE
        ie.doc_id = dc.id AND
        ie.doc_id IN (:filtered_doc_ids) AND
        ie.embed_id <= :extend_first_chunk_size
GROUP BY
        ie.doc_id, name
ORDER
        BY ie.doc_id
            """
                else:
                    # ç”»åƒã‚’ä½¿ã£ã¦å›ç­”ãŒã‚ªãƒ•ã®å ´åˆã€embeddingãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½¿ç”¨
                    select_extend_first_chunk_sql = f"""
SELECT
        json_value(dc.cmetadata, '$.file_name') name,
        MIN(de.embed_id) embed_id,
        RTRIM(XMLCAST(XMLAGG(XMLELEMENT(e, de.embed_data || ',') ORDER BY de.embed_id) AS CLOB), ',') AS embed_data,
        de.doc_id doc_id,
        '999999.0' vector_distance
FROM
        {DEFAULT_COLLECTION_NAME}_embedding de, {DEFAULT_COLLECTION_NAME}_collection dc
WHERE
        de.doc_id = dc.id AND
        de.doc_id IN (:filtered_doc_ids) AND
        de.embed_id <= :extend_first_chunk_size
GROUP BY
        de.doc_id, name
ORDER
        BY de.doc_id
            """
                select_extend_first_chunk_sql = (select_extend_first_chunk_sql
                                                 .replace(':filtered_doc_ids', filtered_doc_ids)
                                                 .replace(':extend_first_chunk_size',
                                                          str(extend_first_chunk_size_input)))
                print(f"{select_extend_first_chunk_sql=}")
                query_sql_output += "\n" + select_extend_first_chunk_sql.strip()
                cursor.execute(select_extend_first_chunk_sql)
                first_chunks_df = pd.DataFrame(columns=docs_dataframe.columns)
                for row in cursor:
                    new_data = pd.DataFrame(
                        {'CONTENT': row[2].read(), 'EMBED_ID': row[1], 'SOURCE': str(row[3]) + ":" + row[0],
                         'DISTANCE': '-', 'SCORE': '-'},
                        index=[2])
                    first_chunks_df = pd.concat([new_data, first_chunks_df], ignore_index=True)
                print(f"{first_chunks_df=}")

                # åˆ›å»ºä¸€ä¸ªç©ºçš„DataFrame,ç”¨äºå­˜å‚¨æ›´æ–°åçš„æ•°æ®
                updated_df = pd.DataFrame(columns=docs_dataframe.columns)

                # è®°å½•æ¯ä¸ªSOURCEçš„åˆå§‹æ’å…¥ä½ç½®
                insert_positions = {}

                # éå†åŸå§‹æ•°æ®çš„æ¯ä¸€è¡Œ
                for index, row in docs_dataframe.iterrows():
                    source = row['SOURCE']

                    # å¦‚æœå½“å‰SOURCEè¿˜æ²¡æœ‰è®°å½•åˆå§‹æ’å…¥ä½ç½®,åˆ™å°†å…¶åˆå§‹åŒ–ä¸ºå½“å‰ä½ç½®
                    if source not in insert_positions:
                        insert_positions[source] = len(updated_df)

                    # æ‰¾åˆ°æ–°æ•°æ®ä¸­ä¸å½“å‰SOURCEç›¸åŒçš„è¡Œ
                    same_source_new_data = first_chunks_df[first_chunks_df['SOURCE'] == source]

                    # éå†æ–°æ•°æ®ä¸­ä¸å½“å‰SOURCEç›¸åŒçš„è¡Œ
                    for _, new_row in same_source_new_data.iterrows():
                        # åœ¨å½“å‰è¡Œä¹‹å‰æ’å…¥æ–°æ•°æ®
                        updated_df = pd.concat([updated_df[:insert_positions[source]],
                                                pd.DataFrame(new_row).T,
                                                updated_df[insert_positions[source]:]])

                        # æ›´æ–°å½“å‰SOURCEçš„æ’å…¥ä½ç½®
                        insert_positions[source] += 1

                    # å°†å½“å‰è¡Œæ·»åŠ åˆ°updated_dfä¸­
                    updated_df = pd.concat([updated_df[:insert_positions[source]],
                                            pd.DataFrame(row).T,
                                            updated_df[insert_positions[source]:]])

                    # æ›´æ–°å½“å‰SOURCEçš„æ’å…¥ä½ç½®
                    insert_positions[source] += 1
                docs_dataframe = updated_df

            conn.commit()

            docs_dataframe['KEY_WORDS'] = docs_dataframe['CONTENT'].apply(
                lambda x: [word for word in search_texts if word.lower() in x.lower()])
            docs_dataframe['KEY_WORDS'] = docs_dataframe['KEY_WORDS'].apply(format_keywords)
            docs_dataframe['CONTENT'] = docs_dataframe['CONTENT'].apply(replace_newlines)

            if answer_by_one_checkbox_input:
                docs_dataframe = docs_dataframe[docs_dataframe['SOURCE'] == docs_dataframe['SOURCE'].iloc[0]]

            docs_dataframe = docs_dataframe.reset_index(drop=True)
            docs_dataframe.insert(0, 'NO', pd.Series(range(1, len(docs_dataframe) + 1)))
            print(f"{docs_dataframe=}")

            if len(docs_dataframe) == 0:
                docs_dataframe = pd.DataFrame(
                    columns=["NO", "CONTENT", "EMBED_ID", "SOURCE", "DISTANCE", "SCORE", "KEY_WORDS"])

            return (
                gr.Textbox(value=query_sql_output.strip()),
                gr.Markdown(
                    "**æ¤œç´¢çµæœæ•°**: " + str(len(docs_dataframe)) + "   |   **æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: (" + str(
                        len(search_texts)) + ")[" + ', '.join(search_texts) + "]",
                    visible=True
                ),
                gr.Dataframe(
                    value=docs_dataframe,
                    wrap=True,
                    headers=["NO", "CONTENT", "EMBED_ID", "SOURCE", "DISTANCE", "SCORE", "KEY_WORDS"],
                    column_widths=["4%", "50%", "6%", "8%", "6%", "6%", "8%"],
                    row_count=(len(docs_dataframe) if len(docs_dataframe) > 0 else 1, "fixed")
                )
            )


def extract_and_format(input_str, search_result_df):
    json_arrays = re.findall(r'\[\n.*?\{.*?}\n.*?]', input_str, flags=re.DOTALL)
    if not json_arrays:
        return (
                input_str +
                f"\n"
                f"---å›ç­”å†…ã§å‚ç…§ã•ã‚Œã¦ã„ã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ---"
                f"\n"
                f"å›ç­”ã«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒå­˜åœ¨ã—ãªã„ã‹ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚"
        )

    extracted = []
    for json_str in json_arrays:
        input_str = input_str.replace(json_str, '')
        json_str = json_str.replace('\n', '').replace('\r', '')
        data = json.loads(json_str)

        for item in data:
            print(f"{item=}")
            if isinstance(item, dict):
                if "EMBED_ID" in item and "SOURCE" in item:
                    extracted.append({
                        "EMBED_ID": item["EMBED_ID"],
                        "SOURCE": item["SOURCE"]
                    })

    formatted = (
            input_str +
            f"\n"
            f"---å›ç­”å†…ã§å‚ç…§ã•ã‚Œã¦ã„ã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ---"
            f"\n"
    )
    formatted += "[\n"
    for item in extracted:
        content = "N/A"
        if item["EMBED_ID"] and isinstance(item["EMBED_ID"], int) and item["SOURCE"]:
            content = search_result_df.loc[
                (search_result_df["EMBED_ID"].astype(int) == int(item["EMBED_ID"])) &
                (search_result_df["SOURCE"] == item["SOURCE"]),
                "CONTENT"
            ].values
            if len(content) > 0:
                content = content[0]
                content = content.replace('"', '\'')
                content = content.replace('\n', ' ').replace('\r', ' ')
        formatted += (
            '    {{\n'
            '        "EMBED_ID": {},\n'
            '        "SOURCE": "{}",\n'
            '        "CONTENT": "{}"\n'
            '    }},\n'
        ).format(item["EMBED_ID"], item["SOURCE"], content)
    if extracted:
        formatted = formatted.rstrip(",\n") + "\n"
    formatted += "]"

    return formatted


def extract_citation(input_str):
    # 2ã¤ã®éƒ¨åˆ†ã®å†…å®¹ã‚’ãƒãƒƒãƒãƒ³ã‚°
    pattern = '^(.*?)\n---å›ç­”å†…ã§å‚ç…§ã•ã‚Œã¦ã„ã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ---\n(.*?)$'
    match = re.search(pattern, input_str, re.DOTALL)
    if match:
        part1 = match.group(1).strip()
        part2 = match.group(2).strip()
        return part1, part2
    else:
        return None, None


#
# def generate_langgpt_prompt_ja(context, query_text, include_citation=False, include_current_time=False):
#     # å›ºå®šã™ã‚‹ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
#     error_message = "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é©åˆ‡ãªå›ç­”ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã§ã—ãŸã€‚åˆ¥ã® LLM ãƒ¢ãƒ‡ãƒ«ã‚’ãŠè©¦ã—ã„ãŸã ãã‹ã€ã‚¯ã‚¨ãƒªã®å†…å®¹ã‚„è¨­å®šã‚’å°‘ã—èª¿æ•´ã—ã¦ã„ãŸã ãã“ã¨ã§è§£æ±ºã§ãã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚"
#
#     # LangGPTãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®åŸºæœ¬æ§‹é€ 
#     prompt = f"""
# # Role: å³æ ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆQA
#
# ## Profile
#
# - Author: User
# - Version: 0.2
# - Language: æ—¥æœ¬èª
# - Description: å³å¯†ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ã®è³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ ã€‚æä¾›ã•ã‚ŒãŸæ–‡è„ˆãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨ã—ã€ä¸€åˆ‡ã®æ”¹å¤‰ã‚’åŠ ãˆãšã«å›ç­”ã—ã¾ã™ã€‚
#
# ### Core Skills
# 1. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®å®Œå…¨ä¸€è‡´æ¤œç´¢
# 2. æ–‡è„ˆæ”¹å¤‰ã®å®Œå…¨æ’é™¤
# 3. å›ç­”ä¸èƒ½æ™‚ã®å®šå‹é€šçŸ¥
# 4. ãƒãƒ«ãƒãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå‡ºåŠ›å¯¾å¿œ
#
# ## Rules
# 1. {error_message}
# 2. å›ç­”ã¯<context>ã®å†…å®¹ã«100%ä¾å­˜
# 3. éƒ¨åˆ†ä¸€è‡´ã‚„æ¨æ¸¬ã‚’ä¸€åˆ‡è¡Œã‚ãªã„
# 4. æ—¥ä»˜æƒ…å ±ãŒã‚ã‚‹å ´åˆã®æ™‚ç³»åˆ—å‡¦ç†ï¼ˆæœ€æ–°æƒ…å ±å„ªå…ˆï¼‰
# 5. å¼•ç”¨æƒ…å ±ã®å³å¯†ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆä¿æŒ
#
# ## Workflow
# 1. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè§£æãƒ•ã‚§ãƒ¼ã‚º
#    - UTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§å³å¯†è§£æ
#    - ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆEMBED_ID/SOURCEï¼‰ã®æŠ½å‡º
# 2. ã‚¯ã‚¨ãƒªãƒãƒƒãƒãƒ³ã‚°ãƒ•ã‚§ãƒ¼ã‚º
#    - å®Œå…¨æ–‡å­—åˆ—ãƒãƒƒãƒãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ é©ç”¨
#    - è¤‡æ•°å€™è£œãŒã‚ã‚‹å ´åˆã¯æœ€æ–°æ—¥ä»˜ã‚’å„ªå…ˆ
# 3. å›ç­”ç”Ÿæˆãƒ•ã‚§ãƒ¼ã‚º
#    - ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã®ç›´æ¥å¼•ç”¨
#    - å¼•ç”¨æƒ…å ±ã®æ§‹é€ åŒ–å‡ºåŠ›ï¼ˆè¦æ±‚æ™‚ï¼‰
# 4. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
#    - ãƒãƒƒãƒãªã— â†’ å®šå‹ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
#    - çŸ›ç›¾ãƒ‡ãƒ¼ã‚¿ â†’ äº‹å®Ÿé–¢ä¿‚ã‚’åˆ—æŒ™
#
# ## Initialization
# As a/an <Role>, you must follow the <Rules> in <Language>.
# ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆQAã‚·ã‚¹ãƒ†ãƒ ãŒèµ·å‹•ã—ã¾ã—ãŸã€‚ä»¥ä¸‹ã®è¦ç´ ã‚’æä¾›ãã ã•ã„ï¼š
#
# <context>
# {context}
# </context>
#
# <query>
# {query_text}
# </query>
# """
#
#     # å¼•ç”¨æƒ…å ±ã®æ¡ä»¶ä»˜ãè¿½åŠ 
#     if include_citation:
#         prompt += """
# ### å¼•ç”¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆè¦ç´„
# - å‡ºåŠ›ç›´å¾Œã«JSONé…åˆ—ã‚’è¿½åŠ 
# - å³å¯†ãªæ§‹é€ ä¿æŒï¼ˆ```jsonä¸ä½¿ç”¨ï¼‰ï¼š
# [
#     {
#         "EMBED_ID": <ä¸€æ„ãªè­˜åˆ¥å­>,
#         "SOURCE": "<æƒ…å ±ã®å‡ºå…¸>",
#         "EXTRACT_TEXT": "<å¼•ç”¨éƒ¨åˆ†ã®åŸæ–‡>"
#     }
# ]
# """
#
#     # æ™‚é–“å‡¦ç†ã®æ¡ä»¶ä»˜ãè¿½åŠ 
#     if include_current_time:
#         current_time = datetime.now().strftime('%Y%m%d%H%M%S')
#         prompt += f"""
# ### æ™‚ç³»åˆ—å‡¦ç†è¦å‰‡
# - åŸºæº–æ™‚åˆ»: {current_time}
# - æœ€æ–°æƒ…å ±åˆ¤å®šã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼š
#   1. æ—¥ä»˜ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–ï¼ˆYYYYMMDDHHMMSSï¼‰
#   2. æ™‚åˆ»è¿‘æ¥é †ã«ã‚½ãƒ¼ãƒˆ
#   3. åŒä¸€æƒ…å ±ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†
# - æœŸé–“æŒ‡å®šã‚¯ã‚¨ãƒªå¯¾å¿œï¼š
#   /period:start=YYYYMMDD,end=YYYYMMDD
# """
#
#     return prompt.strip()


async def chat_document(
        search_result,
        llm_answer_checkbox,
        include_citation,
        include_current_time,
        use_image,
        query_text,
        doc_id_all_checkbox_input,
        doc_id_checkbox_group_input,
        rag_prompt_template
):
    has_error = False
    if not query_text:
        has_error = True
        # gr.Warning("ã‚¯ã‚¨ãƒªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if not doc_id_all_checkbox_input and (not doc_id_checkbox_group_input or doc_id_checkbox_group_input == [""]):
        has_error = True
        # gr.Warning("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’é¸æŠã—ã¦ãã ã•ã„")
    if search_result.empty or (len(search_result) > 0 and search_result.iloc[0]['CONTENT'] == ''):
        has_error = True
        gr.Warning("æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚è¨­å®šã‚‚ã—ãã¯ã‚¯ã‚¨ãƒªã‚’å¤‰æ›´ã—ã¦å†åº¦ã”ç¢ºèªãã ã•ã„ã€‚")
    if has_error:
        yield (
            gr.Markdown(value=""),
            gr.Markdown(value=""),
            gr.Markdown(value=""),
            gr.Markdown(value=""),
            gr.Markdown(value=""),
            gr.Markdown(value=""),
            gr.Markdown(value=""),
            gr.Markdown(value=""),
            gr.Markdown(value=""),
            gr.Markdown(value=""),
            gr.Markdown(value=""),
            gr.Markdown(value=""),
            gr.Markdown(value=""),
            gr.Markdown(value="")
        )
        return

    query_text = query_text.strip()

    xai_grok_3_response = ""
    command_a_response = ""
    llama_4_maverick_response = ""
    llama_4_scout_response = ""
    llama_3_3_70b_response = ""
    llama_3_2_90b_vision_response = ""
    openai_gpt4o_response = ""
    openai_gpt4_response = ""
    azure_openai_gpt4o_response = ""
    azure_openai_gpt4_response = ""

    xai_grok_3_checkbox = False
    command_a_checkbox = False
    llama_4_maverick_checkbox = False
    llama_4_scout_checkbox = False
    llama_3_3_70b_checkbox = False
    llama_3_2_90b_vision_checkbox = False
    openai_gpt4o_checkbox = False
    openai_gpt4_checkbox = False
    azure_openai_gpt4o_checkbox = False
    azure_openai_gpt4_checkbox = False
    if "xai/grok-3" in llm_answer_checkbox:
        xai_grok_3_checkbox = True
    if "cohere/command-a" in llm_answer_checkbox:
        command_a_checkbox = True
    if "meta/llama-4-maverick-17b-128e-instruct-fp8" in llm_answer_checkbox:
        llama_4_maverick_checkbox = True
    if "meta/llama-4-scout-17b-16e-instruct" in llm_answer_checkbox:
        llama_4_scout_checkbox = True
    if "meta/llama-3-3-70b" in llm_answer_checkbox:
        llama_3_3_70b_checkbox = True
    if "meta/llama-3-2-90b-vision" in llm_answer_checkbox:
        llama_3_2_90b_vision_checkbox = True
    if "openai/gpt-4o" in llm_answer_checkbox:
        openai_gpt4o_checkbox = True
    if "openai/gpt-4" in llm_answer_checkbox:
        openai_gpt4_checkbox = True
    if "azure_openai/gpt-4o" in llm_answer_checkbox:
        azure_openai_gpt4o_checkbox = True
    if "azure_openai/gpt-4" in llm_answer_checkbox:
        azure_openai_gpt4_checkbox = True

    # context = '\n'.join(search_result['CONTENT'].astype(str).values)
    context = search_result[['EMBED_ID', 'SOURCE', 'CONTENT']].to_dict('records')
    #     print(f"{context=}")
    #     # system_text = f"""
    #     #         Use the following pieces of Context to answer the question at the end.
    #     #         If you don't know the answer, just say that you don't know, don't try to make up an answer.
    #     #         Use the EXACT TEXT from the Context WITHOUT ANY MODIFICATIONS, REORGANIZATION or EMBELLISHMENT.
    #     #         Don't try to answer anything that isn't in Context.
    #     #         Context:
    #     #         ```
    #     #         {context}
    #     #         ```
    #     #         """
    #     system_text = f"""
    # ---ç›®æ¨™ï¼š---
    # æ¬¡ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨ã—ã¦ã€æœ€å¾Œã«ã‚ã‚‹è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚
    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ãªã„ã“ã¨ã«ã¤ã„ã¦ã¯ç­”ãˆã‚ˆã†ã¨ã—ãªã„ã§ãã ã•ã„ã€‚
    # ã‚‚ã—ç­”ãˆãŒã‚ã‹ã‚‰ãªã„å ´åˆã¯ã€ã€Œç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é©åˆ‡ãªå›ç­”ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã§ã—ãŸã€‚åˆ¥ã® LLM ãƒ¢ãƒ‡ãƒ«ã‚’ãŠè©¦ã—ã„ãŸã ãã‹ã€ã‚¯ã‚¨ãƒªã®å†…å®¹ã‚„è¨­å®šã‚’å°‘ã—èª¿æ•´ã—ã¦ã„ãŸã ãã“ã¨ã§è§£æ±ºã§ãã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ã€ã¨è¨€ã£ã¦ãã ã•ã„ã€‚
    # ç­”ãˆã‚’ã§ã£ã¡ä¸Šã’ã‚ˆã†ã¨ã—ãªã„ã§ãã ã•ã„ã€‚
    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ­£ç¢ºãªãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨ã—ã€**ä¸€åˆ‡ã®ä¿®æ­£ã€å†æ§‹æˆã€ã¾ãŸã¯è„šè‰²ã‚’åŠ ãˆãšã«**ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
    # \n
    # """
    #
    #     if include_citation:
    #         system_text += f"""
    # After providing the answer, **include the 'EMBED_ID' and 'SOURCE' of the 'CONTENT' used to formulate the answer in JSON format**.
    # The JSON array must strictly follow this structure without '```json' and '```' around it:
    #
    # [
    #     {{
    #         "EMBED_ID": <A unique identifier for the content piece.>,
    #         "SOURCE": "<A string indicating the origin of the content.>"
    #     }}
    # ]
    #
    # If multiple pieces of CONTENT are used, include all relevant EMBED_IDs and SOURCEs in the JSON array.
    # \n
    # """
    #
    #     current_time = datetime.now()
    #     formatted_time = current_time.strftime('%Y%m%d%H%M%S')
    #
    #     if include_current_time:
    #         system_text += f"""
    # The current time is {formatted_time} in format '%Y%m%d%H%M%S', When the Context contains multiple entries with dates,
    # please consider these dates carefully when answering the question:
    # - If the question asks about the "latest" or "most recent" information, use data from the most recent date
    # - If the question asks about a specific time period, use data from that corresponding time period
    # - If comparing different time periods, clearly specify which date's data you are referencing
    # \n
    # """
    #
    #     system_text += f"""
    # ---ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼š--- \n
    # <context>
    # {context}
    # </context>
    # \n
    # """
    #
    #     user_text = f"""
    # ---è³ªå•ï¼š--- \n
    # <query>
    # {query_text}
    # </query>
    # \n
    # ---å½¹ã«ç«‹ã¤å›ç­”ï¼š--- \n
    # """

    system_text = ""
    user_text = get_langgpt_rag_prompt(context, query_text, include_citation, include_current_time, rag_prompt_template)

    xai_grok_3_user_text = user_text
    command_a_user_text = user_text

    llama_4_maverick_user_text = user_text
    llama_4_scout_user_text = user_text
    llama_3_3_70b_user_text = user_text
    llama_3_2_90b_vision_user_text = user_text
    openai_gpt4o_user_text = user_text
    openai_gpt4_user_text = user_text
    azure_openai_gpt4o_user_text = user_text
    azure_openai_gpt4_user_text = user_text

    async for xai_grok_3, command_a, llama_4_maverick, llama_4_scout, llama_3_3_70b, llama_3_2_90b_vision, gpt4o, gpt4, azure_gpt4o, azure_gpt4 in chat(
            system_text,
            xai_grok_3_user_text,
            command_a_user_text,
            None,
            llama_4_maverick_user_text,
            None,
            llama_4_scout_user_text,
            llama_3_3_70b_user_text,
            None,
            llama_3_2_90b_vision_user_text,
            openai_gpt4o_user_text,
            openai_gpt4_user_text,
            azure_openai_gpt4o_user_text,
            azure_openai_gpt4_user_text,
            xai_grok_3_checkbox,
            command_a_checkbox,
            llama_4_maverick_checkbox,
            llama_4_scout_checkbox,
            llama_3_3_70b_checkbox,
            llama_3_2_90b_vision_checkbox,
            openai_gpt4o_checkbox,
            openai_gpt4_checkbox,
            azure_openai_gpt4o_checkbox,
            azure_openai_gpt4_checkbox
    ):
        xai_grok_3_response += xai_grok_3
        command_a_response += command_a
        llama_4_maverick_response += llama_4_maverick
        llama_4_scout_response += llama_4_scout
        llama_3_3_70b_response += llama_3_3_70b
        llama_3_2_90b_vision_response += llama_3_2_90b_vision
        openai_gpt4o_response += gpt4o
        openai_gpt4_response += gpt4
        azure_openai_gpt4o_response += azure_gpt4o
        azure_openai_gpt4_response += azure_gpt4
        yield (
            gr.Markdown(value=xai_grok_3_response),
            gr.Markdown(value=command_a_response),
            gr.Markdown(value=llama_4_maverick_response),
            gr.Markdown(value=llama_4_scout_response),
            gr.Markdown(value=llama_3_3_70b_response),
            gr.Markdown(value=llama_3_2_90b_vision_response),
            gr.Markdown(value=openai_gpt4o_response),
            gr.Markdown(value=openai_gpt4_response),
            gr.Markdown(value=azure_openai_gpt4o_response),
            gr.Markdown(value=azure_openai_gpt4_response)
        )


async def append_citation(
        search_result,
        llm_answer_checkbox,
        include_citation,
        query_text,
        doc_id_all_checkbox_input,
        doc_id_checkbox_group_input,
        xai_grok_3_answer_text,
        command_a_answer_text,
        llama_4_maverick_answer_text,
        llama_4_scout_answer_text,
        llama_3_3_70b_answer_text,
        llama_3_2_90b_vision_answer_text,
        openai_gpt4o_answer_text,
        openai_gpt4_answer_text,
        azure_openai_gpt4o_answer_text,
        azure_openai_gpt4_answer_text
):
    has_error = False
    if not query_text:
        has_error = True
        # gr.Warning("ã‚¯ã‚¨ãƒªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if not doc_id_all_checkbox_input and (not doc_id_checkbox_group_input or doc_id_checkbox_group_input == [""]):
        has_error = True
        # gr.Warning("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’é¸æŠã—ã¦ãã ã•ã„")
    if search_result.empty or (len(search_result) > 0 and search_result.iloc[0]['CONTENT'] == ''):
        has_error = True
        # gr.Warning("æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚è¨­å®šã‚‚ã—ãã¯ã‚¯ã‚¨ãƒªã‚’å¤‰æ›´ã—ã¦å†åº¦ã”ç¢ºèªãã ã•ã„ã€‚")
    if has_error:
        yield (
            gr.Markdown(value=xai_grok_3_answer_text),
            gr.Markdown(value=command_a_answer_text),
            gr.Markdown(value=llama_4_maverick_answer_text),
            gr.Markdown(value=llama_4_scout_answer_text),
            gr.Markdown(value=llama_3_3_70b_answer_text),
            gr.Markdown(value=llama_3_2_90b_vision_answer_text),
            gr.Markdown(value=openai_gpt4o_answer_text),
            gr.Markdown(value=openai_gpt4_answer_text),
            gr.Markdown(value=azure_openai_gpt4o_answer_text),
            gr.Markdown(value=azure_openai_gpt4_answer_text)
        )
        return

    if not include_citation:
        yield (
            gr.Markdown(value=xai_grok_3_answer_text),
            gr.Markdown(value=command_a_answer_text),
            gr.Markdown(value=llama_4_maverick_answer_text),
            gr.Markdown(value=llama_4_scout_answer_text),
            gr.Markdown(value=llama_3_3_70b_answer_text),
            gr.Markdown(value=llama_3_2_90b_vision_answer_text),
            gr.Markdown(value=openai_gpt4o_answer_text),
            gr.Markdown(value=openai_gpt4_answer_text),
            gr.Markdown(value=azure_openai_gpt4o_answer_text),
            gr.Markdown(value=azure_openai_gpt4_answer_text)
        )
        return

    if "xai/grok-3" in llm_answer_checkbox:
        xai_grok_3_answer_text = extract_and_format(xai_grok_3_answer_text, search_result)
    if "cohere/command-a" in llm_answer_checkbox:
        command_a_answer_text = extract_and_format(command_a_answer_text, search_result)
    if "meta/llama-4-maverick-17b-128e-instruct-fp8" in llm_answer_checkbox:
        llama_4_maverick_answer_text = extract_and_format(llama_4_maverick_answer_text, search_result)
    if "meta/llama-4-scout-17b-16e-instruct" in llm_answer_checkbox:
        llama_4_scout_answer_text = extract_and_format(llama_4_scout_answer_text, search_result)
    if "meta/llama-3-3-70b" in llm_answer_checkbox:
        llama_3_3_70b_answer_text = extract_and_format(llama_3_3_70b_answer_text, search_result)
    if "meta/llama-3-2-90b-vision" in llm_answer_checkbox:
        llama_3_2_90b_vision_answer_text = extract_and_format(llama_3_2_90b_vision_answer_text, search_result)
    if "openai/gpt-4o" in llm_answer_checkbox:
        openai_gpt4o_answer_text = extract_and_format(openai_gpt4o_answer_text, search_result)
    if "openai/gpt-4" in llm_answer_checkbox:
        openai_gpt4_answer_text = extract_and_format(openai_gpt4_answer_text, search_result)
    if "azure_openai/gpt-4o" in llm_answer_checkbox:
        azure_openai_gpt4o_answer_text = extract_and_format(azure_openai_gpt4o_answer_text, search_result)
    if "azure_openai/gpt-4" in llm_answer_checkbox:
        azure_openai_gpt4_answer_text = extract_and_format(azure_openai_gpt4_answer_text, search_result)
    yield (
        gr.Markdown(value=xai_grok_3_answer_text),
        gr.Markdown(value=command_a_answer_text),
        gr.Markdown(value=llama_4_maverick_answer_text),
        gr.Markdown(value=llama_4_scout_answer_text),
        gr.Markdown(value=llama_3_3_70b_answer_text),
        gr.Markdown(value=llama_3_2_90b_vision_answer_text),
        gr.Markdown(value=openai_gpt4o_answer_text),
        gr.Markdown(value=openai_gpt4_answer_text),
        gr.Markdown(value=azure_openai_gpt4o_answer_text),
        gr.Markdown(value=azure_openai_gpt4_answer_text)
    )
    return


async def process_single_image_streaming(image_url, query_text, llm_answer_checkbox_group, target_models, image_index,
                                         doc_id, img_id, custom_image_prompt=None):
    """
    å˜ä¸€ç”»åƒã‚’é¸æŠã•ã‚ŒãŸLLMãƒ¢ãƒ‡ãƒ«ã§å‡¦ç†ã—ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å½¢å¼ã§å›ç­”ã‚’è¿”ã™

    Args:
        image_url: ç”»åƒã®URL
        query_text: ã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆ
        llm_answer_checkbox_group: é¸æŠã•ã‚ŒãŸLLMãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆ
        target_models: å¯¾è±¡ãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆ
        image_index: ç”»åƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        doc_id: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆID
        img_id: ç”»åƒID
        custom_image_prompt: ã‚«ã‚¹ã‚¿ãƒ ç”»åƒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

    Yields:
        dict: å„ãƒ¢ãƒ‡ãƒ«ã®éƒ¨åˆ†çš„ãªå›ç­”ã‚’å«ã‚€è¾æ›¸
    """
    if custom_image_prompt:
        custom_image_prompt = custom_image_prompt.replace('{{query_text}}', '{query_text}')

    region = get_region()

    # å„ãƒ¢ãƒ‡ãƒ«ã®ã‚¿ã‚¹ã‚¯ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’ä½œæˆ
    async def create_model_task(model):
        llm = None  # LLMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’åˆæœŸåŒ–
        try:
            if model not in llm_answer_checkbox_group:
                # é¸æŠã•ã‚Œã¦ã„ãªã„ãƒ¢ãƒ‡ãƒ«ã¯å³åº§ã«å®Œäº†ã‚’é€šçŸ¥
                yield "TASK_DONE"
                return

            print(f"\n=== ç”»åƒ {image_index} (doc_id: {doc_id}, img_id: {img_id}) - {model} ã§ã®å‡¦ç† ===")

            if model == "meta/llama-4-maverick-17b-128e-instruct-fp8":
                llm = ChatOCIGenAI(
                    model_id="meta.llama-4-maverick-17b-128e-instruct-fp8",
                    provider="meta",
                    service_endpoint=f"https://inference.generativeai.{region}.oci.oraclecloud.com",
                    compartment_id=os.environ["OCI_COMPARTMENT_OCID"],
                    model_kwargs={"temperature": 0.0, "top_p": 0.75, "seed": 42, "max_tokens": 3600},
                )
            elif model == "meta/llama-4-scout-17b-16e-instruct":
                llm = ChatOCIGenAI(
                    model_id="meta.llama-4-scout-17b-16e-instruct",
                    provider="meta",
                    service_endpoint=f"https://inference.generativeai.{region}.oci.oraclecloud.com",
                    compartment_id=os.environ["OCI_COMPARTMENT_OCID"],
                    model_kwargs={"temperature": 0.0, "top_p": 0.75, "seed": 42, "max_tokens": 3600},
                )
            elif model == "meta/llama-3-2-90b-vision":
                llm = ChatOCIGenAI(
                    model_id="meta.llama-3.2-90b-vision-instruct",
                    provider="meta",
                    service_endpoint=f"https://inference.generativeai.{region}.oci.oraclecloud.com",
                    compartment_id=os.environ["OCI_COMPARTMENT_OCID"],
                    model_kwargs={"temperature": 0.0, "top_p": 0.75, "seed": 42, "max_tokens": 3600},
                )
            elif model == "openai/gpt-4o":
                load_dotenv(find_dotenv())
                llm = ChatOpenAI(
                    model="gpt-4o",
                    temperature=0,
                    top_p=0.75,
                    seed=42,
                    max_tokens=None,
                    timeout=None,
                    max_retries=2,
                    api_key=os.environ["OPENAI_API_KEY"],
                    base_url=os.environ["OPENAI_BASE_URL"],
                )
            elif model == "azure_openai/gpt-4o":
                load_dotenv(find_dotenv())
                llm = AzureChatOpenAI(
                    deployment_name="gpt-4o",
                    temperature=0,
                    top_p=0.75,
                    seed=42,
                    max_tokens=None,
                    timeout=None,
                    max_retries=2,
                    azure_endpoint=os.environ["AZURE_OPENAI_ENDPOINT_GPT_4O"],
                    openai_api_key=os.environ["AZURE_OPENAI_API_KEY"],
                    openai_api_version=os.environ["AZURE_OPENAI_API_VERSION_GPT_4O"],
                )
            else:
                # æœªå¯¾å¿œã®ãƒ¢ãƒ‡ãƒ«ã¯å³åº§ã«å®Œäº†ã‚’é€šçŸ¥
                yield "TASK_DONE"
                return

            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ
            prompt_text = get_image_qa_prompt(query_text, custom_image_prompt)
            prompt_text = prompt_text.replace('{{query_text}}', '{query_text}')
            human_message = HumanMessage(content=[
                {
                    "type": "text",
                    "text": prompt_text
                },
                {
                    "type": "image_url",
                    "image_url": {"url": image_url},
                },
            ])
            messages = [human_message]

            # LLMã«é€ä¿¡ã—ã¦å›ç­”ã‚’å–å¾—
            start_time = time.time()
            # langfuse_handler = CallbackHandler(
            #     secret_key=os.environ["LANGFUSE_SECRET_KEY"],
            #     public_key=os.environ["LANGFUSE_PUBLIC_KEY"],
            #     host=os.environ["LANGFUSE_HOST"],
            # )

            # è¡¨ç¤ºç”¨ã«ç”»åƒã‚’åœ§ç¸®
            compressed_image_url = compress_image_for_display(image_url)

            # ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±ã‚’æœ€åˆã«yieldï¼ˆåœ§ç¸®ã•ã‚ŒãŸç”»åƒã‚’ä½¿ç”¨ï¼‰
            header_text = f"\n\n---\n\n![ç”»åƒ]({compressed_image_url})\n\n**ç”»åƒ {image_index} (doc_id: {doc_id}, img_id: {img_id}) ã«ã‚ˆã‚‹å›ç­”ï¼š**\n\n"
            # header_text = f"\n\n**ç”»åƒ {image_index} (doc_id: {doc_id}, img_id: {img_id}) ã«ã‚ˆã‚‹å›ç­”ï¼š**\n\n"
            yield header_text

            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§å›ç­”ã‚’å–å¾—
            # Avoid for: Error uploading media: HTTPConnectionPool(host='minio', port=9000)
            # async for chunk in llm.astream(messages, config={"callbacks": [langfuse_handler],
            #                                                  "metadata": {"ls_model_name": model}}):
            async for chunk in llm.astream(messages):
                if chunk.content:
                    print(chunk.content, end="", flush=True)
                    yield chunk.content

            end_time = time.time()
            inference_time = end_time - start_time
            print(f"\n\næ¨è«–æ™‚é–“: {inference_time:.2f}ç§’")
            print(f"=== {model} ã§ã®å‡¦ç†å®Œäº† ===\n")

            # æ¨è«–æ™‚é–“ã‚’è¿½åŠ 
            yield f"\n\næ¨è«–æ™‚é–“: {inference_time:.2f}ç§’\n\n"
            yield "TASK_DONE"

        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ ({model}): {e}")
            # è¡¨ç¤ºç”¨ã«ç”»åƒã‚’åœ§ç¸®
            compressed_image_url = compress_image_for_display(image_url)
            error_text = f"\n\n---\n\n![ç”»åƒ]({compressed_image_url})\n\n**ç”»åƒ {image_index} (doc_id: {doc_id}, img_id: {img_id}) ã«ã‚ˆã‚‹å›ç­”ï¼š**\n\nã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\n\n"
            # error_text = f"\n\n**ç”»åƒ {image_index} (doc_id: {doc_id}, img_id: {img_id}) ã«ã‚ˆã‚‹å›ç­”ï¼š**\n\nã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\n\n"
            yield error_text
            yield "TASK_DONE"
        finally:
            # ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼šLLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®æ¥ç¶šã‚’é©åˆ‡ã«é–‰ã˜ã‚‹
            try:
                await cleanup_llm_client_async(llm)
                print(f"ãƒ¢ãƒ‡ãƒ« {model} ã®LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸ")
            except Exception as cleanup_error:
                print(f"ãƒ¢ãƒ‡ãƒ« {model} ã®LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {cleanup_error}")
            finally:
                llm = None  # å‚ç…§ã‚’ã‚¯ãƒªã‚¢

                # è¿½åŠ ã®å¼·åˆ¶ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                import gc
                gc.collect()  # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ

    # å„ãƒ¢ãƒ‡ãƒ«ã®ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’ä½œæˆ
    llama_4_maverick_gen = create_model_task("meta/llama-4-maverick-17b-128e-instruct-fp8")
    llama_4_scout_gen = create_model_task("meta/llama-4-scout-17b-16e-instruct")
    llama_3_2_90b_vision_gen = create_model_task("meta/llama-3-2-90b-vision")
    openai_gpt4o_gen = create_model_task("openai/gpt-4o")
    azure_openai_gpt4o_gen = create_model_task("azure_openai/gpt-4o")

    # å„ãƒ¢ãƒ‡ãƒ«ã®å¿œç­”ã‚’è“„ç©
    llama_4_maverick_response = ""
    llama_4_scout_response = ""
    llama_3_2_90b_vision_response = ""
    openai_gpt4o_response = ""
    azure_openai_gpt4o_response = ""

    # å„ãƒ¢ãƒ‡ãƒ«ã®çŠ¶æ…‹ã‚’è¿½è·¡
    responses_status = ["", "", "", "", ""]

    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šï¼ˆæœ€å¤§5åˆ†ï¼‰
    import asyncio
    timeout_seconds = 300
    start_time = time.time()

    try:
        while True:
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒã‚§ãƒƒã‚¯
            if time.time() - start_time > timeout_seconds:
                print(f"ç”»åƒå‡¦ç†ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸï¼ˆ{timeout_seconds}ç§’ï¼‰")
                break

            responses = ["", "", "", "", ""]
            generators = [llama_4_maverick_gen, llama_4_scout_gen, llama_3_2_90b_vision_gen, openai_gpt4o_gen,
                          azure_openai_gpt4o_gen]

            for i, gen in enumerate(generators):
                if responses_status[i] == "TASK_DONE":
                    continue

                try:
                    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§anextã‚’å®Ÿè¡Œ
                    response = await asyncio.wait_for(anext(gen), timeout=30.0)
                    if response:
                        if response == "TASK_DONE":
                            responses_status[i] = response
                        else:
                            responses[i] = response
                except StopAsyncIteration:
                    responses_status[i] = "TASK_DONE"
                except asyncio.TimeoutError:
                    print(f"ãƒ¢ãƒ‡ãƒ« {i} ã®å‡¦ç†ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ")
                    responses_status[i] = "TASK_DONE"
                except Exception as e:
                    print(f"ãƒ¢ãƒ‡ãƒ« {i} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                    responses_status[i] = "TASK_DONE"

            # å¿œç­”ã‚’è“„ç©
            llama_4_maverick_response += responses[0]
            llama_4_scout_response += responses[1]
            llama_3_2_90b_vision_response += responses[2]
            openai_gpt4o_response += responses[3]
            azure_openai_gpt4o_response += responses[4]

            # ç¾åœ¨ã®çŠ¶æ…‹ã‚’yield
            yield {
                "meta/llama-4-maverick-17b-128e-instruct-fp8": llama_4_maverick_response,
                "meta/llama-4-scout-17b-16e-instruct": llama_4_scout_response,
                "meta/llama-3-2-90b-vision": llama_3_2_90b_vision_response,
                "openai/gpt-4o": openai_gpt4o_response,
                "azure_openai/gpt-4o": azure_openai_gpt4o_response
            }

            # ã™ã¹ã¦ã®ã‚¿ã‚¹ã‚¯ãŒå®Œäº†ã—ãŸã‹ãƒã‚§ãƒƒã‚¯
            if all(response_status == "TASK_DONE" for response_status in responses_status):
                print("All image processing tasks completed")
                break

    finally:
        # æœ€çµ‚çš„ãªãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼šã™ã¹ã¦ã®ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’é©åˆ‡ã«é–‰ã˜ã‚‹
        generators = [llama_4_maverick_gen, llama_4_scout_gen, llama_3_2_90b_vision_gen, openai_gpt4o_gen,
                      azure_openai_gpt4o_gen]
        generator_names = ["llama_4_maverick", "llama_4_scout", "llama_3_2_90b_vision", "openai_gpt4o",
                           "azure_openai_gpt4o"]

        for i, gen in enumerate(generators):
            try:
                # éåŒæœŸã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ã®å ´åˆ
                if hasattr(gen, 'aclose'):
                    await gen.aclose()
                    print(f"ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ {generator_names[i]} ã®éåŒæœŸã‚¯ãƒ­ãƒ¼ã‚ºãŒå®Œäº†ã—ã¾ã—ãŸ")
                elif hasattr(gen, 'close'):
                    gen.close()
                    print(f"ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ {generator_names[i]} ã®åŒæœŸã‚¯ãƒ­ãƒ¼ã‚ºãŒå®Œäº†ã—ã¾ã—ãŸ")

                # ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼å†…ã®LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚‚ç¢ºå®Ÿã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                if hasattr(gen, 'gi_frame') and gen.gi_frame is not None:
                    # ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ãŒã¾ã å®Ÿè¡Œä¸­ã®å ´åˆã€å¼·åˆ¶çµ‚äº†
                    try:
                        gen.close()
                        print(f"å®Ÿè¡Œä¸­ã®ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ {generator_names[i]} ã‚’å¼·åˆ¶çµ‚äº†ã—ã¾ã—ãŸ")
                    except GeneratorExit:
                        pass  # æ­£å¸¸ãªçµ‚äº†
                    except Exception as force_close_error:
                        print(f"ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ {generator_names[i]} ã®å¼·åˆ¶çµ‚äº†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {force_close_error}")

            except Exception as cleanup_error:
                print(f"ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ {generator_names[i]} ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {cleanup_error}")

        # è»½é‡ãªHTTPæ¥ç¶šã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        await lightweight_cleanup()

        # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å¼·åˆ¶å®Ÿè¡Œã—ã¦ãƒªã‚½ãƒ¼ã‚¹ã‚’è§£æ”¾
        import gc
        gc.collect()
        print("å˜ä¸€ç”»åƒå‡¦ç†ã®ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸ")


async def process_multiple_images_streaming(image_data_list, query_text, llm_answer_checkbox_group, target_models,
                                            custom_image_prompt=None):
    """
    è¤‡æ•°ã®ç”»åƒã‚’ä¸€åº¦ã«VLMã«é€ä¿¡ã—ã¦å‡¦ç†ã—ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å½¢å¼ã§å›ç­”ã‚’è¿”ã™

    Args:
        image_data_list: ç”»åƒãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ [(base64_data, doc_id, img_id), ...]
        query_text: ã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆ
        llm_answer_checkbox_group: é¸æŠã•ã‚ŒãŸLLMãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆ
        target_models: å¯¾è±¡ãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆ
        custom_image_prompt: ã‚«ã‚¹ã‚¿ãƒ ç”»åƒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

    Yields:
        dict: å„ãƒ¢ãƒ‡ãƒ«ã®éƒ¨åˆ†çš„ãªå›ç­”ã‚’å«ã‚€è¾æ›¸
    """
    if custom_image_prompt:
        custom_image_prompt = custom_image_prompt.replace('{{query_text}}', '{query_text}')

    region = get_region()

    # ç”»åƒURLãƒªã‚¹ãƒˆã‚’ä½œæˆ
    image_urls = []
    for base64_data, doc_id, img_id in image_data_list:
        image_url = f"data:image/png;base64,{base64_data}"
        image_urls.append(image_url)

    print(f"è¤‡æ•°ç”»åƒå‡¦ç†é–‹å§‹: {len(image_urls)}æšã®ç”»åƒã‚’ä¸€æ‹¬å‡¦ç†")

    # å„ãƒ¢ãƒ‡ãƒ«ã®ã‚¿ã‚¹ã‚¯ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’ä½œæˆ
    async def create_model_task(model):
        llm = None  # LLMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’åˆæœŸåŒ–
        try:
            if model not in llm_answer_checkbox_group:
                # é¸æŠã•ã‚Œã¦ã„ãªã„ãƒ¢ãƒ‡ãƒ«ã¯å³åº§ã«å®Œäº†ã‚’é€šçŸ¥
                yield "TASK_DONE"
                return

            print(f"\n=== è¤‡æ•°ç”»åƒ ({len(image_urls)}æš) - {model} ã§ã®å‡¦ç† ===")

            if model == "meta/llama-4-maverick-17b-128e-instruct-fp8":
                llm = ChatOCIGenAI(
                    model_id="meta.llama-4-maverick-17b-128e-instruct-fp8",
                    provider="meta",
                    service_endpoint=f"https://inference.generativeai.{region}.oci.oraclecloud.com",
                    compartment_id=os.environ["OCI_COMPARTMENT_OCID"],
                    model_kwargs={"temperature": 0.0, "top_p": 0.75, "seed": 42, "max_tokens": 3600},
                )
            elif model == "meta/llama-4-scout-17b-16e-instruct":
                llm = ChatOCIGenAI(
                    model_id="meta.llama-4-scout-17b-16e-instruct",
                    provider="meta",
                    service_endpoint=f"https://inference.generativeai.{region}.oci.oraclecloud.com",
                    compartment_id=os.environ["OCI_COMPARTMENT_OCID"],
                    model_kwargs={"temperature": 0.0, "top_p": 0.75, "seed": 42, "max_tokens": 3600},
                )
            elif model == "meta/llama-3-2-90b-vision":
                llm = ChatOCIGenAI(
                    model_id="meta.llama-3.2-90b-vision-instruct",
                    provider="meta",
                    service_endpoint=f"https://inference.generativeai.{region}.oci.oraclecloud.com",
                    compartment_id=os.environ["OCI_COMPARTMENT_OCID"],
                    model_kwargs={"temperature": 0.0, "top_p": 0.75, "seed": 42, "max_tokens": 3600},
                )
            elif model == "openai/gpt-4o":
                load_dotenv(find_dotenv())
                llm = ChatOpenAI(
                    model="gpt-4o",
                    temperature=0,
                    top_p=0.75,
                    seed=42,
                    max_tokens=None,
                    timeout=None,
                    max_retries=2,
                    api_key=os.environ["OPENAI_API_KEY"],
                    base_url=os.environ["OPENAI_BASE_URL"],
                )
            elif model == "azure_openai/gpt-4o":
                load_dotenv(find_dotenv())
                llm = AzureChatOpenAI(
                    deployment_name="gpt-4o",
                    temperature=0,
                    top_p=0.75,
                    seed=42,
                    max_tokens=None,
                    timeout=None,
                    max_retries=2,
                    azure_endpoint=os.environ["AZURE_OPENAI_ENDPOINT_GPT_4O"],
                    openai_api_key=os.environ["AZURE_OPENAI_API_KEY"],
                    openai_api_version=os.environ["AZURE_OPENAI_API_VERSION_GPT_4O"],
                )
            else:
                # æœªå¯¾å¿œã®ãƒ¢ãƒ‡ãƒ«ã¯å³åº§ã«å®Œäº†ã‚’é€šçŸ¥
                yield "TASK_DONE"
                return

            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆï¼ˆè¤‡æ•°ç”»åƒå¯¾å¿œï¼‰
            prompt_text = get_image_qa_prompt(query_text, custom_image_prompt)
            prompt_text = prompt_text.replace('{{query_text}}', '{query_text}')

            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ§‹ç¯‰
            message_content = [{"type": "text", "text": prompt_text}]

            # å„ç”»åƒã‚’è¿½åŠ 
            for i, image_url in enumerate(image_urls):
                message_content.append({
                    "type": "image_url",
                    "image_url": {"url": image_url},
                })

            human_message = HumanMessage(content=message_content)
            messages = [human_message]

            # LLMã«é€ä¿¡ã—ã¦å›ç­”ã‚’å–å¾—
            start_time = time.time()

            # è¡¨ç¤ºç”¨ã«ç”»åƒã‚’åœ§ç¸®ã—ã¦ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±ã‚’ä½œæˆ
            compressed_images_text = ""
            for i, (image_url, (_, doc_id, img_id)) in enumerate(zip(image_urls, image_data_list), 1):
                compressed_image_url = compress_image_for_display(image_url)
                compressed_images_text += f"\n\n![ç”»åƒ{i}]({compressed_image_url})\n"

            # ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±ã‚’æœ€åˆã«yield
            header_text = f"\n\n---\n{compressed_images_text}\n**{len(image_urls)}æšã®ç”»åƒã«ã‚ˆã‚‹å›ç­”ï¼š**\n\n"
            yield header_text

            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§å›ç­”ã‚’å–å¾—
            async for chunk in llm.astream(messages):
                if chunk.content:
                    print(chunk.content, end="", flush=True)
                    yield chunk.content

            end_time = time.time()
            inference_time = end_time - start_time
            print(f"\n\næ¨è«–æ™‚é–“: {inference_time:.2f}ç§’")
            print(f"=== {model} ã§ã®å‡¦ç†å®Œäº† ===\n")

            # æ¨è«–æ™‚é–“ã‚’è¿½åŠ 
            yield f"\n\næ¨è«–æ™‚é–“: {inference_time:.2f}ç§’\n\n"
            yield "TASK_DONE"

        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ ({model}): {e}")
            # è¡¨ç¤ºç”¨ã«ç”»åƒã‚’åœ§ç¸®ã—ã¦ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ
            compressed_images_text = ""
            for i, (image_url, (_, doc_id, img_id)) in enumerate(zip(image_urls, image_data_list), 1):
                compressed_image_url = compress_image_for_display(image_url)
                compressed_images_text += f"\n\n![ç”»åƒ{i}]({compressed_image_url})\n"

            error_text = f"\n\n---\n{compressed_images_text}\n**{len(image_urls)}æšã®ç”»åƒã«ã‚ˆã‚‹å›ç­”ï¼š**\n\nã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\n\n"
            yield error_text
            yield "TASK_DONE"
        finally:
            # ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼šLLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®æ¥ç¶šã‚’é©åˆ‡ã«é–‰ã˜ã‚‹
            await cleanup_llm_client_async(llm)
            llm = None  # å‚ç…§ã‚’ã‚¯ãƒªã‚¢

    # å„ãƒ¢ãƒ‡ãƒ«ã®ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’ä½œæˆ
    llama_4_maverick_gen = create_model_task("meta/llama-4-maverick-17b-128e-instruct-fp8")
    llama_4_scout_gen = create_model_task("meta/llama-4-scout-17b-16e-instruct")
    llama_3_2_90b_vision_gen = create_model_task("meta/llama-3-2-90b-vision")
    openai_gpt4o_gen = create_model_task("openai/gpt-4o")
    azure_openai_gpt4o_gen = create_model_task("azure_openai/gpt-4o")

    # å„ãƒ¢ãƒ‡ãƒ«ã®å¿œç­”ã‚’è“„ç©
    llama_4_maverick_response = ""
    llama_4_scout_response = ""
    llama_3_2_90b_vision_response = ""
    openai_gpt4o_response = ""
    azure_openai_gpt4o_response = ""

    # å„ãƒ¢ãƒ‡ãƒ«ã®çŠ¶æ…‹ã‚’è¿½è·¡
    responses_status = ["", "", "", "", ""]

    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šï¼ˆæœ€å¤§5åˆ†ï¼‰
    import asyncio
    timeout_seconds = 300
    start_time = time.time()

    try:
        while True:
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒã‚§ãƒƒã‚¯
            if time.time() - start_time > timeout_seconds:
                print(f"è¤‡æ•°ç”»åƒå‡¦ç†ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸï¼ˆ{timeout_seconds}ç§’ï¼‰")
                break

            responses = ["", "", "", "", ""]
            generators = [llama_4_maverick_gen, llama_4_scout_gen, llama_3_2_90b_vision_gen, openai_gpt4o_gen,
                          azure_openai_gpt4o_gen]

            for i, gen in enumerate(generators):
                if responses_status[i] == "TASK_DONE":
                    continue

                try:
                    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§anextã‚’å®Ÿè¡Œ
                    response = await asyncio.wait_for(anext(gen), timeout=30.0)
                    if response:
                        if response == "TASK_DONE":
                            responses_status[i] = response
                        else:
                            responses[i] = response
                except StopAsyncIteration:
                    responses_status[i] = "TASK_DONE"
                except asyncio.TimeoutError:
                    print(f"ãƒ¢ãƒ‡ãƒ« {i} ã®å‡¦ç†ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ")
                    responses_status[i] = "TASK_DONE"
                except Exception as e:
                    print(f"ãƒ¢ãƒ‡ãƒ« {i} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                    responses_status[i] = "TASK_DONE"

            # å¿œç­”ã‚’è“„ç©
            llama_4_maverick_response += responses[0]
            llama_4_scout_response += responses[1]
            llama_3_2_90b_vision_response += responses[2]
            openai_gpt4o_response += responses[3]
            azure_openai_gpt4o_response += responses[4]

            # ç¾åœ¨ã®çŠ¶æ…‹ã‚’yield
            yield {
                "meta/llama-4-maverick-17b-128e-instruct-fp8": llama_4_maverick_response,
                "meta/llama-4-scout-17b-16e-instruct": llama_4_scout_response,
                "meta/llama-3-2-90b-vision": llama_3_2_90b_vision_response,
                "openai/gpt-4o": openai_gpt4o_response,
                "azure_openai/gpt-4o": azure_openai_gpt4o_response
            }

            # ã™ã¹ã¦ã®ã‚¿ã‚¹ã‚¯ãŒå®Œäº†ã—ãŸã‹ãƒã‚§ãƒƒã‚¯
            if all(response_status == "TASK_DONE" for response_status in responses_status):
                print("All multiple image processing tasks completed")
                break

    finally:
        # æœ€çµ‚çš„ãªãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼šã™ã¹ã¦ã®ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’é©åˆ‡ã«é–‰ã˜ã‚‹
        generators = [llama_4_maverick_gen, llama_4_scout_gen, llama_3_2_90b_vision_gen, openai_gpt4o_gen,
                      azure_openai_gpt4o_gen]
        generator_names = ["llama_4_maverick", "llama_4_scout", "llama_3_2_90b_vision", "openai_gpt4o",
                           "azure_openai_gpt4o"]

        for i, gen in enumerate(generators):
            try:
                # éåŒæœŸã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ã®å ´åˆ
                if hasattr(gen, 'aclose'):
                    await gen.aclose()
                    print(f"è¤‡æ•°ç”»åƒå‡¦ç†: ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ {generator_names[i]} ã®éåŒæœŸã‚¯ãƒ­ãƒ¼ã‚ºãŒå®Œäº†ã—ã¾ã—ãŸ")
                elif hasattr(gen, 'close'):
                    gen.close()
                    print(f"è¤‡æ•°ç”»åƒå‡¦ç†: ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ {generator_names[i]} ã®åŒæœŸã‚¯ãƒ­ãƒ¼ã‚ºãŒå®Œäº†ã—ã¾ã—ãŸ")

                # ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼å†…ã®LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚‚ç¢ºå®Ÿã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                if hasattr(gen, 'gi_frame') and gen.gi_frame is not None:
                    # ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ãŒã¾ã å®Ÿè¡Œä¸­ã®å ´åˆã€å¼·åˆ¶çµ‚äº†
                    try:
                        gen.close()
                        print(f"è¤‡æ•°ç”»åƒå‡¦ç†: å®Ÿè¡Œä¸­ã®ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ {generator_names[i]} ã‚’å¼·åˆ¶çµ‚äº†ã—ã¾ã—ãŸ")
                    except GeneratorExit:
                        pass  # æ­£å¸¸ãªçµ‚äº†
                    except Exception as force_close_error:
                        print(
                            f"è¤‡æ•°ç”»åƒå‡¦ç†: ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ {generator_names[i]} ã®å¼·åˆ¶çµ‚äº†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {force_close_error}")

            except Exception as cleanup_error:
                print(
                    f"è¤‡æ•°ç”»åƒå‡¦ç†: ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ {generator_names[i]} ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {cleanup_error}")

        # è»½é‡ãªHTTPæ¥ç¶šã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        await lightweight_cleanup()

        # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å¼·åˆ¶å®Ÿè¡Œã—ã¦ãƒªã‚½ãƒ¼ã‚¹ã‚’è§£æ”¾
        import gc
        gc.collect()
        print("è¤‡æ•°ç”»åƒå‡¦ç†ã®ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸ")


async def process_image_answers_streaming(
        search_result,
        use_image,
        single_image_processing,
        llm_answer_checkbox_group,
        query_text,
        llama_4_maverick_image_answer_text,
        llama_4_scout_image_answer_text,
        llama_3_2_90b_vision_image_answer_text,
        openai_gpt4o_image_answer_text,
        azure_openai_gpt4o_image_answer_text,
        custom_image_prompt=None
):
    """
    ç”»åƒã‚’ä½¿ã£ã¦å›ç­”ãŒã‚ªãƒ³ã®å ´åˆã€æ¤œç´¢çµæœã‹ã‚‰ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€
    é¸æŠã•ã‚ŒãŸLLMãƒ¢ãƒ‡ãƒ«ã§ç”»åƒå‡¦ç†ã‚’è¡Œã„ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å½¢å¼ã§å›ç­”ã‚’å‡ºåŠ›ã™ã‚‹

    å‡¦ç†ã®æµã‚Œï¼š
    1. æ¤œç´¢çµæœã‹ã‚‰doc_idã¨embed_idã®ãƒšã‚¢ã‚’æŠ½å‡º
    2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å¯¾å¿œã™ã‚‹ç”»åƒã®base64ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆæœ€å¤§10å€‹ã¾ã§ï¼‰
    3. å–å¾—ã—ãŸç”»åƒã‚’å„é¸æŠã•ã‚ŒãŸLLMãƒ¢ãƒ‡ãƒ«ã§ä¸¦è¡Œå‡¦ç†
    4. ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å½¢å¼ã§å›ç­”ã‚’å‡ºåŠ›

    æ³¨æ„ï¼šãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨å¿œç­”æ™‚é–“ã‚’è€ƒæ…®ã—ã€å‡¦ç†ã™ã‚‹ç”»åƒæ•°ã¯æœ€å¤§10å€‹ã«åˆ¶é™ã•ã‚Œã¦ã„ã¾ã™ã€‚

    Args:
        search_result: æ¤œç´¢çµæœ
        use_image: ç”»åƒã‚’ä½¿ã£ã¦å›ç­”ã™ã‚‹ã‹ã©ã†ã‹
        llm_answer_checkbox_group: é¸æŠã•ã‚ŒãŸLLMãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆ
        query_text: ã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆ
        llama_4_maverick_image_answer_text: Llama 4 Maverick ã®ç”»åƒå›ç­”ãƒ†ã‚­ã‚¹ãƒˆ
        llama_4_scout_image_answer_text: Llama 4 Scout ã®ç”»åƒå›ç­”ãƒ†ã‚­ã‚¹ãƒˆ
        llama_3_2_90b_vision_image_answer_text: Llama 3.2 90B Vision ã®ç”»åƒå›ç­”ãƒ†ã‚­ã‚¹ãƒˆ
        openai_gpt4o_image_answer_text: OpenAI GPT-4o ã®ç”»åƒå›ç­”ãƒ†ã‚­ã‚¹ãƒˆ
        azure_openai_gpt4o_image_answer_text: Azure OpenAI GPT-4o ã®ç”»åƒå›ç­”ãƒ†ã‚­ã‚¹ãƒˆ
        custom_image_prompt: ã‚«ã‚¹ã‚¿ãƒ ç”»åƒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

    Yields:
        tuple: å„ãƒ¢ãƒ‡ãƒ«ã®æ›´æ–°ã•ã‚ŒãŸç”»åƒå›ç­”ã‚’å«ã‚€Gradio Markdownã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ã‚¿ãƒ—ãƒ«
    """
    print("process_image_answers_streaming() é–‹å§‹...")

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ—ãƒ¼ãƒ«ã®å¥åº·çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯
    if not check_database_pool_health():
        print("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ—ãƒ¼ãƒ«ã«å•é¡ŒãŒã‚ã‚Šã¾ã™")
        yield (
            gr.Markdown(value=llama_4_maverick_image_answer_text),
            gr.Markdown(value=llama_4_scout_image_answer_text),
            gr.Markdown(value=llama_3_2_90b_vision_image_answer_text),
            gr.Markdown(value=openai_gpt4o_image_answer_text),
            gr.Markdown(value=azure_openai_gpt4o_image_answer_text)
        )
        return

    # ç”»åƒã‚’ä½¿ã£ã¦å›ç­”ãŒã‚ªãƒ•ã®å ´åˆã¯ä½•ã‚‚ã—ãªã„
    if not use_image:
        print("ç”»åƒã‚’ä½¿ã£ã¦å›ç­”ãŒã‚ªãƒ•ã®ãŸã‚ã€base64_dataå–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        yield (
            gr.Markdown(value=llama_4_maverick_image_answer_text),
            gr.Markdown(value=llama_4_scout_image_answer_text),
            gr.Markdown(value=llama_3_2_90b_vision_image_answer_text),
            gr.Markdown(value=openai_gpt4o_image_answer_text),
            gr.Markdown(value=azure_openai_gpt4o_image_answer_text)
        )
        return

    # æ¤œç´¢çµæœãŒç©ºã®å ´åˆã¯ä½•ã‚‚ã—ãªã„
    if search_result.empty or (len(search_result) > 0 and search_result.iloc[0]['CONTENT'] == ''):
        print("æ¤œç´¢çµæœãŒç©ºã®ãŸã‚ã€base64_dataå–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        yield (
            gr.Markdown(value=llama_4_maverick_image_answer_text),
            gr.Markdown(value=llama_4_scout_image_answer_text),
            gr.Markdown(value=llama_3_2_90b_vision_image_answer_text),
            gr.Markdown(value=openai_gpt4o_image_answer_text),
            gr.Markdown(value=azure_openai_gpt4o_image_answer_text)
        )
        return

    # æŒ‡å®šã•ã‚ŒãŸLLMãƒ¢ãƒ‡ãƒ«ãŒãƒã‚§ãƒƒã‚¯ã•ã‚Œã¦ã„ã‚‹ã‹ã‚’ç¢ºèª
    target_models = [
        "meta/llama-4-maverick-17b-128e-instruct-fp8",
        "meta/llama-4-scout-17b-16e-instruct",
        "meta/llama-3-2-90b-vision",
        "openai/gpt-4o",
        "azure_openai/gpt-4o"
    ]

    # llm_answer_checkbox_groupã«æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®ã„ãšã‚Œã‹ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    has_target_model = any(model in llm_answer_checkbox_group for model in target_models)

    if not has_target_model:
        print(
            "å¯¾è±¡ã®LLMãƒ¢ãƒ‡ãƒ«ï¼ˆllama-4-maverick, llama-4-scout, llama-3-2-90b-vision, gpt-4oï¼‰ãŒãƒã‚§ãƒƒã‚¯ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€base64_dataå–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        yield (
            gr.Markdown(value=llama_4_maverick_image_answer_text),
            gr.Markdown(value=llama_4_scout_image_answer_text),
            gr.Markdown(value=llama_3_2_90b_vision_image_answer_text),
            gr.Markdown(value=openai_gpt4o_image_answer_text),
            gr.Markdown(value=azure_openai_gpt4o_image_answer_text)
        )
        return

    print("æ¡ä»¶ã‚’æº€ãŸã—ã¦ã„ã‚‹ãŸã‚ã€base64_dataã‚’å–å¾—ã—ã¾ã™...")

    try:
        # æ¤œç´¢çµæœã‹ã‚‰doc_idã¨embed_idã‚’å–å¾—
        doc_embed_pairs = []
        for _, row in search_result.iterrows():
            source = row['SOURCE']
            embed_id = row['EMBED_ID']
            if ':' in source:
                doc_id = source.split(':')[0]
                doc_embed_pair = (doc_id, embed_id)
                if doc_embed_pair not in doc_embed_pairs:
                    doc_embed_pairs.append(doc_embed_pair)

        if not doc_embed_pairs:
            print("æ¤œç´¢çµæœã‹ã‚‰doc_idã¨embed_idã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            yield (
                gr.Markdown(value=llama_4_maverick_image_answer_text),
                gr.Markdown(value=llama_4_scout_image_answer_text),
                gr.Markdown(value=llama_3_2_90b_vision_image_answer_text),
                gr.Markdown(value=openai_gpt4o_image_answer_text),
                gr.Markdown(value=azure_openai_gpt4o_image_answer_text)
            )
            return

        print(f"å–å¾—ã—ãŸdoc_id, embed_idãƒšã‚¢æ•°: {len(doc_embed_pairs)}")
        print(f"æœ€åˆã®5ãƒšã‚¢: {doc_embed_pairs[:5]}")

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰distinct base64_dataã‚’å–å¾—
        try:
            with pool.acquire() as conn:
                with conn.cursor() as cursor:
                    # ã¾ãš_image_embeddingãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰img_idã‚’å–å¾—
                    embed_where_conditions = []
                    for doc_id, embed_id in doc_embed_pairs:
                        embed_where_conditions.append(f"(doc_id = '{doc_id}' AND embed_id = {embed_id})")

                    embed_where_clause = " OR ".join(embed_where_conditions)

                    # _image_embeddingãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰img_idã‚’å–å¾—
                    get_img_ids_sql = f"""
                        SELECT doc_id, embed_id, img_id
                        FROM (
                            SELECT
                                doc_id,
                                embed_id,
                                img_id,
                                ROW_NUMBER() OVER (PARTITION BY doc_id, img_id ORDER BY embed_id ASC) as rn
                            FROM {DEFAULT_COLLECTION_NAME}_image_embedding
                            WHERE ({embed_where_clause})
                            AND img_id IS NOT NULL
                        ) subquery
                        WHERE rn = 1
                        ORDER BY doc_id, img_id ASC
                    """

                    print(f"img_idå–å¾—SQL: {get_img_ids_sql}")
                    cursor.execute(get_img_ids_sql)

                    doc_img_pairs = []
                    for row in cursor:
                        doc_id = row[0]
                        embed_id = row[1]
                        img_id = row[2]
                        doc_img_pair = (doc_id, img_id)
                        if doc_img_pair not in doc_img_pairs:
                            doc_img_pairs.append(doc_img_pair)
                            print(f"è¦‹ã¤ã‹ã£ãŸãƒšã‚¢: doc_id={doc_id}, embed_id={embed_id}, img_id={img_id}")

                    if not doc_img_pairs:
                        print("_image_embeddingãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰img_idã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                        yield (
                            gr.Markdown(value=llama_4_maverick_image_answer_text),
                            gr.Markdown(value=llama_4_scout_image_answer_text),
                            gr.Markdown(value=llama_3_2_90b_vision_image_answer_text),
                            gr.Markdown(value=openai_gpt4o_image_answer_text),
                            gr.Markdown(value=azure_openai_gpt4o_image_answer_text)
                        )
                        return

                    print(f"å–å¾—ã—ãŸdoc_id, img_idãƒšã‚¢æ•°: {len(doc_img_pairs)}")

                    # æ¬¡ã«_imageãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰base64_dataã‚’å–å¾—
                    img_where_conditions = []
                    for doc_id, img_id in doc_img_pairs:
                        img_where_conditions.append(f"(doc_id = '{doc_id}' AND img_id = {img_id})")

                    img_where_clause = " OR ".join(img_where_conditions)

                    # base64ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆæœ€å¤§10ä»¶ã¾ã§ï¼‰
                    select_sql = f"""
                    SELECT base64_data, doc_id, img_id
                    FROM {DEFAULT_COLLECTION_NAME}_image
                    WHERE ({img_where_clause})
                    AND base64_data IS NOT NULL
                    FETCH FIRST 10 ROWS ONLY
                    """

                    print(f"å®Ÿè¡Œã™ã‚‹SQL: {select_sql}")
                    cursor.execute(select_sql)

                    base64_data_list = []
                    for row in cursor:
                        if row[0] is not None:
                            try:
                                # CLOBã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®èª­ã¿å–ã‚Š
                                if hasattr(row[0], 'read'):
                                    base64_string = row[0].read()
                                    # 10MBåˆ¶é™ãƒã‚§ãƒƒã‚¯
                                    if len(base64_string) > 10 * 1024 * 1024:
                                        print(f"Base64ãƒ‡ãƒ¼ã‚¿ãŒå¤§ãã™ãã¾ã™ï¼ˆ{len(base64_string)}æ–‡å­—ï¼‰ã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                                        continue
                                else:
                                    base64_string = str(row[0])

                                base64_data_list.append((base64_string, row[1], row[2]))

                            except Exception as e:
                                print(f"CLOBèª­ã¿å–ã‚Šã‚¨ãƒ©ãƒ¼: {e}")
                                continue

                    print(f"å–å¾—ã—ãŸbase64_dataã®æ•°: {len(base64_data_list)}")

                    # åˆæœŸåŒ–ï¼šç¾åœ¨ã®ç”»åƒå›ç­”ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿æŒï¼ˆç´¯ç©ç”¨ï¼‰
                    accumulated_llama_4_maverick_text = llama_4_maverick_image_answer_text
                    accumulated_llama_4_scout_text = llama_4_scout_image_answer_text
                    accumulated_llama_3_2_90b_vision_text = llama_3_2_90b_vision_image_answer_text
                    accumulated_openai_gpt4o_text = openai_gpt4o_image_answer_text
                    accumulated_azure_openai_gpt4o_text = azure_openai_gpt4o_image_answer_text

                    # å‡¦ç†æ–¹å¼ã‚’é¸æŠï¼š1æšãšã¤å‡¦ç† vs ä¸€æ‹¬å‡¦ç†
                    if base64_data_list:
                        if single_image_processing:
                            # 1æšãšã¤å‡¦ç†ãƒ¢ãƒ¼ãƒ‰
                            print(f"å˜ä¸€ç”»åƒå‡¦ç†é–‹å§‹: {len(base64_data_list)}æšã®ç”»åƒã‚’1æšãšã¤å‡¦ç†ä¸­...")

                            # å„base64_dataã«å¯¾ã—ã¦LLMã§å‡¦ç†
                            for i, (base64_data, doc_id, img_id) in enumerate(base64_data_list, 1):
                                print(f"ç”»åƒ {i} (doc_id: {doc_id}, img_id: {img_id}) ã‚’å‡¦ç†ä¸­...")

                                # base64ãƒ‡ãƒ¼ã‚¿ã‚’data:image/png;base64,{base64_data}å½¢å¼ã«å¤‰æ›
                                image_url = f"data:image/png;base64,{base64_data}"

                                # å„ãƒ¢ãƒ‡ãƒ«ã®ç¾åœ¨ã®ç”»åƒã«å¯¾ã™ã‚‹å›ç­”ã‚’ä¿æŒ
                                current_image_llama_4_maverick = ""
                                current_image_llama_4_scout = ""
                                current_image_llama_3_2_90b_vision = ""
                                current_image_openai_gpt4o = ""
                                current_image_azure_openai_gpt4o = ""

                                # é¸æŠã•ã‚ŒãŸLLMãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦å‡¦ç†ã‚’å®Ÿè¡Œã—ã€çµæœã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§å–å¾—
                                async for llm_results in process_single_image_streaming(
                                        image_url,
                                        query_text,
                                        llm_answer_checkbox_group,
                                        target_models,
                                        i,
                                        doc_id,
                                        img_id,
                                        custom_image_prompt
                                ):
                                    # å„LLMã®çµæœã‚’ç¾åœ¨ã®ç”»åƒã®å›ç­”ã¨ã—ã¦æ›´æ–°
                                    if "meta/llama-4-maverick-17b-128e-instruct-fp8" in llm_results:
                                        current_image_llama_4_maverick = llm_results[
                                            "meta/llama-4-maverick-17b-128e-instruct-fp8"]

                                    if "meta/llama-4-scout-17b-16e-instruct" in llm_results:
                                        current_image_llama_4_scout = llm_results["meta/llama-4-scout-17b-16e-instruct"]

                                    if "meta/llama-3-2-90b-vision" in llm_results:
                                        current_image_llama_3_2_90b_vision = llm_results["meta/llama-3-2-90b-vision"]

                                    if "openai/gpt-4o" in llm_results:
                                        current_image_openai_gpt4o = llm_results["openai/gpt-4o"]

                                    if "azure_openai/gpt-4o" in llm_results:
                                        current_image_azure_openai_gpt4o = llm_results["azure_openai/gpt-4o"]

                                    # ç´¯ç©ãƒ†ã‚­ã‚¹ãƒˆã¨ç¾åœ¨ã®ç”»åƒã®å›ç­”ã‚’çµåˆã—ã¦è¡¨ç¤º
                                    current_llama_4_maverick_text = accumulated_llama_4_maverick_text + current_image_llama_4_maverick
                                    current_llama_4_scout_text = accumulated_llama_4_scout_text + current_image_llama_4_scout
                                    current_llama_3_2_90b_vision_text = accumulated_llama_3_2_90b_vision_text + current_image_llama_3_2_90b_vision
                                    current_openai_gpt4o_text = accumulated_openai_gpt4o_text + current_image_openai_gpt4o
                                    current_azure_openai_gpt4o_text = accumulated_azure_openai_gpt4o_text + current_image_azure_openai_gpt4o

                                    # æ›´æ–°ã•ã‚ŒãŸç”»åƒå›ç­”çµæœã‚’yield
                                    yield (
                                        gr.Markdown(value=current_llama_4_maverick_text),
                                        gr.Markdown(value=current_llama_4_scout_text),
                                        gr.Markdown(value=current_llama_3_2_90b_vision_text),
                                        gr.Markdown(value=current_openai_gpt4o_text),
                                        gr.Markdown(value=current_azure_openai_gpt4o_text)
                                    )

                                # ç¾åœ¨ã®ç”»åƒã®å‡¦ç†ãŒå®Œäº†ã—ãŸã‚‰ã€ç´¯ç©ãƒ†ã‚­ã‚¹ãƒˆã«è¿½åŠ 
                                accumulated_llama_4_maverick_text += current_image_llama_4_maverick
                                accumulated_llama_4_scout_text += current_image_llama_4_scout
                                accumulated_llama_3_2_90b_vision_text += current_image_llama_3_2_90b_vision
                                accumulated_openai_gpt4o_text += current_image_openai_gpt4o
                                accumulated_azure_openai_gpt4o_text += current_image_azure_openai_gpt4o
                        else:
                            # ä¸€æ‹¬å‡¦ç†ãƒ¢ãƒ¼ãƒ‰
                            print(f"è¤‡æ•°ç”»åƒä¸€æ‹¬å‡¦ç†é–‹å§‹: {len(base64_data_list)}æšã®ç”»åƒã‚’ä¸€æ‹¬å‡¦ç†ä¸­...")

                            # å„ãƒ¢ãƒ‡ãƒ«ã®å›ç­”ã‚’ä¿æŒ
                            current_llama_4_maverick = ""
                            current_llama_4_scout = ""
                            current_llama_3_2_90b_vision = ""
                            current_openai_gpt4o = ""
                            current_azure_openai_gpt4o = ""

                            # è¤‡æ•°ç”»åƒã‚’ä¸€æ‹¬å‡¦ç†
                            async for llm_results in process_multiple_images_streaming(
                                    base64_data_list,
                                    query_text,
                                    llm_answer_checkbox_group,
                                    target_models,
                                    custom_image_prompt
                            ):
                                # å„LLMã®çµæœã‚’è¤‡æ•°ç”»åƒã®å›ç­”ã¨ã—ã¦æ›´æ–°
                                if "meta/llama-4-maverick-17b-128e-instruct-fp8" in llm_results:
                                    current_llama_4_maverick = llm_results[
                                        "meta/llama-4-maverick-17b-128e-instruct-fp8"]

                                if "meta/llama-4-scout-17b-16e-instruct" in llm_results:
                                    current_llama_4_scout = llm_results["meta/llama-4-scout-17b-16e-instruct"]

                                if "meta/llama-3-2-90b-vision" in llm_results:
                                    current_llama_3_2_90b_vision = llm_results["meta/llama-3-2-90b-vision"]

                                if "openai/gpt-4o" in llm_results:
                                    current_openai_gpt4o = llm_results["openai/gpt-4o"]

                                if "azure_openai/gpt-4o" in llm_results:
                                    current_azure_openai_gpt4o = llm_results["azure_openai/gpt-4o"]

                                # ç´¯ç©ãƒ†ã‚­ã‚¹ãƒˆã¨è¤‡æ•°ç”»åƒã®å›ç­”ã‚’çµåˆã—ã¦è¡¨ç¤º
                                current_llama_4_maverick_text = accumulated_llama_4_maverick_text + current_llama_4_maverick
                                current_llama_4_scout_text = accumulated_llama_4_scout_text + current_llama_4_scout
                                current_llama_3_2_90b_vision_text = accumulated_llama_3_2_90b_vision_text + current_llama_3_2_90b_vision
                                current_openai_gpt4o_text = accumulated_openai_gpt4o_text + current_openai_gpt4o
                                current_azure_openai_gpt4o_text = accumulated_azure_openai_gpt4o_text + current_azure_openai_gpt4o

                                # æ›´æ–°ã•ã‚ŒãŸç”»åƒå›ç­”çµæœã‚’yield
                                yield (
                                    gr.Markdown(value=current_llama_4_maverick_text),
                                    gr.Markdown(value=current_llama_4_scout_text),
                                    gr.Markdown(value=current_llama_3_2_90b_vision_text),
                                    gr.Markdown(value=current_openai_gpt4o_text),
                                    gr.Markdown(value=current_azure_openai_gpt4o_text)
                                )

        except Exception as db_e:
            print(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {db_e}")
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ç¾åœ¨ã®çŠ¶æ…‹ã‚’yield
            yield (
                gr.Markdown(value=llama_4_maverick_image_answer_text),
                gr.Markdown(value=llama_4_scout_image_answer_text),
                gr.Markdown(value=llama_3_2_90b_vision_image_answer_text),
                gr.Markdown(value=openai_gpt4o_image_answer_text),
                gr.Markdown(value=azure_openai_gpt4o_image_answer_text)
            )
            return

    except Exception as e:
        print(f"base64_dataå–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ç¾åœ¨ã®çŠ¶æ…‹ã‚’yield
        yield (
            gr.Markdown(value=llama_4_maverick_image_answer_text),
            gr.Markdown(value=llama_4_scout_image_answer_text),
            gr.Markdown(value=llama_3_2_90b_vision_image_answer_text),
            gr.Markdown(value=openai_gpt4o_image_answer_text),
            gr.Markdown(value=azure_openai_gpt4o_image_answer_text)
        )

    finally:
        # ç”»åƒå‡¦ç†å®Œäº†å¾Œã®è»½é‡ãªãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        try:
            await lightweight_cleanup()
        except Exception as cleanup_error:
            print(f"è»½é‡ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {cleanup_error}")

    print("process_image_answers_streaming() å®Œäº†")


async def eval_by_ragas(
        query_text,
        doc_id_all_checkbox_input,
        doc_id_checkbox_group_input,
        search_result,
        llm_answer_checkbox_group,
        llm_evaluation_checkbox,
        system_text,
        standard_answer_text,
        xai_grok_3_response,
        command_a_response,
        llama_4_maverick_response,
        llama_4_scout_response,
        llama_3_3_70b_response,
        llama_3_2_90b_vision_response,
        openai_gpt4o_response,
        openai_gpt4_response,
        azure_openai_gpt4o_response,
        azure_openai_gpt4_response
):
    has_error = False
    if not query_text:
        has_error = True
        # gr.Warning("ã‚¯ã‚¨ãƒªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if not doc_id_all_checkbox_input and (not doc_id_checkbox_group_input or doc_id_checkbox_group_input == [""]):
        has_error = True
        # gr.Warning("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’é¸æŠã—ã¦ãã ã•ã„")
    if search_result.empty or (len(search_result) > 0 and search_result.iloc[0]['CONTENT'] == ''):
        has_error = True
        # gr.Warning("æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚è¨­å®šã‚‚ã—ãã¯ã‚¯ã‚¨ãƒªã‚’å¤‰æ›´ã—ã¦å†åº¦ã”ç¢ºèªãã ã•ã„ã€‚")
    if llm_evaluation_checkbox and (not llm_answer_checkbox_group or llm_answer_checkbox_group == [""]):
        has_error = True
        gr.Warning("LLM è©•ä¾¡ã‚’ã‚ªãƒ³ã«ã™ã‚‹å ´åˆã€å°‘ãªãã¨ã‚‚1ã¤ã®LLM ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„")
    if llm_evaluation_checkbox and not system_text:
        has_error = True
        gr.Warning("LLM è©•ä¾¡ã‚’ã‚ªãƒ³ã«ã™ã‚‹å ´åˆã€LLM è©•ä¾¡ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ»ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if llm_evaluation_checkbox and not standard_answer_text:
        has_error = True
        gr.Warning("LLM è©•ä¾¡ã‚’ã‚ªãƒ³ã«ã™ã‚‹å ´åˆã€LLM è©•ä¾¡ã®æ¨™æº–å›ç­”ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if has_error:
        yield (
            gr.Markdown(value=""),
            gr.Markdown(value=""),
            gr.Markdown(value=""),
            gr.Markdown(value=""),
            gr.Markdown(value=""),
            gr.Markdown(value=""),
            gr.Markdown(value=""),
            gr.Markdown(value=""),
            gr.Markdown(value=""),
            gr.Markdown(value=""),
            gr.Markdown(value=""),
            gr.Markdown(value=""),
            gr.Markdown(value="")
        )
        return

    def remove_last_line(text):
        if text:
            lines = text.splitlines()
            if lines[-1].startswith("æ¨è«–æ™‚é–“"):
                lines.pop()
            return '\n'.join(lines)
        else:
            return text

    if standard_answer_text:
        standard_answer_text = standard_answer_text.strip()
    else:
        standard_answer_text = "å…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
    print(f"{llm_evaluation_checkbox=}")
    if not llm_evaluation_checkbox:
        yield (
            gr.Markdown(value=""),
            gr.Markdown(value=""),
            gr.Markdown(value=""),
            gr.Markdown(value=""),
            gr.Markdown(value=""),
            gr.Markdown(value=""),
            gr.Markdown(value=""),
            gr.Markdown(value=""),
            gr.Markdown(value=""),
            gr.Markdown(value=""),
            gr.Markdown(value=""),
            gr.Markdown(value=""),
            gr.Markdown(value="")
        )
    else:
        xai_grok_3_checkbox = False
        command_a_checkbox = False
        llama_4_maverick_checkbox = False
        llama_4_scout_checkbox = False
        llama_3_3_70b_checkbox = False
        llama_3_2_90b_vision_checkbox = False
        openai_gpt4o_checkbox = False
        openai_gpt4_checkbox = False
        azure_openai_gpt4o_checkbox = False
        azure_openai_gpt4_checkbox = False
        if "xai/grok-3" in llm_answer_checkbox_group:
            xai_grok_3_checkbox = True
        if "cohere/command-a" in llm_answer_checkbox_group:
            command_a_checkbox = True
        if "meta/llama-4-maverick-17b-128e-instruct-fp8" in llm_answer_checkbox_group:
            llama_4_maverick_checkbox = True
        if "meta/llama-4-scout-17b-16e-instruct" in llm_answer_checkbox_group:
            llama_4_scout_checkbox = True
        if "meta/llama-3-3-70b" in llm_answer_checkbox_group:
            llama_3_3_70b_checkbox = True
        if "meta/llama-3-2-90b-vision" in llm_answer_checkbox_group:
            llama_3_2_90b_vision_checkbox = True
        if "openai/gpt-4o" in llm_answer_checkbox_group:
            openai_gpt4o_checkbox = True
        if "openai/gpt-4" in llm_answer_checkbox_group:
            openai_gpt4_checkbox = True
        if "azure_openai/gpt-4o" in llm_answer_checkbox_group:
            azure_openai_gpt4o_checkbox = True
        if "azure_openai/gpt-4" in llm_answer_checkbox_group:
            azure_openai_gpt4_checkbox = True

        xai_grok_3_response = remove_last_line(xai_grok_3_response)
        command_a_response = remove_last_line(command_a_response)
        llama_4_maverick_response = remove_last_line(llama_4_maverick_response)
        llama_4_scout_response = remove_last_line(llama_4_scout_response)
        llama_3_3_70b_response = remove_last_line(llama_3_3_70b_response)
        llama_3_2_90b_vision_response = remove_last_line(llama_3_2_90b_vision_response)
        openai_gpt4o_response = remove_last_line(openai_gpt4o_response)
        openai_gpt4_response = remove_last_line(openai_gpt4_response)
        azure_openai_gpt4o_response = remove_last_line(azure_openai_gpt4o_response)
        azure_openai_gpt4_response = remove_last_line(azure_openai_gpt4_response)

        xai_grok_3_user_text = f"""
-æ¨™æº–å›ç­”-
 {standard_answer_text}

-ä¸ãˆã‚‰ã‚ŒãŸå›ç­”-
 {xai_grok_3_response}

-å‡ºåŠ›-\nã€€"""

        command_a_user_text = f"""
-æ¨™æº–å›ç­”-
 {standard_answer_text}

-ä¸ãˆã‚‰ã‚ŒãŸå›ç­”-
 {command_a_response}

-å‡ºåŠ›-\nã€€"""



        llama_4_maverick_user_text = f"""
-æ¨™æº–å›ç­”-
{standard_answer_text}

-ä¸ãˆã‚‰ã‚ŒãŸå›ç­”-
{llama_4_maverick_response}

-å‡ºåŠ›-\nã€€"""

        llama_4_scout_user_text = f"""
-æ¨™æº–å›ç­”-
{standard_answer_text}

-ä¸ãˆã‚‰ã‚ŒãŸå›ç­”-
{llama_4_scout_response}

-å‡ºåŠ›-\nã€€"""

        llama_3_3_70b_user_text = f"""
-æ¨™æº–å›ç­”-
{standard_answer_text}

-ä¸ãˆã‚‰ã‚ŒãŸå›ç­”-
{llama_3_3_70b_response}

-å‡ºåŠ›-\nã€€"""

        llama_3_2_90b_vision_user_text = f"""
-æ¨™æº–å›ç­”-
{standard_answer_text}

-ä¸ãˆã‚‰ã‚ŒãŸå›ç­”-
{llama_3_2_90b_vision_response}

-å‡ºåŠ›-\nã€€"""

        openai_gpt4o_user_text = f"""
-æ¨™æº–å›ç­”-
{standard_answer_text}

-ä¸ãˆã‚‰ã‚ŒãŸå›ç­”-
{openai_gpt4o_response}

-å‡ºåŠ›-\nã€€"""

        openai_gpt4_user_text = f"""
-æ¨™æº–å›ç­”-
{standard_answer_text}

-ä¸ãˆã‚‰ã‚ŒãŸå›ç­”-
{openai_gpt4_response}

-å‡ºåŠ›-\nã€€"""

        azure_openai_gpt4o_user_text = f"""
-æ¨™æº–å›ç­”-
{standard_answer_text}

-ä¸ãˆã‚‰ã‚ŒãŸå›ç­”-
{azure_openai_gpt4o_response}

-å‡ºåŠ›-\nã€€"""

        azure_openai_gpt4_user_text = f"""
-æ¨™æº–å›ç­”-
{standard_answer_text}

-ä¸ãˆã‚‰ã‚ŒãŸå›ç­”-
{azure_openai_gpt4_response}

-å‡ºåŠ›-\nã€€"""

        eval_xai_grok_3_response = ""
        eval_command_a_response = ""
        eval_llama_4_maverick_response = ""
        eval_llama_4_scout_response = ""
        eval_llama_3_3_70b_response = ""
        eval_llama_3_2_90b_vision_response = ""
        eval_openai_gpt4o_response = ""
        eval_openai_gpt4_response = ""
        eval_azure_openai_gpt4o_response = ""
        eval_azure_openai_gpt4_response = ""

        async for xai_grok_3, command_a, llama_4_maverick, llama_4_scout, llama_3_3_70b, llama_3_2_90b_vision, gpt4o, gpt4, azure_gpt4o, azure_gpt4 in chat(
                system_text,
                xai_grok_3_user_text,
                command_a_user_text,
                None,
                llama_4_maverick_user_text,
                None,
                llama_4_scout_user_text,
                llama_3_3_70b_user_text,
                None,
                llama_3_2_90b_vision_user_text,
                openai_gpt4o_user_text,
                openai_gpt4_user_text,
                azure_openai_gpt4o_user_text,
                azure_openai_gpt4_user_text,
                xai_grok_3_checkbox,
                command_a_checkbox,
                llama_4_maverick_checkbox,
                llama_4_scout_checkbox,
                llama_3_3_70b_checkbox,
                llama_3_2_90b_vision_checkbox,
                openai_gpt4o_checkbox,
                openai_gpt4_checkbox,
                azure_openai_gpt4o_checkbox,
                azure_openai_gpt4_checkbox
        ):
            eval_xai_grok_3_response += xai_grok_3
            eval_command_a_response += command_a
            eval_llama_4_maverick_response += llama_4_maverick
            eval_llama_4_scout_response += llama_4_scout
            eval_llama_3_3_70b_response += llama_3_3_70b
            eval_llama_3_2_90b_vision_response += llama_3_2_90b_vision
            eval_openai_gpt4o_response += gpt4o
            eval_openai_gpt4_response += gpt4
            eval_azure_openai_gpt4o_response += azure_gpt4o
            eval_azure_openai_gpt4_response += azure_gpt4
            yield (
                gr.Markdown(value=eval_xai_grok_3_response),
                gr.Markdown(value=eval_command_a_response),
                gr.Markdown(value=eval_llama_4_maverick_response),
                gr.Markdown(value=eval_llama_4_scout_response),
                gr.Markdown(value=eval_llama_3_3_70b_response),
                gr.Markdown(value=eval_llama_3_2_90b_vision_response),
                gr.Markdown(value=eval_openai_gpt4o_response),
                gr.Markdown(value=eval_openai_gpt4_response),
                gr.Markdown(value=eval_azure_openai_gpt4o_response),
                gr.Markdown(value=eval_azure_openai_gpt4_response)
            )


def generate_download_file(
        search_result,
        llm_answer_checkbox_group,
        include_citation,
        llm_evaluation_checkbox,
        query_text,
        doc_id_all_checkbox_input,
        doc_id_checkbox_group_input,
        standard_answer_text,
        xai_grok_3_response,
        command_a_response,
        llama_4_maverick_response,
        llama_4_scout_response,
        llama_3_3_70b_response,
        llama_3_2_90b_vision_response,
        openai_gpt4o_response,
        openai_gpt4_response,
        azure_openai_gpt4o_response,
        azure_openai_gpt4_response,
        xai_grok_3_evaluation,
        command_a_evaluation,
        llama_4_maverick_evaluation,
        llama_4_scout_evaluation,
        llama_3_3_70b_evaluation,
        llama_3_2_90b_vision_evaluation,
        openai_gpt4o_evaluation,
        openai_gpt4_evaluation,
        azure_openai_gpt4o_evaluation,
        azure_openai_gpt4_evaluation
):
    if not query_text:
        return gr.DownloadButton(value=None, visible=False)
    if not doc_id_all_checkbox_input and (not doc_id_checkbox_group_input or doc_id_checkbox_group_input == [""]):
        return gr.DownloadButton(value=None, visible=False)
    if search_result.empty or (len(search_result) > 0 and search_result.iloc[0]['CONTENT'] == ''):
        return gr.DownloadButton(value=None, visible=False)
    # ã‚µãƒ³ãƒ—ãƒ«DataFrameã‚’ä½œæˆ
    if llm_evaluation_checkbox:
        standard_answer_text = standard_answer_text
    else:
        standard_answer_text = ""
    df1 = pd.DataFrame({'ã‚¯ã‚¨ãƒª': [query_text], 'æ¨™æº–å›ç­”': [standard_answer_text]})

    df2 = search_result

    if "xai/grok-3" in llm_answer_checkbox_group:
        xai_grok_3_response = xai_grok_3_response
        xai_grok_3_referenced_contexts = ""
        if include_citation:
            xai_grok_3_response, xai_grok_3_referenced_contexts = extract_citation(xai_grok_3_response)
        if llm_evaluation_checkbox:
            xai_grok_3_evaluation = xai_grok_3_evaluation
        else:
            xai_grok_3_evaluation = ""
    else:
        xai_grok_3_response = ""
        xai_grok_3_evaluation = ""
        xai_grok_3_referenced_contexts = ""

    if "cohere/command-a" in llm_answer_checkbox_group:
        command_a_response = command_a_response
        command_a_referenced_contexts = ""
        if include_citation:
            command_a_response, command_a_referenced_contexts = extract_citation(command_a_response)
        if llm_evaluation_checkbox:
            command_a_evaluation = command_a_evaluation
        else:
            command_a_evaluation = ""
    else:
        command_a_response = ""
        command_a_evaluation = ""
        command_a_referenced_contexts = ""



    if "meta/llama-4-maverick-17b-128e-instruct-fp8" in llm_answer_checkbox_group:
        llama_4_maverick_response = llama_4_maverick_response
        llama_4_maverick_referenced_contexts = ""
        if include_citation:
            llama_4_maverick_response, llama_4_maverick_referenced_contexts = extract_citation(
                llama_4_maverick_response)
        if llm_evaluation_checkbox:
            llama_4_maverick_evaluation = llama_4_maverick_evaluation
        else:
            llama_4_maverick_evaluation = ""
    else:
        llama_4_maverick_response = ""
        llama_4_maverick_evaluation = ""
        llama_4_maverick_referenced_contexts = ""

    if "meta/llama-4-scout-17b-16e-instruct" in llm_answer_checkbox_group:
        llama_4_scout_response = llama_4_scout_response
        llama_4_scout_referenced_contexts = ""
        if include_citation:
            llama_4_scout_response, llama_4_scout_referenced_contexts = extract_citation(llama_4_scout_response)
        if llm_evaluation_checkbox:
            llama_4_scout_evaluation = llama_4_scout_evaluation
        else:
            llama_4_scout_evaluation = ""
    else:
        llama_4_scout_response = ""
        llama_4_scout_evaluation = ""
        llama_4_scout_referenced_contexts = ""

    if "meta/llama-3-3-70b" in llm_answer_checkbox_group:
        llama_3_3_70b_response = llama_3_3_70b_response
        llama_3_3_70b_referenced_contexts = ""
        if include_citation:
            llama_3_3_70b_response, llama_3_3_70b_referenced_contexts = extract_citation(llama_3_3_70b_response)
        if llm_evaluation_checkbox:
            llama_3_3_70b_evaluation = llama_3_3_70b_evaluation
        else:
            llama_3_3_70b_evaluation = ""
    else:
        llama_3_3_70b_response = ""
        llama_3_3_70b_evaluation = ""
        llama_3_3_70b_referenced_contexts = ""

    if "meta/llama-3-2-90b-vision" in llm_answer_checkbox_group:
        llama_3_2_90b_vision_response = llama_3_2_90b_vision_response
        llama_3_2_90b_vision_referenced_contexts = ""
        if include_citation:
            llama_3_2_90b_vision_response, llama_3_2_90b_vision_referenced_contexts = extract_citation(
                llama_3_2_90b_vision_response)
        if llm_evaluation_checkbox:
            llama_3_2_90b_vision_evaluation = llama_3_2_90b_vision_evaluation
        else:
            llama_3_2_90b_vision_evaluation = ""
    else:
        llama_3_2_90b_vision_response = ""
        llama_3_2_90b_vision_evaluation = ""
        llama_3_2_90b_vision_referenced_contexts = ""

    if "openai/gpt-4o" in llm_answer_checkbox_group:
        openai_gpt4o_response = openai_gpt4o_response
        openai_gpt4o_referenced_contexts = ""
        if include_citation:
            openai_gpt4o_response, openai_gpt4o_referenced_contexts = extract_citation(openai_gpt4o_response)
        if llm_evaluation_checkbox:
            openai_gpt4o_evaluation = openai_gpt4o_evaluation
        else:
            openai_gpt4o_evaluation = ""
    else:
        openai_gpt4o_response = ""
        openai_gpt4o_evaluation = ""
        openai_gpt4o_referenced_contexts = ""

    if "openai/gpt-4" in llm_answer_checkbox_group:
        openai_gpt4_response = openai_gpt4_response
        openai_gpt4_referenced_contexts = ""
        if include_citation:
            openai_gpt4_response, openai_gpt4_referenced_contexts = extract_citation(openai_gpt4_response)
        if llm_evaluation_checkbox:
            openai_gpt4_evaluation = openai_gpt4_evaluation
        else:
            openai_gpt4_evaluation = ""
    else:
        openai_gpt4_response = ""
        openai_gpt4_evaluation = ""
        openai_gpt4_referenced_contexts = ""

    if "azure_openai/gpt-4o" in llm_answer_checkbox_group:
        azure_openai_gpt4o_response = azure_openai_gpt4o_response
        azure_openai_gpt4o_referenced_contexts = ""
        if include_citation:
            azure_openai_gpt4o_response, azure_openai_gpt4o_referenced_contexts = extract_citation(
                azure_openai_gpt4o_response)
        if llm_evaluation_checkbox:
            azure_openai_gpt4o_evaluation = azure_openai_gpt4o_evaluation
        else:
            azure_openai_gpt4o_evaluation = ""
    else:
        azure_openai_gpt4o_response = ""
        azure_openai_gpt4o_evaluation = ""
        azure_openai_gpt4o_referenced_contexts = ""

    if "azure_openai/gpt-4" in llm_answer_checkbox_group:
        azure_openai_gpt4_response = azure_openai_gpt4_response
        azure_openai_gpt4_referenced_contexts = ""
        if include_citation:
            azure_openai_gpt4_response, azure_openai_gpt4_referenced_contexts = extract_citation(
                azure_openai_gpt4_response)
        if llm_evaluation_checkbox:
            azure_openai_gpt4_evaluation = azure_openai_gpt4_evaluation
        else:
            azure_openai_gpt4_evaluation = ""
    else:
        azure_openai_gpt4_response = ""
        azure_openai_gpt4_evaluation = ""
        azure_openai_gpt4_referenced_contexts = ""

    df3 = pd.DataFrame(
        {
            'LLM ãƒ¢ãƒ‡ãƒ«':
                [
                    "xai/grok-3",
                    "cohere/command-a",
                    "meta/llama-4-maverick-17b-128e-instruct-fp8",
                    "meta/llama-4-scout-17b-16e-instruct",
                    "meta/llama-3-3-70b",
                    "meta/llama-3-2-90b-vision",
                    "openai/gpt-4o",
                    "openai/gpt-4",
                    "azure_openai/gpt-4o",
                    "azure_openai/gpt-4"
                ],
            'LLM ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸': [
                xai_grok_3_response,
                command_a_response,
                llama_4_maverick_response,
                llama_4_scout_response,
                llama_3_3_70b_response,
                llama_3_2_90b_vision_response,
                openai_gpt4o_response,
                openai_gpt4_response,
                azure_openai_gpt4o_response,
                azure_openai_gpt4_response
            ],
            'å¼•ç”¨ Contexts': [
                xai_grok_3_referenced_contexts,
                command_a_referenced_contexts,
                llama_4_maverick_referenced_contexts,
                llama_4_scout_referenced_contexts,
                llama_3_3_70b_referenced_contexts,
                llama_3_2_90b_vision_referenced_contexts,
                openai_gpt4o_referenced_contexts,
                openai_gpt4_referenced_contexts,
                azure_openai_gpt4o_referenced_contexts,
                azure_openai_gpt4_referenced_contexts
            ],
            'LLM è©•ä¾¡çµæœ': [
                xai_grok_3_evaluation,
                command_a_evaluation,
                llama_4_maverick_evaluation,
                llama_4_scout_evaluation,
                llama_3_3_70b_evaluation,
                llama_3_2_90b_vision_evaluation,
                openai_gpt4o_evaluation,
                openai_gpt4_evaluation,
                azure_openai_gpt4o_evaluation,
                azure_openai_gpt4_evaluation
            ]
        }
    )

    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å®šç¾©
    filepath = '/tmp/query_result.xlsx'

    # ExcelWriterã‚’ä½¿ç”¨ã—ã¦è¤‡æ•°ã®DataFrameã‚’ç•°ãªã‚‹ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã¿
    with pd.ExcelWriter(filepath) as writer:
        df1.to_excel(writer, sheet_name='Sheet1', index=False)
        df2.to_excel(writer, sheet_name='Sheet2', index=False)
        df3.to_excel(writer, sheet_name='Sheet3', index=False)

    print(f"Excelãƒ•ã‚¡ã‚¤ãƒ«ãŒ {filepath} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
    return gr.DownloadButton(value=filepath, visible=True)


def generate_eval_result_file():
    print("in generate_eval_result_file() start...")

    with pool.acquire() as conn:
        with conn.cursor() as cursor:
            select_sql = """
                         SELECT r.query_id,
                                r.query,
                                r.standard_answer,
                                r.sql,
                                f.llm_name,
                                f.llm_answer,
                                f.ragas_evaluation_result,
                                f.human_evaluation_result,
                                f.user_comment,
                                TO_CHAR(r.created_date, 'YYYY-MM-DD HH24:MI:SS') AS created_date
                         FROM RAG_QA_RESULT r
                                  JOIN
                              RAG_QA_FEEDBACK f
                              ON
                                  r.query_id = f.query_id \
                         """

            cursor.execute(select_sql)

            # åˆ—åã‚’å–å¾—
            columns = [col[0] for col in cursor.description]

            # ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            data = cursor.fetchall()

            print(f"{columns=}")

            # ãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«å¤‰æ›
            result_df = pd.DataFrame(data, columns=columns)

            print(f"{result_df=}")

            # åˆ—åã‚’æ—¥æ–‡ã«å¤‰æ›´
            result_df.rename(columns={
                'QUERY_ID': 'ã‚¯ã‚¨ãƒªID',
                'QUERY': 'ã‚¯ã‚¨ãƒª',
                'STANDARD_ANSWER': 'æ¨™æº–å›ç­”',
                'SQL': 'ä½¿ç”¨ã•ã‚ŒãŸSQL',
                'LLM_NAME': 'LLM ãƒ¢ãƒ‡ãƒ«',
                'LLM_ANSWER': 'LLM ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸',
                'RAGAS_EVALUATION_RESULT': 'LLM è©•ä¾¡çµæœ',
                'HUMAN_EVALUATION_RESULT': 'Human è©•ä¾¡çµæœ',
                'USER_COMMENT': 'Human ã‚³ãƒ¡ãƒ³ãƒˆ',
                'CREATED_DATE': 'ä½œæˆæ—¥æ™‚'
            }, inplace=True)

            print(f"{result_df=}")

            # å¿…è¦ã«å¿œã˜ã¦created_dateåˆ—ã‚’datetimeå‹ã«å¤‰æ›
            result_df['ä½œæˆæ—¥æ™‚'] = pd.to_datetime(result_df['ä½œæˆæ—¥æ™‚'], format='%Y-%m-%d %H:%M:%S')

            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å®šç¾©
            filepath = '/tmp/evaluation_result.xlsx'

            # ExcelWriterã‚’ä½¿ç”¨ã—ã¦è¤‡æ•°ã®DataFrameã‚’ç•°ãªã‚‹ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã¿
            with pd.ExcelWriter(filepath) as writer:
                result_df.to_excel(writer, sheet_name='Sheet1', index=False)

            print(f"Excelãƒ•ã‚¡ã‚¤ãƒ«ãŒ {filepath} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
            gr.Info("è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã®ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸ")
            return gr.DownloadButton(value=filepath, visible=True)


def set_query_id_state():
    print("in set_query_id_state() start...")
    return generate_unique_id("query_")


def insert_query_result(
        search_result,
        query_id,
        query,
        doc_id_all_checkbox_input,
        doc_id_checkbox_group_input,
        sql,
        llm_answer_checkbox_group,
        llm_evaluation_checkbox,
        standard_answer_text,
        xai_grok_3_response,
        command_a_response,

        llama_4_maverick_response,
        llama_4_scout_response,
        llama_3_3_70b_response,
        llama_3_2_90b_vision_response,
        openai_gpt4o_response,
        openai_gpt4_response,
        azure_openai_gpt4o_response,
        azure_openai_gpt4_response,
        xai_grok_3_evaluation,
        command_a_evaluation,

        llama_4_maverick_evaluation,
        llama_4_scout_evaluation,
        llama_3_3_70b_evaluation,
        llama_3_2_90b_vision_evaluation,
        openai_gpt4o_evaluation,
        openai_gpt4_evaluation,
        azure_openai_gpt4o_evaluation,
        azure_openai_gpt4_evaluation
):
    print("in insert_query_result() start...")
    if not query:
        return
    if not doc_id_all_checkbox_input and (not doc_id_checkbox_group_input or doc_id_checkbox_group_input == [""]):
        return
    if search_result.empty or (len(search_result) > 0 and search_result.iloc[0]['CONTENT'] == ''):
        return
    with pool.acquire() as conn:
        with conn.cursor() as cursor:
            # ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒå­˜åœ¨ã—ãªã„å ´åˆã€æŒ¿å…¥æ“ä½œã‚’å®Ÿè¡Œ
            insert_sql = """
                         INSERT INTO RAG_QA_RESULT (query_id,
                                                    query,
                                                    standard_answer,
                                                    sql)
                         VALUES (:1,
                                 :2,
                                 :3,
                                 :4) \
                         """
            cursor.setinputsizes(None, None, None, oracledb.CLOB)
            cursor.execute(
                insert_sql,
                [
                    query_id,
                    query,
                    standard_answer_text,
                    sql
                ]
            )

            if "xai/grok-3" in llm_answer_checkbox_group:
                xai_grok_3_response = xai_grok_3_response
                if llm_evaluation_checkbox:
                    xai_grok_3_evaluation = xai_grok_3_evaluation
                else:
                    xai_grok_3_evaluation = ""

                insert_sql = """
                             INSERT INTO RAG_QA_FEEDBACK (query_id,
                                                          llm_name,
                                                          llm_answer,
                                                          ragas_evaluation_result)
                             VALUES (:1,
                                     :2,
                                     :3,
                                     :4) \
                             """
                cursor.setinputsizes(None, None, oracledb.CLOB, oracledb.CLOB)
                cursor.execute(
                    insert_sql,
                    [
                        query_id,
                        "xai/grok-3",
                        xai_grok_3_response,
                        xai_grok_3_evaluation
                    ]
                )

            if "cohere/command-a" in llm_answer_checkbox_group:
                command_a_response = command_a_response
                if llm_evaluation_checkbox:
                    command_a_evaluation = command_a_evaluation
                else:
                    command_a_evaluation = ""

                insert_sql = """
                             INSERT INTO RAG_QA_FEEDBACK (query_id,
                                                          llm_name,
                                                          llm_answer,
                                                          ragas_evaluation_result)
                             VALUES (:1,
                                     :2,
                                     :3,
                                     :4) \
                             """
                cursor.setinputsizes(None, None, oracledb.CLOB, oracledb.CLOB)
                cursor.execute(
                    insert_sql,
                    [
                        query_id,
                        "cohere/command-a",
                        command_a_response,
                        command_a_evaluation
                    ]
                )


            if "meta/llama-4-maverick-17b-128e-instruct-fp8" in llm_answer_checkbox_group:
                llama_4_maverick_response = llama_4_maverick_response
                if llm_evaluation_checkbox:
                    llama_4_maverick_evaluation = llama_4_maverick_evaluation
                else:
                    llama_4_maverick_evaluation = ""

                insert_sql = """
                             INSERT INTO RAG_QA_FEEDBACK (query_id,
                                                          llm_name,
                                                          llm_answer,
                                                          ragas_evaluation_result)
                             VALUES (:1,
                                     :2,
                                     :3,
                                     :4) \
                             """
                cursor.setinputsizes(None, None, oracledb.CLOB, oracledb.CLOB)
                cursor.execute(
                    insert_sql,
                    [
                        query_id,
                        "meta/llama-4-maverick-17b-128e-instruct-fp8",
                        llama_4_maverick_response,
                        llama_4_maverick_evaluation
                    ]
                )

            if "meta/llama-4-scout-17b-16e-instruct" in llm_answer_checkbox_group:
                llama_4_scout_response = llama_4_scout_response
                if llm_evaluation_checkbox:
                    llama_4_scout_evaluation = llama_4_scout_evaluation
                else:
                    llama_4_scout_evaluation = ""

                insert_sql = """
                             INSERT INTO RAG_QA_FEEDBACK (query_id,
                                                          llm_name,
                                                          llm_answer,
                                                          ragas_evaluation_result)
                             VALUES (:1,
                                     :2,
                                     :3,
                                     :4) \
                             """
                cursor.setinputsizes(None, None, oracledb.CLOB, oracledb.CLOB)
                cursor.execute(
                    insert_sql,
                    [
                        query_id,
                        "meta/llama-4-scout-17b-16e-instruct",
                        llama_4_scout_response,
                        llama_4_scout_evaluation
                    ]
                )

            if "meta/llama-3-3-70b" in llm_answer_checkbox_group:
                llama_3_3_70b_response = llama_3_3_70b_response
                if llm_evaluation_checkbox:
                    llama_3_3_70b_evaluation = llama_3_3_70b_evaluation
                else:
                    llama_3_3_70b_evaluation = ""

                insert_sql = """
                             INSERT INTO RAG_QA_FEEDBACK (query_id,
                                                          llm_name,
                                                          llm_answer,
                                                          ragas_evaluation_result)
                             VALUES (:1,
                                     :2,
                                     :3,
                                     :4) \
                             """
                cursor.setinputsizes(None, None, oracledb.CLOB, oracledb.CLOB)
                cursor.execute(
                    insert_sql,
                    [
                        query_id,
                        "meta/llama-3-3-70b",
                        llama_3_3_70b_response,
                        llama_3_3_70b_evaluation
                    ]
                )

            if "meta/llama-3-2-90b-vision" in llm_answer_checkbox_group:
                llama_3_2_90b_vision_response = llama_3_2_90b_vision_response
                if llm_evaluation_checkbox:
                    llama_3_2_90b_vision_evaluation = llama_3_2_90b_vision_evaluation
                else:
                    llama_3_2_90b_vision_evaluation = ""

                insert_sql = """
                             INSERT INTO RAG_QA_FEEDBACK (query_id,
                                                          llm_name,
                                                          llm_answer,
                                                          ragas_evaluation_result)
                             VALUES (:1,
                                     :2,
                                     :3,
                                     :4) \
                             """
                cursor.setinputsizes(None, None, oracledb.CLOB, oracledb.CLOB)
                cursor.execute(
                    insert_sql,
                    [
                        query_id,
                        "meta/llama-3-2-90b-vision",
                        llama_3_2_90b_vision_response,
                        llama_3_2_90b_vision_evaluation
                    ]
                )

            if "openai/gpt-4o" in llm_answer_checkbox_group:
                openai_gpt4o_response = openai_gpt4o_response
                if llm_evaluation_checkbox:
                    openai_gpt4o_evaluation = openai_gpt4o_evaluation
                else:
                    openai_gpt4o_evaluation = ""

                insert_sql = """
                             INSERT INTO RAG_QA_FEEDBACK (query_id,
                                                          llm_name,
                                                          llm_answer,
                                                          ragas_evaluation_result)
                             VALUES (:1,
                                     :2,
                                     :3,
                                     :4) \
                             """
                cursor.setinputsizes(None, None, oracledb.CLOB, oracledb.CLOB)
                cursor.execute(
                    insert_sql,
                    [
                        query_id,
                        "openai/gpt-4o",
                        openai_gpt4o_response,
                        openai_gpt4o_evaluation
                    ]
                )

            if "openai/gpt-4" in llm_answer_checkbox_group:
                openai_gpt4_response = openai_gpt4_response
                if llm_evaluation_checkbox:
                    openai_gpt4_evaluation = openai_gpt4_evaluation
                else:
                    openai_gpt4_evaluation = ""

                insert_sql = """
                             INSERT INTO RAG_QA_FEEDBACK (query_id,
                                                          llm_name,
                                                          llm_answer,
                                                          ragas_evaluation_result)
                             VALUES (:1,
                                     :2,
                                     :3,
                                     :4) \
                             """
                cursor.setinputsizes(None, None, oracledb.CLOB, oracledb.CLOB)
                cursor.execute(
                    insert_sql,
                    [
                        query_id,
                        "openai/gpt-4",
                        openai_gpt4_response,
                        openai_gpt4_evaluation
                    ]
                )

            if "azure_openai/gpt-4o" in llm_answer_checkbox_group:
                azure_openai_gpt4o_response = azure_openai_gpt4o_response
                if llm_evaluation_checkbox:
                    azure_openai_gpt4o_evaluation = azure_openai_gpt4o_evaluation
                else:
                    azure_openai_gpt4o_evaluation = ""

                insert_sql = """
                             INSERT INTO RAG_QA_FEEDBACK (query_id,
                                                          llm_name,
                                                          llm_answer,
                                                          ragas_evaluation_result)
                             VALUES (:1,
                                     :2,
                                     :3,
                                     :4) \
                             """
                cursor.setinputsizes(None, None, oracledb.CLOB, oracledb.CLOB)
                cursor.execute(
                    insert_sql,
                    [
                        query_id,
                        "azure_openai/gpt-4o",
                        azure_openai_gpt4o_response,
                        azure_openai_gpt4o_evaluation
                    ]
                )

            if "azure_openai/gpt-4" in llm_answer_checkbox_group:
                azure_openai_gpt4_response = azure_openai_gpt4_response
                if llm_evaluation_checkbox:
                    azure_openai_gpt4_evaluation = azure_openai_gpt4_evaluation
                else:
                    azure_openai_gpt4_evaluation = ""

                insert_sql = """
                             INSERT INTO RAG_QA_FEEDBACK (query_id,
                                                          llm_name,
                                                          llm_answer,
                                                          ragas_evaluation_result)
                             VALUES (:1,
                                     :2,
                                     :3,
                                     :4) \
                             """
                cursor.setinputsizes(None, None, oracledb.CLOB, oracledb.CLOB)
                cursor.execute(
                    insert_sql,
                    [
                        query_id,
                        "azure_openai/gpt-4",
                        azure_openai_gpt4_response,
                        azure_openai_gpt4_evaluation
                    ]
                )

        conn.commit()


def delete_document(server_directory, doc_ids):
    has_error = False
    if not server_directory:
        has_error = True
        gr.Warning("ã‚µãƒ¼ãƒãƒ¼ãƒ»ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    print(f"{doc_ids=}")
    if not doc_ids or len(doc_ids) == 0 or (len(doc_ids) == 1 and doc_ids[0] == ''):
        has_error = True
        gr.Warning("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’é¸æŠã—ã¦ãã ã•ã„")
    if has_error:
        return (
            gr.Textbox(value=""),
            gr.Radio(),
            gr.CheckboxGroup()
        )

    output_sql = ""
    with pool.acquire() as conn, conn.cursor() as cursor:
        for doc_id in filter(bool, doc_ids):
            server_path = get_server_path(doc_id)
            if os.path.exists(server_path):
                os.remove(server_path)
                print(f"File {doc_id} deleted successfully")
            else:
                print(f"File {doc_id} not found")

            delete_embedding_sql = f"""
DELETE FROM {DEFAULT_COLLECTION_NAME}_embedding
WHERE doc_id = :doc_id """
            delete_collection_sql = f"""
DELETE FROM {DEFAULT_COLLECTION_NAME}_collection
WHERE id = :doc_id """
            delete_image_sql = f"""
DELETE FROM {DEFAULT_COLLECTION_NAME}_image
WHERE doc_id = :doc_id """
            delete_image_embedding_sql = f"""
DELETE FROM {DEFAULT_COLLECTION_NAME}_image_embedding
WHERE doc_id = :doc_id """

            output_sql += delete_embedding_sql.strip().replace(":doc_id", "'" + doc_id + "'") + ";\n"
            output_sql += delete_collection_sql.strip().replace(":doc_id", "'" + doc_id + "'") + ";\n"
            output_sql += delete_image_sql.strip().replace(":doc_id", "'" + doc_id + "'") + ";\n"
            output_sql += delete_image_embedding_sql.strip().replace(":doc_id", "'" + doc_id + "'") + ";\n"

            cursor.execute(delete_embedding_sql, doc_id=doc_id)
            cursor.execute(delete_collection_sql, doc_id=doc_id)
            cursor.execute(delete_image_sql, doc_id=doc_id)
            cursor.execute(delete_image_embedding_sql, doc_id=doc_id)

            conn.commit()

    doc_list = get_doc_list()
    return (
        gr.Textbox(value=output_sql),
        gr.Radio(doc_list),
        gr.CheckboxGroup(choices=doc_list, value=[]),
        gr.CheckboxGroup(choices=doc_list)
    )


theme = gr.themes.Default(
    spacing_size="sm",
    font=[GoogleFont(name="Noto Sans JP"), GoogleFont(name="Noto Sans SC"), GoogleFont(name="Roboto")]
).set(
)

with gr.Blocks(css=custom_css, theme=theme) as app:
    gr.Markdown(value="# RAGç²¾åº¦ã‚ã’ãŸã‚ã†", elem_classes="main_Header")
    gr.Markdown(value="### LLMï¼†RAGç²¾åº¦è©•ä¾¡ãƒ„ãƒ¼ãƒ«",
                elem_classes="sub_Header")

    query_id_state = gr.State()

    with gr.Tabs() as tabs:
        with gr.TabItem(label="ç’°å¢ƒè¨­å®š") as tab_setting:
            with gr.TabItem(label="OCI GenAIã®è¨­å®š*") as tab_create_oci_cred:
                with gr.Accordion(label="ä½¿ç”¨ã•ã‚ŒãŸSQL", open=False) as tab_create_oci_cred_sql_accordion:
                    tab_create_oci_cred_sql_text = gr.Textbox(
                        label="SQL",
                        show_label=False,
                        lines=25,
                        max_lines=50,
                        autoscroll=False,
                        interactive=False,
                        show_copy_button=True
                    )
                with gr.Row():
                    with gr.Column():
                        tab_create_oci_cred_user_ocid_text = gr.Textbox(
                            label="User OCID*",
                            lines=1,
                            interactive=True
                        )
                with gr.Row():
                    with gr.Column():
                        tab_create_oci_cred_tenancy_ocid_text = gr.Textbox(
                            label="Tenancy OCID*",
                            lines=1,
                            interactive=True
                        )
                with gr.Row():
                    with gr.Column():
                        tab_create_oci_cred_fingerprint_text = gr.Textbox(
                            label="Fingerprint*",
                            lines=1,
                            interactive=True
                        )
                with gr.Row():
                    with gr.Column():
                        tab_create_oci_cred_private_key_file = gr.File(
                            label="Private Key*",
                            file_types=[".pem"],
                            type="filepath",
                            interactive=True
                        )
                with gr.Row():
                    with gr.Column():
                        tab_create_oci_cred_region_text = gr.Dropdown(
                            choices=["ap-osaka-1", "us-chicago-1"],
                            label="Region*",
                            interactive=True,
                            value="ap-osaka-1",
                        )
                with gr.Row():
                    with gr.Column():
                        tab_create_oci_clear_button = gr.ClearButton(value="ã‚¯ãƒªã‚¢")
                    with gr.Column():
                        tab_create_oci_cred_button = gr.Button(value="è¨­å®š/å†è¨­å®š", variant="primary")
                with gr.Accordion(label="OCI GenAIã®ãƒ†ã‚¹ãƒˆ", open=False) as tab_create_oci_cred_test_accordion:
                    with gr.Row():
                        with gr.Column():
                            tab_create_oci_cred_test_query_text = gr.Textbox(
                                label="ãƒ†ã‚­ã‚¹ãƒˆ",
                                lines=1,
                                max_lines=1,
                                value="ã“ã‚“ã«ã¡ã‚"
                            )
                    with gr.Row():
                        with gr.Column():
                            tab_create_oci_cred_test_vector_text = gr.Textbox(
                                label="ãƒ™ã‚¯ãƒˆãƒ«",
                                lines=10,
                                max_lines=10,
                                autoscroll=False
                            )
                    with gr.Row():
                        with gr.Column():
                            tab_create_oci_cred_test_button = gr.Button(value="ãƒ†ã‚¹ãƒˆ", variant="primary")
            with gr.TabItem(label="ãƒ†ãƒ¼ãƒ–ãƒ«ã®ä½œæˆ*") as tab_create_table:
                with gr.Accordion(label="ä½¿ç”¨ã•ã‚ŒãŸSQL", open=False) as tab_create_table_sql_accordion:
                    tab_create_table_sql_text = gr.Textbox(
                        label="SQL",
                        show_label=False,
                        lines=25,
                        max_lines=50,
                        autoscroll=False,
                        interactive=False,
                        show_copy_button=True
                    )
                with gr.Row():
                    with gr.Column():
                        tab_create_table_button = gr.Button(value="ä½œæˆ/å†ä½œæˆ", variant="primary")
            with gr.TabItem(label="Cohereã®è¨­å®š(ã‚ªãƒ—ã‚·ãƒ§ãƒ³)") as tab_create_cohere_cred:
                with gr.Row():
                    with gr.Column():
                        tab_create_cohere_cred_api_key_text = gr.Textbox(
                            label="API Key*",
                            type="password",
                            lines=1,
                            interactive=True
                        )
                with gr.Row():
                    with gr.Column():
                        tab_create_cohere_cred_button = gr.Button(value="è¨­å®š/å†è¨­å®š", variant="primary")
            with gr.TabItem(label="OpenAIã®è¨­å®š(ã‚ªãƒ—ã‚·ãƒ§ãƒ³)") as tab_create_openai_cred:
                with gr.Row():
                    with gr.Column():
                        tab_create_openai_cred_base_url_text = gr.Textbox(
                            label="Base URL*", lines=1, interactive=True
                        )
                with gr.Row():
                    with gr.Column():
                        tab_create_openai_cred_api_key_text = gr.Textbox(
                            label="API Key*",
                            type="password",
                            lines=1,
                            interactive=True
                        )
                with gr.Row():
                    with gr.Column():
                        tab_create_openai_cred_button = gr.Button(value="è¨­å®š/å†è¨­å®š", variant="primary")
            with gr.TabItem(label="Azure OpenAIã®è¨­å®š(ã‚ªãƒ—ã‚·ãƒ§ãƒ³)") as tab_create_azure_openai_cred:
                with gr.Row():
                    with gr.Column():
                        tab_create_azure_openai_cred_api_key_text = gr.Textbox(
                            label="API Key*",
                            type="password",
                            lines=1,
                            interactive=True
                        )
                with gr.Row():
                    with gr.Column():
                        tab_create_azure_openai_cred_endpoint_gpt_4o_text = gr.Textbox(
                            label="GPT-4O Endpoint*",
                            lines=1,
                            interactive=True
                        )
                with gr.Row():
                    with gr.Column():
                        tab_create_azure_openai_cred_endpoint_gpt_4_text = gr.Textbox(
                            label="GPT-4 Endpoint(ã‚ªãƒ—ã‚·ãƒ§ãƒ³)",
                            lines=1,
                            interactive=True
                        )
                with gr.Row():
                    with gr.Column():
                        tab_create_azure_openai_cred_button = gr.Button(value="è¨­å®š/å†è¨­å®š", variant="primary")

            with gr.TabItem(label="Langfuseã®è¨­å®š(ã‚ªãƒ—ã‚·ãƒ§ãƒ³)") as tab_create_langfuse_cred:
                with gr.Row():
                    with gr.Column():
                        tab_create_langfuse_cred_secret_key_text = gr.Textbox(
                            label="LANGFUSE_SECRET_KEY*",
                            lines=1,
                            interactive=True,
                            placeholder="sk-lf-..."
                        )
                with gr.Row():
                    with gr.Column():
                        tab_create_langfuse_cred_public_key_text = gr.Textbox(
                            label="LANGFUSE_PUBLIC_KEY*",
                            lines=1,
                            interactive=True,
                            placeholder="pk-lf-..."
                        )
                with gr.Row():
                    with gr.Column():
                        tab_create_langfuse_cred_host_text = gr.Textbox(
                            label="LANGFUSE_HOST*", lines=1,
                            interactive=True,
                            placeholder="http://xxx.xxx.xxx.xxx:3000"
                        )
                with gr.Row():
                    with gr.Column():
                        tab_create_langfuse_cred_button = gr.Button(value="è¨­å®š/å†è¨­å®š", variant="primary")
        with gr.TabItem(label="LLMè©•ä¾¡") as tab_llm_evaluation:
            with gr.TabItem(label="LLMã¨ãƒãƒ£ãƒƒãƒˆ") as tab_chat_with_llm:
                with gr.Row():
                    with gr.Column():
                        tab_chat_with_llm_answer_checkbox_group = gr.CheckboxGroup(
                            [
                                "xai/grok-3",
                                "cohere/command-a",
                                "meta/llama-4-maverick-17b-128e-instruct-fp8",
                                "meta/llama-4-scout-17b-16e-instruct",
                                "meta/llama-3-3-70b",
                                "meta/llama-3-2-90b-vision",
                                "openai/gpt-4o",
                                "openai/gpt-4",
                                "azure_openai/gpt-4o",
                                "azure_openai/gpt-4"],
                            label="LLM ãƒ¢ãƒ‡ãƒ«*",
                            value=[]
                        )
                with gr.Accordion(
                        label="XAI Grok-3 ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
                        visible=False,
                        open=True
                ) as tab_chat_with_llm_xai_grok_3_accordion:
                    tab_chat_with_xai_grok_3_answer_text = gr.Markdown(
                        show_copy_button=True,
                        height=200,
                        min_height=200,
                        max_height=300
                    )
                with gr.Accordion(
                        label="Command-A ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
                        visible=False,
                        open=True
                ) as tab_chat_with_llm_command_a_accordion:
                    tab_chat_with_command_a_answer_text = gr.Markdown(
                        show_copy_button=True,
                        height=200,
                        min_height=200,
                        max_height=300
                    )


                with gr.Accordion(
                        label="Llama 4 Maverick 17b ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
                        visible=False,
                        open=True
                ) as tab_chat_with_llm_llama_4_maverick_accordion:
                    tab_chat_with_llama_4_maverick_answer_text = gr.Markdown(
                        show_copy_button=True,
                        height=200,
                        min_height=200,
                        max_height=300
                    )
                with gr.Accordion(
                        label="Llama 4 Scout 17b ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
                        visible=False,
                        open=True
                ) as tab_chat_with_llm_llama_4_scout_accordion:
                    tab_chat_with_llama_4_scout_answer_text = gr.Markdown(
                        show_copy_button=True,
                        height=200,
                        min_height=200,
                        max_height=300
                    )
                with gr.Accordion(
                        label="Llama 3.3 70b ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
                        visible=False,
                        open=True
                ) as tab_chat_with_llm_llama_3_3_70b_accordion:
                    tab_chat_with_llama_3_3_70b_answer_text = gr.Markdown(
                        show_copy_button=True,
                        height=200,
                        min_height=200,
                        max_height=300
                    )
                with gr.Accordion(
                        label="Llama 3.2 90b Vision ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
                        visible=False,
                        open=True
                ) as tab_chat_with_llm_llama_3_2_90b_vision_accordion:
                    tab_chat_with_llama_3_2_90b_vision_answer_text = gr.Markdown(
                        show_copy_button=True,
                        height=200,
                        min_height=200,
                        max_height=300
                    )
                with gr.Accordion(
                        label="OpenAI gpt-4o ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
                        visible=False,
                        open=True
                ) as tab_chat_with_llm_openai_gpt4o_accordion:
                    tab_chat_with_openai_gpt4o_answer_text = gr.Markdown(
                        show_copy_button=True,
                        height=200,
                        min_height=200,
                        max_height=300
                    )
                with gr.Accordion(
                        label="OpenAI gpt-4 ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
                        visible=False,
                        open=True
                ) as tab_chat_with_llm_openai_gpt4_accordion:
                    tab_chat_with_openai_gpt4_answer_text = gr.Markdown(
                        show_copy_button=True,
                        height=200,
                        min_height=200,
                        max_height=300
                    )
                with gr.Accordion(
                        label="Azure OpenAI gpt-4o ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
                        visible=False,
                        open=True
                ) as tab_chat_with_llm_azure_openai_gpt4o_accordion:
                    tab_chat_with_azure_openai_gpt4o_answer_text = gr.Markdown(
                        show_copy_button=True,
                        height=200,
                        min_height=200,
                        max_height=300
                    )
                with gr.Accordion(
                        label="Azure OpenAI gpt-4 ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
                        visible=False,
                        open=True
                ) as tab_chat_with_llm_azure_openai_gpt4_accordion:
                    tab_chat_with_azure_openai_gpt4_answer_text = gr.Markdown(
                        show_copy_button=True,
                        height=200,
                        min_height=200,
                        max_height=300
                    )

                with gr.Accordion(open=False, label="ã‚·ã‚¹ãƒ†ãƒ ãƒ»ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸"):
                    #                     tab_chat_with_llm_system_text = gr.Textbox(label="ã‚·ã‚¹ãƒ†ãƒ ãƒ»ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸*", show_label=False, lines=5,
                    #                                                                max_lines=15,
                    #                                                                value="You are a helpful assistant. \n\
                    # Please respond to me in the same language I use for my messages. \n\
                    # If I switch languages, please switch your responses accordingly.")
                    tab_chat_with_llm_system_text = gr.Textbox(
                        label="ã‚·ã‚¹ãƒ†ãƒ ãƒ»ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸*",
                        show_label=False,
                        lines=5,
                        max_lines=15,
                        value=get_chat_system_message()
                    )
                with gr.Accordion(open=False,
                                  label="ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«(ã‚ªãƒ—ã‚·ãƒ§ãƒ³) - Llama-4-Maverickã€Llama-4-Scoutã€Llama-3.2-90B-Visionãƒ¢ãƒ‡ãƒ«ã‚’åˆ©ç”¨ã™ã‚‹å ´åˆã«é™ã‚Šã€ã“ã®ç”»åƒå…¥åŠ›ãŒé©ç”¨ã•ã‚Œã¾ã™ã€‚"):
                    tab_chat_with_llm_query_image = gr.Image(
                        label="",
                        interactive=True,
                        type="filepath",
                        height=300,
                        show_label=False,
                    )
                with gr.Row():
                    with gr.Column():
                        tab_chat_with_llm_query_text = gr.Textbox(
                            label="ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ»ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸*",
                            lines=2,
                            max_lines=5
                        )
                with gr.Row():
                    with gr.Column():
                        tab_chat_with_llm_clear_button = gr.ClearButton(value="ã‚¯ãƒªã‚¢")
                    with gr.Column():
                        tab_chat_with_llm_chat_button = gr.Button(value="é€ä¿¡", variant="primary")
        with gr.TabItem(label="RAGè©•ä¾¡", elem_classes="inner_tab") as tab_rag_evaluation:
            with gr.TabItem(label="Step-0.å‰å‡¦ç†") as tab_convert_document:
                with gr.TabItem(label="MarkItDown") as tab_convert_by_markitdown_document:
                    with gr.Row():
                        with gr.Column():
                            tab_convert_document_convert_by_markitdown_file_text = gr.File(
                                label="å¤‰æ›å‰ã®ãƒ•ã‚¡ã‚¤ãƒ«*",
                                file_types=[
                                    ".csv", ".xls", ".xlsx", ".json", ".xml", ".ppt", ".pptx", ".doc", ".docx", ".pdf",
                                    ".txt", ".png", ".jpg", ".jpeg"
                                ],
                                type="filepath",
                                interactive=True,
                            )
                    with gr.Accordion("Advanced Settings", open=False):
                        with gr.Row():
                            with gr.Column():
                                tab_convert_document_convert_by_markitdown_use_llm_checkbox = gr.Checkbox(
                                    label="LLMã«ã‚ˆã‚‹å‡¦ç†ã‚’æœ‰åŠ¹ã«ã™ã‚‹",
                                    value=True,
                                    info=(
                                        f"OCI Generative AI Visionãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã•ã‚Œã¾ã™ã€‚['.jpg','.jpeg','.png','.ppt','.pptx']ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚"
                                    ),
                                    interactive=True,
                                )
                        with gr.Row():
                            with gr.Column():
                                tab_convert_document_convert_by_markitdown_llm_prompt_text = gr.Textbox(
                                    label="LLM ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ»ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
                                    value=get_markitdown_llm_prompt(),
                                    interactive=True,
                                    lines=2,
                                    max_lines=5,
                                )
                    with gr.Row():
                        with gr.Column():
                            tab_convert_document_convert_by_markitdown_button = gr.Button(
                                value="Markdownã¸å¤‰æ›",
                                variant="primary")
                with gr.TabItem(label="Excel2Text") as tab_convert_excel_to_text_document:
                    with gr.Row():
                        with gr.Column():
                            tab_convert_document_convert_excel_to_text_file_text = gr.File(
                                label="å¤‰æ›å‰ã®ãƒ•ã‚¡ã‚¤ãƒ«*",
                                file_types=[
                                    ".csv", ".xls", ".xlsx"
                                ],
                                type="filepath",
                                interactive=True,
                            )
                    with gr.Row():
                        with gr.Column():
                            tab_convert_document_convert_button = gr.Button(
                                value="Excelã‚’Textã¸å¤‰æ›",
                                variant="primary")
            with gr.TabItem(label="Step-1.èª­è¾¼ã¿") as tab_load_document:
                with gr.Accordion(label="ä½¿ç”¨ã•ã‚ŒãŸSQL", open=False) as tab_load_document_sql_accordion:
                    tab_load_document_output_sql_text = gr.Textbox(
                        label="ä½¿ç”¨ã•ã‚ŒãŸSQL",
                        show_label=False,
                        lines=10,
                        autoscroll=False,
                        show_copy_button=True
                    )
                with gr.Row():
                    with gr.Column():
                        tab_load_document_doc_id_text = gr.Textbox(
                            label="Doc ID",
                            lines=1,
                            interactive=False
                        )
                with gr.Row(visible=False):
                    with gr.Column():
                        tab_load_document_page_count_text = gr.Textbox(
                            label="ãƒšãƒ¼ã‚¸æ•°",
                            lines=1,
                            interactive=False
                        )
                with gr.Row():
                    with gr.Column():
                        tab_load_document_page_content_text = gr.Textbox(
                            label="ã‚³ãƒ³ãƒ†ãƒ³ãƒ„",
                            lines=15,
                            max_lines=15,
                            autoscroll=False,
                            show_copy_button=True,
                            interactive=False
                        )
                with gr.Row():
                    with gr.Column():
                        tab_load_document_file_text = gr.File(
                            label="ãƒ•ã‚¡ã‚¤ãƒ«*",
                            file_types=[
                                ".txt", ".csv", ".doc", ".docx", ".epub", ".image",
                                ".md", ".msg", ".odt", ".org", ".pdf", ".ppt",
                                ".pptx",
                                ".rtf", ".rst", ".tsv", ".xls", ".xlsx"
                            ],
                            type="filepath")
                    with gr.Column():
                        tab_load_document_server_directory_text = gr.Text(
                            label="ã‚µãƒ¼ãƒãƒ¼ãƒ»ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª*",
                            value="/u01/data/no1rag/"
                        )
                with gr.Row():
                    with gr.Column():
                        tab_load_document_metadata_text = gr.Textbox(
                            label="ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿(ã‚ªãƒ—ã‚·ãƒ§ãƒ³)",
                            lines=1,
                            max_lines=1,
                            autoscroll=True,
                            show_copy_button=False,
                            interactive=True,
                            info="key1=value1,key2=value2,... ã®å½¢å¼ã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚\"'ãªã©ã®è¨˜å·ã¯ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“ã€‚",
                            placeholder="key1=value1,key2=value2,..."
                        )
                with gr.Row():
                    with gr.Column():
                        tab_load_document_load_button = gr.Button(value="èª­è¾¼ã¿", variant="primary")
            with gr.TabItem(label="Step-2.åˆ†å‰²ãƒ»ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒ»ä¿å­˜") as tab_split_document:
                # with gr.Accordion("UTL_TO_CHUNKS è¨­å®š*"):
                with gr.Accordion(
                        "CHUNKS è¨­å®š*(<FIXED_DELIMITER>ã¨ã„ã†åˆ†å‰²ç¬¦ãŒãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«å«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€ãƒãƒ£ãƒ³ã‚¯ã¯<FIXED_DELIMITER>åˆ†å‰²ã•ã‚Œã€MaxãŠã‚ˆã³Overlapã®è¨­å®šã¯ç„¡è¦–ã•ã‚Œã¾ã™ã€‚)"):
                    with gr.Row():
                        with gr.Column():
                            tab_split_document_chunks_language_radio = gr.Radio(
                                label="LANGUAGE",
                                choices=[
                                    ("JAPANESE", "JAPANESE"),
                                    ("AMERICAN", "AMERICAN")
                                ],
                                value="JAPANESE",
                                visible=False,
                                info="Default value: JAPANESEã€‚å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã®è¨€èªã‚’æŒ‡å®šã€‚ãƒ†ã‚­ã‚¹ãƒˆã«è¨€èªã«ã‚ˆã£ã¦è§£é‡ˆãŒç•°ãªã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ç‰¹å®šã®æ–‡å­—(ä¾‹ãˆã°ã€å¥èª­ç‚¹ã‚„ç•¥èª)ãŒå«ã¾ã‚Œã‚‹å ´åˆã€ã“ã®è¨€èªã®æŒ‡å®šã¯ç‰¹ã«é‡è¦ã§ã™ã€‚Oracle Database Globalization Support Guideã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹NLSã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹è¨€èªåã¾ãŸã¯ç•¥ç§°ã‚’æŒ‡å®šã§ãã‚‹ã€‚"
                            )
                        with gr.Column():
                            tab_split_document_chunks_by_radio = gr.Radio(
                                label="BY",
                                choices=[
                                    ("BY CHARACTERS (or BY CHARS )",
                                     "CHARACTERS"),
                                    ("BY WORDS", "WORDS"),
                                    ("BY VOCABULARY", "VOCABULARY")
                                ],
                                value="WORDS",
                                visible=False,
                                info="Default value: BY WORDSã€‚ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ã®æ–¹æ³•ã‚’æ–‡å­—ã€å˜èªã€èªå½™ãƒˆãƒ¼ã‚¯ãƒ³ã§æŒ‡å®šã€‚BY CHARACTERS: æ–‡å­—æ•°ã§è¨ˆç®—ã—ã¦åˆ†å‰²ã€‚BY WORDS: å˜èªæ•°ã‚’è¨ˆç®—ã—ã¦åˆ†å‰²ã€å˜èªã”ã¨ã«ç©ºç™½æ–‡å­—ãŒå…¥ã‚‹è¨€èªãŒå¯¾è±¡ã€æ—¥æœ¬èªã€ä¸­å›½èªã€ã‚¿ã‚¤èªãªã©ã®å ´åˆã€å„ãƒã‚¤ãƒ†ã‚£ãƒ–æ–‡å­—ã¯å˜èªï¼ˆãƒ¦ãƒ‹ã‚°ãƒ©ãƒ ï¼‰ã¨ã—ã¦ã¿ãªã•ã‚Œã‚‹ã€‚BY VOCABULARY: èªå½™ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—ã—ã¦åˆ†å‰²ã€CREATE_VOCABULARYãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ä½¿ã£ã¦ã€èªå½™ç™»éŒ²ãŒå¯èƒ½ã€‚",
                            )
                    with gr.Row():
                        with gr.Column(scale=1):
                            # tab_split_document_chunks_max_text = gr.Text(label="Max",
                            #                                              value="256",
                            #                                              lines=1,
                            #                                              info="",
                            #                                              )
                            tab_split_document_chunks_max_slider = gr.Slider(
                                label="Max",
                                value=320,
                                minimum=64,
                                maximum=512,
                                step=1,
                            )
                        with gr.Column(scale=1):
                            tab_split_document_chunks_overlap_slider = gr.Slider(
                                label="Overlap(Percentage of Max)",
                                minimum=0,
                                maximum=20,
                                step=5,
                                value=0,
                            )
                    with gr.Row():
                        with gr.Column():
                            tab_split_document_chunks_split_by_radio = gr.Radio(
                                label="SPLIT [BY]",
                                choices=[
                                    ("NONE", "NONE"),
                                    ("NEWLINE", "NEWLINE"),
                                    ("BLANKLINE", "BLANKLINE"),
                                    ("SPACE", "SPACE"),
                                    ("RECURSIVELY", "RECURSIVELY"),
                                    ("SENTENCE", "SENTENCE"),
                                    ("CUSTOM", "CUSTOM")
                                ],
                                value="RECURSIVELY",
                                visible=False,
                                info="Default value: RECURSIVELYã€‚ãƒ†ã‚­ã‚¹ãƒˆãŒæœ€å¤§ã‚µã‚¤ã‚ºã«é”ã—ãŸã¨ãã«ã€ã©ã†ã‚„ã£ã¦åˆ†å‰²ã™ã‚‹ã‹ã‚’æŒ‡å®šã€‚ãƒãƒ£ãƒ³ã‚¯ã®é©åˆ‡ãªå¢ƒç•Œã‚’å®šç¾©ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã™ã‚‹ã€‚NONE: MAXæŒ‡å®šã•ã‚Œã¦ã„ã‚‹æ–‡å­—æ•°ã€å˜èªæ•°ã€èªå½™ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã«é”ã—ãŸã‚‰åˆ†å‰²ã€‚NEWLINE: MAXæŒ‡å®šã‚µã‚¤ã‚ºã‚’è¶…ãˆã¦ãƒ†ã‚­ã‚¹ãƒˆã®è¡Œæœ«ã§åˆ†å‰²ã€‚BLANKLINE: æŒ‡å®šã‚µã‚¤ã‚ºã‚’è¶…ãˆã¦BLANKLINEï¼ˆ2å›ã®æ”¹è¡Œï¼‰ã®æœ«å°¾ã§åˆ†å‰²ã€‚SPACE: MAXæŒ‡å®šã‚µã‚¤ã‚ºã‚’è¶…ãˆã¦ç©ºç™½ã®è¡Œæœ«ã§åˆ†å‰²ã€‚RECURSIVELY: BLANKLINE, NEWLINE, SPACE, NONEã®é †ã«æ¡ä»¶ã«å¿œã˜ã¦åˆ†å‰²ã™ã‚‹ã€‚1.å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆãŒmaxå€¤ä»¥ä¸Šã®å ´åˆã€æœ€åˆã®åˆ†å‰²æ–‡å­—ã§åˆ†å‰²ã€2.1.ãŒå¤±æ•—ã—ãŸå ´åˆã«ã€2ç•ªç›®ã®åˆ†å‰²æ–‡å­—ã§åˆ†å‰²ã€3.åˆ†å‰²æ–‡å­—ãŒå­˜åœ¨ã—ãªã„å ´åˆã€ãƒ†ã‚­ã‚¹ãƒˆä¸­ã®ã©ã®ä½ç½®ã«ãŠã„ã¦ã‚‚MAXã§åˆ†å‰²ã€‚SENTENCE: æ–‡æœ«ã®å¥èª­ç‚¹ã§åˆ†å‰²BY WORDSã¨BY VOCABULARYã§ã®ã¿æŒ‡å®šå¯èƒ½ã€‚MAXè¨­å®šã®ä»•æ–¹ã«ã‚ˆã£ã¦ã¯å¿…ãšå¥èª­ç‚¹ã§åŒºåˆ‡ã‚‰ã‚Œã‚‹ã‚ã‘ã§ã¯ãªã„ã®ã§æ³¨æ„ã€‚ä¾‹ãˆã°ã€æ–‡ãŒMAXå€¤ã‚ˆã‚Šã‚‚å¤§ãã„å ´åˆã€MAXå€¤ã§åŒºåˆ‡ã‚‰ã‚Œã‚‹ã€‚MAXå€¤ã‚ˆã‚Šã‚‚å°ã•ãªæ–‡ã®å ´åˆã§ã€2æ–‡ä»¥ä¸ŠãŒMAXã®åˆ¶é™å†…ã«åã¾ã‚‹ã®ã§ã‚ã‚Œã°1ã¤ã«åã‚ã‚‹ã€‚CUSTOM: ã‚«ã‚¹ã‚¿ãƒ åˆ†å‰²æ–‡å­—ãƒªã‚¹ãƒˆã«åŸºã¥ã„ã¦åˆ†å‰²ã€åˆ†å‰²æ–‡å­—ã¯æœ€å¤§16å€‹ã¾ã§ã€é•·ã•ã¯ãã‚Œãã‚Œ10æ–‡å­—ã¾ã§ã§æŒ‡å®šå¯èƒ½ã€‚"
                            )
                        with gr.Column():
                            tab_split_document_chunks_split_by_custom_text = gr.Text(
                                label="CUSTOM SPLIT CHARACTERS(SPLIT [BY] = CUSTOMã®å ´åˆã®ã¿æœ‰åŠ¹)",
                                # value="'\u3002', '.'",
                                visible=False,
                                info="ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®ã‚«ã‚¹ã‚¿ãƒ åˆ†å‰²æ–‡å­—ãƒªã‚¹ãƒˆã«åŸºã¥ã„ã¦åˆ†å‰²ã€åˆ†å‰²æ–‡å­—ã¯æœ€å¤§16å€‹ã¾ã§ã€é•·ã•ã¯ãã‚Œãã‚Œ10æ–‡å­—ã¾ã§ã§æŒ‡å®šå¯èƒ½ã€‚ã‚¿ãƒ– (\t)ã€æ”¹è¡Œ (\n)ã€ãŠã‚ˆã³ãƒ©ã‚¤ãƒ³ãƒ•ã‚£ãƒ¼ãƒ‰ (\r) ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ã¿ã‚’çœç•¥ã§ãã‚‹ã€‚ãŸã¨ãˆã°ã€'<html>','</html>'"
                            )
                    with gr.Row():
                        with gr.Column():
                            tab_split_document_chunks_normalize_radio = gr.Radio(
                                label="NORMALIZE",
                                choices=[("NONE", "NONE"),
                                         ("ALL", "ALL"),
                                         ("OPTIONS", "OPTIONS")],
                                value="ALL",
                                visible=False,
                                info="Default value: ALLã€‚ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹éš›ã«ã‚ã‚ŠãŒã¡ãªå•é¡Œã«ã¤ã„ã¦ã€è‡ªå‹•çš„ã«å‰å‡¦ç†ã€å¾Œå‡¦ç†ã‚’å®Ÿè¡Œã—é«˜å“è³ªãªãƒãƒ£ãƒ³ã‚¯ã¨ã—ã¦æ ¼ç´ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã€‚NONE: å‡¦ç†ã‚’è¡Œã‚ãªã„ã€‚ALL: ãƒãƒ«ãƒãƒã‚¤ãƒˆã®å¥èª­ç‚¹ã‚’ã‚·ãƒ³ã‚°ãƒ«ãƒã‚¤ãƒˆã«æ­£è¦åŒ–ã€‚OPTIONS: PUNCTUATION: ã‚¹ãƒãƒ¼ãƒˆå¼•ç”¨ç¬¦ã€ã‚¹ãƒãƒ¼ãƒˆãƒã‚¤ãƒ•ãƒ³ã€ãƒãƒ«ãƒãƒã‚¤ãƒˆå¥èª­ç‚¹ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å«ã‚ã‚‹ã€‚WHITESPACE: ä¸è¦ãªæ–‡å­—ã‚’å‰Šé™¤ã—ã¦ç©ºç™½ã‚’æœ€å°é™ã«æŠ‘ãˆã‚‹ä¾‹ãˆã°ç©ºç™½è¡Œã¯ãã®ã¾ã¾ã«ã€ä½™åˆ†ãªæ”¹è¡Œã‚„ã‚¹ãƒšãƒ¼ã‚¹ã€ã‚¿ãƒ–ã‚’å‰Šé™¤ã™ã‚‹ã€‚WIDECHAR: ãƒãƒ«ãƒãƒã‚¤ãƒˆæ•°å­—ã¨ãƒ­ãƒ¼ãƒå­—ã‚’ã‚·ãƒ³ã‚°ãƒ«ãƒã‚¤ãƒˆã«æ­£è¦åŒ–ã™ã‚‹"
                            )
                        with gr.Column():
                            tab_split_document_chunks_normalize_options_checkbox_group = gr.CheckboxGroup(
                                label="NORMALIZE OPTIONS(NORMALIZE = OPTIONSã®å ´åˆã®ã¿æœ‰åŠ¹ã‹ã¤å¿…é ˆ)",
                                choices=[
                                    ("PUNCTUATION", "PUNCTUATION"),
                                    ("WHITESPACE", "WHITESPACE"),
                                    ("WIDECHAR", "WIDECHAR")],
                                visible=False,
                                info="ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹éš›ã«ã‚ã‚ŠãŒã¡ãªå•é¡Œã«ã¤ã„ã¦ã€è‡ªå‹•çš„ã«å‰å‡¦ç†ã€å¾Œå‡¦ç†ã‚’å®Ÿè¡Œã—é«˜å“è³ªãªãƒãƒ£ãƒ³ã‚¯ã¨ã—ã¦æ ¼ç´ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã€‚PUNCTUATION: ã‚¹ãƒãƒ¼ãƒˆå¼•ç”¨ç¬¦ã€ã‚¹ãƒãƒ¼ãƒˆãƒã‚¤ãƒ•ãƒ³ã€ãƒãƒ«ãƒãƒã‚¤ãƒˆå¥èª­ç‚¹ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å«ã‚ã‚‹ã€‚WHITESPACE: ä¸è¦ãªæ–‡å­—ã‚’å‰Šé™¤ã—ã¦ç©ºç™½ã‚’æœ€å°é™ã«æŠ‘ãˆã‚‹ä¾‹ãˆã°ç©ºç™½è¡Œã¯ãã®ã¾ã¾ã«ã€ä½™åˆ†ãªæ”¹è¡Œã‚„ã‚¹ãƒšãƒ¼ã‚¹ã€ã‚¿ãƒ–ã‚’å‰Šé™¤ã™ã‚‹ã€‚WIDECHAR: ãƒãƒ«ãƒãƒã‚¤ãƒˆæ•°å­—ã¨ãƒ­ãƒ¼ãƒå­—ã‚’ã‚·ãƒ³ã‚°ãƒ«ãƒã‚¤ãƒˆã«æ­£è¦åŒ–ã™ã‚‹"
                            )

                with gr.Row():
                    with gr.Column():
                        # doc_id_text = gr.Textbox(label="Doc ID*", lines=1)
                        tab_split_document_doc_id_radio = gr.Radio(
                            choices=get_doc_list(),
                            label="ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ*",
                        )

                with gr.Row():
                    with gr.Column():
                        tab_split_document_split_button = gr.Button(value="åˆ†å‰²", variant="primary")
                    with gr.Column():
                        tab_split_document_embed_save_button = gr.Button(
                            value="ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦ä¿å­˜ï¼ˆãƒ‡ãƒ¼ã‚¿é‡ãŒå¤šã„ã¨æ™‚é–“ãŒã‹ã‹ã‚‹ï¼‰",
                            variant="primary"
                        )

                with gr.Accordion(label="ä½¿ç”¨ã•ã‚ŒãŸSQL", open=False) as tab_split_document_sql_accordion:
                    tab_split_document_output_sql_text = gr.Textbox(
                        label="ä½¿ç”¨ã•ã‚ŒãŸSQL",
                        show_label=False,
                        lines=10,
                        autoscroll=False,
                        show_copy_button=True
                    )
                with gr.Row():
                    tab_split_document_chunks_count = gr.Textbox(label="ãƒãƒ£ãƒ³ã‚¯æ•°", lines=1)
                with gr.Row():
                    tab_split_document_chunks_result_dataframe = gr.Dataframe(
                        label="ãƒãƒ£ãƒ³ã‚¯çµæœ",
                        headers=["CHUNK_ID", "CHUNK_OFFSET", "CHUNK_LENGTH", "CHUNK_DATA"],
                        datatype=["str", "str", "str", "str"],
                        row_count=(1, "fixed"),
                        col_count=(4, "fixed"),
                        wrap=True,
                        column_widths=["10%", "10%", "10%", "70%"]
                    )
                with gr.Accordion(label="ãƒãƒ£ãƒ³ã‚¯è©³ç´°",
                                  open=True) as tab_split_document_chunks_result_detail:
                    with gr.Row():
                        with gr.Column():
                            tab_split_document_chunks_result_detail_chunk_id = gr.Textbox(
                                label="CHUNK_ID",
                                interactive=False,
                                lines=1
                            )
                    with gr.Row():
                        with gr.Column():
                            tab_split_document_chunks_result_detail_chunk_data = gr.Textbox(
                                label="CHUNK_DATA",
                                interactive=True,
                                lines=5,
                                max_lines=5,
                            )
                    with gr.Row():
                        with gr.Column():
                            tab_split_document_chunks_result_detail_update_button = gr.Button(
                                value="æ›´æ–°",
                                variant="primary"
                            )

            with gr.TabItem(label="Step-3.å‰Šé™¤(ã‚ªãƒ—ã‚·ãƒ§ãƒ³)") as tab_delete_document:
                with gr.Accordion(
                        label="ä½¿ç”¨ã•ã‚ŒãŸSQL",
                        open=False
                ) as tab_delete_document_sql_accordion:
                    tab_delete_document_delete_sql = gr.Textbox(
                        label="ç”Ÿæˆã•ã‚ŒãŸSQL",
                        show_label=False,
                        lines=10,
                        autoscroll=False,
                        show_copy_button=True
                    )
                with gr.Row():
                    with gr.Column():
                        tab_delete_document_server_directory_text = gr.Text(
                            label="ã‚µãƒ¼ãƒãƒ¼ãƒ»ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª*",
                            value="/u01/data/no1rag/"
                        )
                with gr.Row():
                    with gr.Column():
                        # doc_id_text = gr.Textbox(label="Doc ID*", lines=1)
                        tab_delete_document_doc_ids_checkbox_group = gr.CheckboxGroup(
                            choices=get_doc_list(),
                            label="ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ*",
                            type="value",
                            value=[],
                        )
                with gr.Row():
                    with gr.Column():
                        tab_delete_document_delete_button = gr.Button(value="å‰Šé™¤", variant="primary")
            with gr.TabItem(label="Step-4.ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ãƒãƒ£ãƒƒãƒˆ") as tab_chat_document:
                with gr.Row():
                    with gr.Column():
                        tab_chat_document_llm_answer_checkbox_group = gr.CheckboxGroup(
                            [
                                "xai/grok-3",
                                "cohere/command-a",
                                "meta/llama-4-maverick-17b-128e-instruct-fp8",
                                "meta/llama-4-scout-17b-16e-instruct",
                                "meta/llama-3-3-70b",
                                "meta/llama-3-2-90b-vision",
                                "openai/gpt-4o",
                                "openai/gpt-4",
                                "azure_openai/gpt-4o",
                                "azure_openai/gpt-4"
                            ],
                            label="LLM ãƒ¢ãƒ‡ãƒ«",
                            value=[]
                        )
                with gr.Row():
                    with gr.Column():
                        tab_chat_document_question_embedding_model_checkbox_group = gr.CheckboxGroup(
                            ["cohere/embed-multilingual-v3.0"],
                            label="Embedding ãƒ¢ãƒ‡ãƒ«*",
                            value="cohere/embed-multilingual-v3.0",
                            interactive=False
                        )
                    with gr.Column():
                        tab_chat_document_reranker_model_radio = gr.Radio(
                            [
                                "None",
                                # "cohere/rerank-multilingual-v3.1",
                                # "cohere/rerank-english-v3.1",
                                "cohere/rerank-multilingual-v3.0",
                                "cohere/rerank-english-v3.0",
                            ],
                            label="Rerank ãƒ¢ãƒ‡ãƒ«*", value="None")
                with gr.Row():
                    with gr.Column():
                        tab_chat_document_top_k_slider = gr.Slider(
                            label="é¡ä¼¼æ¤œç´¢ Top-K*",
                            minimum=1,
                            maximum=100,
                            step=1,
                            info="Default value: 20ã€‚é¡ä¼¼åº¦è·é›¢ã®ä½ã„é †ï¼ˆ=é¡ä¼¼åº¦ã®é«˜ã„é †ï¼‰ã§ä¸Šä½Kä»¶ã®ã¿ã‚’æŠ½å‡ºã™ã‚‹ã€‚",
                            interactive=True,
                            value=20
                        )
                    with gr.Column():
                        tab_chat_document_threshold_value_slider = gr.Slider(
                            label="é¡ä¼¼æ¤œç´¢é–¾å€¤*",
                            minimum=0.10,
                            info="Default value: 0.55ã€‚é¡ä¼¼åº¦è·é›¢ãŒé–¾å€¤ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æŠ½å‡ºã™ã‚‹ã€‚",
                            maximum=0.95,
                            step=0.05,
                            value=0.55
                        )
                with gr.Row():
                    with gr.Column():
                        tab_chat_document_reranker_top_k_slider = gr.Slider(
                            label="Rerank Top-K*",
                            minimum=1,
                            maximum=50,
                            step=1,
                            info="Default value: 5ã€‚Rerank Scoreã®é«˜ã„é †ã§ä¸Šä½Kä»¶ã®ã¿ã‚’æŠ½å‡ºã™ã‚‹ã€‚",
                            interactive=True,
                            value=5
                        )
                    with gr.Column():
                        tab_chat_document_reranker_threshold_slider = gr.Slider(
                            label="Rerank Score é–¾å€¤*",
                            minimum=0.0,
                            info="Default value: 0.0045ã€‚Rerank ScoreãŒé–¾å€¤ä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æŠ½å‡ºã™ã‚‹ã€‚",
                            maximum=0.99,
                            step=0.0005,
                            value=0.0045,
                            interactive=True
                        )
                with gr.Accordion("Advanced Settings", open=False):
                    with gr.Row():
                        with gr.Column():
                            tab_chat_document_answer_by_one_checkbox = gr.Checkbox(
                                label="Highest-Ranked-One æ–‡æ›¸ã«ã‚ˆã‚‹å›ç­”",
                                value=False,
                                info="ä»–ã®ã™ã¹ã¦ã®æ–‡æ›¸ã‚’ç„¡è¦–ã—ã€æœ€ä¸Šä½ã«ãƒ©ãƒ³ã‚¯ã•ã‚ŒãŸ1ã¤ã®æ–‡æ›¸ã®ã¿ã«ã‚ˆã£ã¦å›ç­”ã™ã‚‹ã€‚"
                            )
                        with gr.Column():
                            pass
                        with gr.Column(visible=False):
                            tab_chat_document_partition_by_k_slider = gr.Slider(
                                label="Partition-By-K",
                                minimum=0,
                                maximum=20,
                                step=1,
                                interactive=True,
                                info="Default value: 0ã€‚é¡ä¼¼æ¤œç´¢ã®å¯¾è±¡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°ã‚’æŒ‡å®šã€‚0: å…¨éƒ¨ã€‚1: 1å€‹ã€‚2ï¼š2å€‹ã€‚... n: nå€‹ã€‚"
                            )
                    with gr.Row():
                        with gr.Column():
                            tab_chat_document_extend_first_chunk_size = gr.Slider(
                                label="Extend-First-K", minimum=0,
                                maximum=50,
                                step=1,
                                interactive=True,
                                value=0,
                                info="Default value: 0ã€‚DISTANCEè¨ˆç®—å¯¾è±¡å¤–ã€‚æ¤œç´¢ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ã‚’æ‹¡å¼µã™ã‚‹æ•°ã‚’æŒ‡å®šã€‚0: æ‹¡å¼µã—ãªã„ã€‚ 1: æœ€åˆã®1å€‹ã‚’æ‹¡å¼µã€‚2: æœ€åˆã®2å€‹ã‚’æ‹¡å¼µã€‚ ... n: æœ€åˆã®nå€‹ã‚’æ‹¡å¼µã€‚"
                            )
                        with gr.Column():
                            tab_chat_document_extend_around_chunk_size = gr.Slider(
                                label="Extend-Around-K",
                                minimum=0,
                                maximum=50, step=2,
                                interactive=True,
                                value=2,
                                info="Default value: 2ã€‚DISTANCEè¨ˆç®—å¯¾è±¡å¤–ã€‚æ¤œç´¢ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ã‚’æ‹¡å¼µã™ã‚‹æ•°ã‚’æŒ‡å®šã€‚0: æ‹¡å¼µã—ãªã„ã€‚ 2: 2å€‹ã§å‰å¾Œãã‚Œãã‚Œ1å€‹ã‚’æ‹¡å¼µã€‚4: 4å€‹ã§å‰å¾Œãã‚Œãã‚Œ2å€‹ã‚’æ‹¡å¼µã€‚... n: nå€‹ã§å‰å¾Œãã‚Œãã‚Œn/2å€‹ã‚’æ‹¡å¼µã€‚"
                            )
                    with gr.Row():
                        with gr.Column():
                            tab_chat_document_text_search_checkbox = gr.Checkbox(
                                label="ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢",
                                value=False,
                                info="ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ã¯å…ƒã®ã‚¯ã‚¨ãƒªã«é™å®šã•ã‚Œã€ã‚¯ã‚¨ãƒªã®æ‹¡å¼µã§ç”Ÿæˆã•ã‚ŒãŸã‚¯ã‚¨ãƒªã¯ç„¡è¦–ã•ã‚Œã‚‹ã€‚"
                            )
                        with gr.Column():
                            tab_chat_document_text_search_k_slider = gr.Slider(
                                label="ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢Limit-K",
                                minimum=1,
                                maximum=10,
                                step=1,
                                value=6,
                                info="Default value: 6ã€‚ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ã«ä½¿ç”¨ã§ãã‚‹å˜èªæ•°ã®åˆ¶é™ã€‚"
                            )
                    with gr.Accordion(label="RAG Prompt è¨­å®š", open=False) as tab_chat_document_rag_prompt_accordion:
                        with gr.Row():
                            with gr.Column():
                                tab_chat_document_rag_prompt_text = gr.Textbox(
                                    label="RAG Prompt ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ",
                                    lines=15,
                                    max_lines=25,
                                    interactive=True,
                                    show_copy_button=True,
                                    value=get_langgpt_rag_prompt("{{context}}", "{{query_text}}", False, False),
                                    info="RAGã§ä½¿ç”¨ã•ã‚Œã‚‹promptãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ã™ã€‚{{context}}ã¨{{query_text}}ã¯å®Ÿè¡Œæ™‚ã«ç½®æ›ã•ã‚Œã¾ã™ã€‚"
                                )
                        with gr.Row():
                            with gr.Column(scale=1):
                                tab_chat_document_rag_prompt_reset_button = gr.Button(
                                    value="ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«æˆ»ã™",
                                    variant="secondary"
                                )
                            with gr.Column(scale=1):
                                tab_chat_document_rag_prompt_save_button = gr.Button(
                                    value="ä¿å­˜",
                                    variant="primary",
                                    visible=False,
                                )
                    with gr.Row():
                        with gr.Column():
                            tab_chat_document_include_citation_checkbox = gr.Checkbox(
                                label="å›ç­”ã«å¼•ç”¨ã‚’å«ã‚ã‚‹",
                                value=False,
                                info="å›ç­”ã«ã¯å¼•ç”¨ã‚’å«ã‚ã€ä½¿ç”¨ã—ãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’å¼•ç”¨ã¨ã—ã¦å‡ºåŠ›ã™ã‚‹ã€‚"
                            )
                        with gr.Column():
                            tab_chat_document_include_current_time_checkbox = gr.Checkbox(
                                label="Promptã«ç¾åœ¨ã®æ™‚é–“ã‚’å«ã‚ã‚‹",
                                value=False,
                                info="Promptã«å›ç­”æ™‚ã®ç¾åœ¨ã®æ™‚é–“ã‚’å«ã‚ã¾ã™ã€‚"
                            )
                    with gr.Row():
                        with gr.Column():
                            tab_chat_document_use_image_checkbox = gr.Checkbox(
                                label="ç”»åƒã‚’ä½¿ã£ã¦å›ç­”",
                                value=False,
                                info="RAGã®å›ç­”æ™‚ã«ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã€‚ãŸã ã—ã€å‡¦ç†ã™ã‚‹ç”»åƒæ•°ã‚’10å€‹ã«åˆ¶é™ã€‚"
                            )
                        with gr.Column():
                            tab_chat_document_single_image_processing_checkbox = gr.Checkbox(
                                label="1æšãšã¤å‡¦ç†",
                                value=True,
                                info="ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã¨ç”»åƒã‚’1æšãšã¤å‡¦ç†ã€ãƒã‚§ãƒƒã‚¯ã—ãªã„ã¨å…¨ã¦ã®ç”»åƒã‚’ä¸€æ‹¬å‡¦ç†ã€‚"
                            )
                    with gr.Accordion(label="ç”»åƒ Prompt è¨­å®š", open=False,
                                      visible=False) as tab_chat_document_image_prompt_accordion:
                        with gr.Row():
                            with gr.Column():
                                tab_chat_document_image_prompt_text = gr.Textbox(
                                    label="ç”»åƒ Prompt ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ",
                                    lines=15,
                                    max_lines=25,
                                    interactive=True,
                                    show_copy_button=True,
                                    value=get_image_qa_prompt("{{query_text}}"),
                                    info="ç”»åƒå‡¦ç†ã§ä½¿ç”¨ã•ã‚Œã‚‹promptãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ã™ã€‚{{query_text}}ã¯å®Ÿè¡Œæ™‚ã«ç½®æ›ã•ã‚Œã¾ã™ã€‚"
                                )
                        with gr.Row():
                            with gr.Column(scale=1):
                                tab_chat_document_image_prompt_reset_button = gr.Button(
                                    value="ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«æˆ»ã™",
                                    variant="secondary"
                                )
                            with gr.Column(scale=1):
                                gr.Markdown("&nbsp;")
                                # tab_chat_document_image_prompt_save_button = gr.Button(
                                #     value="ä¿å­˜",
                                #     variant="primary",
                                #     visible=False,
                                # )
                    with gr.Row():
                        with gr.Column():
                            tab_chat_document_document_metadata_text = gr.Textbox(
                                label="ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿",
                                lines=1,
                                max_lines=1,
                                autoscroll=True,
                                show_copy_button=False,
                                interactive=True,
                                info="key1=value1,key2=value2,... ã®å½¢å¼ã§å…¥åŠ›ã™ã‚‹ã€‚",
                                placeholder="key1=value1,key2=value2,..."
                            )
                with gr.Row(visible=False):
                    tab_chat_document_accuracy_plan_radio = gr.Radio(
                        [
                            "Somewhat Inaccurate",
                            "Decent Accuracy",
                            "Extremely Precise"
                        ],
                        label="Accuracy Plan*",
                        value="Somewhat Inaccurate",
                        interactive=True
                    )
                with gr.Row():
                    tab_chat_document_llm_evaluation_checkbox = gr.Checkbox(
                        label="LLM è©•ä¾¡",
                        show_label=True,
                        interactive=True,
                        value=False,
                    )
                with gr.Row():
                    #                     tab_chat_document_system_message_text = gr.Textbox(label="ã‚·ã‚¹ãƒ†ãƒ ãƒ»ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸*", lines=15,
                    #                                                                        max_lines=20,
                    #                                                                        interactive=True,
                    #                                                                        visible=False,
                    #                                                                        value=f"""You are an ANSWER EVALUATOR.
                    # Your task is to compare a given answer to a standard answer and evaluate its quality.
                    # Respond with a score from 0 to 10 for each of the following criteria:
                    # 1. Correctness (0 being completely incorrect, 10 being perfectly correct)
                    # 2. Completeness (0 being entirely incomplete, 10 being fully complete)
                    # 3. Clarity (0 being very unclear, 10 being extremely clear)
                    # 4. Conciseness (0 being extremely verbose, 10 being optimally concise)
                    #
                    # After providing scores, give a brief explanation for each score.
                    # Finally, provide an overall score from 0 to 10 and a summary of the evaluation.
                    # Please respond to me in the same language I use for my messages.
                    # If I switch languages, please switch your responses accordingly.
                    #                 """)
                    tab_chat_document_system_message_text = gr.Textbox(
                        label="ã‚·ã‚¹ãƒ†ãƒ ãƒ»ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸*",
                        lines=15,
                        max_lines=20,
                        interactive=True,
                        visible=False,
                        value=get_llm_evaluation_system_message())
                with gr.Row():
                    tab_chat_document_standard_answer_text = gr.Textbox(
                        label="æ¨™æº–å›ç­”*",
                        lines=2,
                        interactive=True,
                        visible=False
                    )
                with gr.Accordion("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ*", open=True):
                    with gr.Row():
                        with gr.Column():
                            tab_chat_document_doc_id_all_checkbox = gr.Checkbox(label="å…¨éƒ¨", value=True)
                    with gr.Row():
                        with gr.Column():
                            # doc_id_text = gr.Textbox(label="Doc ID*", lines=1)
                            tab_chat_document_doc_id_checkbox_group = gr.CheckboxGroup(
                                choices=get_doc_list(),
                                label="ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ*",
                                show_label=False,
                                interactive=False,
                            )
                with gr.Row() as tab_chat_document_searched_query_row:
                    with gr.Column():
                        tab_chat_document_query_text = gr.Textbox(label="ã‚¯ã‚¨ãƒª*", lines=2)
                # with gr.Accordion("Sub-Query/RAG-Fusion/HyDE/Step-Back-Prompting/Customized-Multi-Step-Query", open=True):
                with gr.Accordion("ã‚¯ã‚¨ãƒªã®æ‹¡å¼µ", open=False):
                    with gr.Row():
                        # generate_query_radio = gr.Radio(
                        #     ["None", "Sub-Query", "RAG-Fusion", "HyDE", "Step-Back-Prompting",
                        #      "Customized-Multi-Step-Query"],
                        tab_chat_document_generate_query_radio = gr.Radio(
                            [
                                ("None", "None"),
                                ("ã‚µãƒ–ã‚¯ã‚¨ãƒª", "Sub-Query"),
                                ("é¡ä¼¼ã‚¯ã‚¨ãƒª", "RAG-Fusion"),
                                ("ä»®å›ç­”", "HyDE"),
                                ("æŠ½è±¡åŒ–ã‚¯ã‚¨ãƒª", "Step-Back-Prompting")
                            ],
                            label="LLMã«ã‚ˆã£ã¦ç”Ÿæˆï¼Ÿ",
                            value="None",
                            interactive=True
                        )
                    with gr.Row():
                        tab_chat_document_sub_query1_text = gr.Textbox(
                            # label="(Sub-Query)ã‚µãƒ–ã‚¯ã‚¨ãƒª1/(RAG-Fusion)é¡ä¼¼ã‚¯ã‚¨ãƒª1/(HyDE)ä»®å›ç­”1/(Step-Back-Prompting)æŠ½è±¡åŒ–ã‚¯ã‚¨ãƒª1/(Customized-Multi-Step-Query)ãƒãƒ«ãƒã‚¹ãƒ†ãƒƒãƒ—ã‚¯ã‚¨ãƒª1",
                            label="ç”Ÿæˆã•ã‚ŒãŸã‚¯ã‚¨ãƒª1",
                            lines=1,
                            interactive=True,
                            info=""
                        )
                    with gr.Row():
                        tab_chat_document_sub_query2_text = gr.Textbox(
                            # label="(Sub-Query)ã‚µãƒ–ã‚¯ã‚¨ãƒª2/(RAG-Fusion)é¡ä¼¼ã‚¯ã‚¨ãƒª2/(HyDE)ä»®å›ç­”2/(Step-Back-Prompting)æŠ½è±¡åŒ–ã‚¯ã‚¨ãƒª2/(Customized-Multi-Step-Query)ãƒãƒ«ãƒã‚¹ãƒ†ãƒƒãƒ—ã‚¯ã‚¨ãƒª2",
                            label="ç”Ÿæˆã•ã‚ŒãŸã‚¯ã‚¨ãƒª2",
                            lines=1,
                            interactive=True
                        )
                    with gr.Row():
                        tab_chat_document_sub_query3_text = gr.Textbox(
                            # label="(Sub-Query)ã‚µãƒ–ã‚¯ã‚¨ãƒª3/(RAG-Fusion)é¡ä¼¼ã‚¯ã‚¨ãƒª3/(HyDE)ä»®å›ç­”3/(Step-Back-Prompting)æŠ½è±¡åŒ–ã‚¯ã‚¨ãƒª3/(Customized-Multi-Step-Query)ãƒãƒ«ãƒã‚¹ãƒ†ãƒƒãƒ—ã‚¯ã‚¨ãƒª3",
                            label="ç”Ÿæˆã•ã‚ŒãŸã‚¯ã‚¨ãƒª3",
                            lines=1,
                            interactive=True
                        )
                with gr.Row() as tab_chat_document_chat_document_row:
                    with gr.Column():
                        tab_chat_document_chat_document_button = gr.Button(value="é€ä¿¡", variant="primary")
                with gr.Accordion(label="ä½¿ç”¨ã•ã‚ŒãŸSQL", open=False) as tab_chat_document_sql_accordion:
                    tab_chat_document_output_sql_text = gr.Textbox(
                        label="ä½¿ç”¨ã•ã‚ŒãŸSQL",
                        show_label=False,
                        lines=25,
                        max_lines=25,
                        autoscroll=False,
                        show_copy_button=True
                    )
                with gr.Row() as tab_chat_document_searched_data_summary_row:
                    with gr.Column(scale=10):
                        tab_chat_document_searched_data_summary_text = gr.Markdown(
                            value="",
                            visible=False
                        )
                    with gr.Column(scale=2):
                        tab_chat_document_download_output_button = gr.DownloadButton(
                            label="ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                            visible=False,
                            variant="secondary",
                            size="sm"
                        )
                with gr.Row() as searched_result_row:
                    with gr.Column():
                        tab_chat_document_searched_result_dataframe = gr.Dataframe(
                            headers=["NO", "CONTENT", "EMBED_ID", "SOURCE", "DISTANCE", "SCORE", "KEY_WORDS"],
                            datatype=["str", "str", "str", "str", "str", "str", "str"],
                            row_count=(5, "fixed"),
                            max_height=400,
                            col_count=(7, "fixed"),
                            wrap=True,
                            column_widths=["4%", "50%", "6%", "8%", "6%", "6%", "8%"],
                            interactive=False,
                        )
                with gr.Accordion(
                        label="XAI Grok-3 ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
                        visible=False,
                        open=True
                ) as tab_chat_document_llm_xai_grok_3_accordion:
                    tab_chat_document_xai_grok_3_answer_text = gr.Markdown(
                        show_copy_button=True,
                        height=300,
                        min_height=300,
                        max_height=300
                    )
                    with gr.Accordion(
                            label="Human è©•ä¾¡",
                            visible=True,
                            open=True
                    ) as tab_chat_document_llm_xai_grok_3_human_evaluation_accordion:
                        with gr.Row():
                            tab_chat_document_xai_grok_3_answer_human_eval_feedback_radio = gr.Radio(
                                show_label=False,
                                choices=[
                                    ("Good response", "good"),
                                    ("Neutral response", "neutral"),
                                    ("Bad response", "bad"),
                                ],
                                value="good",
                                container=False,
                                interactive=True,
                            )
                        with gr.Row():
                            with gr.Column(scale=11):
                                tab_chat_document_xai_grok_3_answer_human_eval_feedback_text = gr.Textbox(
                                    show_label=False,
                                    container=False,
                                    lines=2,
                                    interactive=True,
                                    autoscroll=True,
                                    placeholder="å…·ä½“çš„ãªæ„è¦‹ã‚„æ„Ÿæƒ³ã‚’è‡ªç”±ã«æ›¸ã„ã¦ãã ã•ã„ã€‚",
                                )
                            with gr.Column(scale=1):
                                tab_chat_document_xai_grok_3_answer_human_eval_feedback_send_button = gr.Button(
                                    value="é€ä¿¡",
                                    variant="primary",
                                )
                    with gr.Accordion(
                            label="LLM è©•ä¾¡çµæœ",
                            visible=False,
                            open=True
                    ) as tab_chat_document_llm_xai_grok_3_evaluation_accordion:
                        tab_chat_document_xai_grok_3_evaluation_text = gr.Markdown(
                            show_copy_button=True,
                            height=200,
                            min_height=200,
                            max_height=300
                        )
                with gr.Accordion(
                        label="Command-A ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
                        visible=False,
                        open=True
                ) as tab_chat_document_llm_command_a_accordion:
                    tab_chat_document_command_a_answer_text = gr.Markdown(
                        show_copy_button=True,
                        height=300,
                        min_height=300,
                        max_height=300
                    )
                    with gr.Accordion(
                            label="Human è©•ä¾¡",
                            visible=True,
                            open=True
                    ) as tab_chat_document_llm_command_a_human_evaluation_accordion:
                        with gr.Row():
                            tab_chat_document_command_a_answer_human_eval_feedback_radio = gr.Radio(
                                show_label=False,
                                choices=[
                                    ("Good response", "good"),
                                    ("Neutral response", "neutral"),
                                    ("Bad response", "bad"),
                                ],
                                value="good",
                                container=False,
                                interactive=True,
                            )
                        with gr.Row():
                            with gr.Column(scale=11):
                                tab_chat_document_command_a_answer_human_eval_feedback_text = gr.Textbox(
                                    show_label=False,
                                    container=False,
                                    lines=2,
                                    interactive=True,
                                    autoscroll=True,
                                    placeholder="å…·ä½“çš„ãªæ„è¦‹ã‚„æ„Ÿæƒ³ã‚’è‡ªç”±ã«æ›¸ã„ã¦ãã ã•ã„ã€‚",
                                )
                            with gr.Column(scale=1):
                                tab_chat_document_command_a_answer_human_eval_feedback_send_button = gr.Button(
                                    value="é€ä¿¡",
                                    variant="primary",
                                )
                    with gr.Accordion(
                            label="LLM è©•ä¾¡çµæœ",
                            visible=False,
                            open=True
                    ) as tab_chat_document_llm_command_a_evaluation_accordion:
                        tab_chat_document_command_a_evaluation_text = gr.Markdown(
                            show_copy_button=True,
                            height=200,
                            min_height=200,
                            max_height=300
                        )


                with gr.Accordion(
                        label="Llama 4 Maverick 17b ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
                        visible=False,
                        open=True
                ) as tab_chat_document_llm_llama_4_maverick_accordion:
                    tab_chat_document_llama_4_maverick_answer_text = gr.Markdown(
                        show_copy_button=True,
                        height=300,
                        min_height=300,
                        max_height=300
                    )
                    with gr.Accordion(
                            label="ç”»åƒå›ç­”",
                            visible=False,
                            open=True
                    ) as tab_chat_document_llm_llama_4_maverick_image_accordion:
                        tab_chat_document_llama_4_maverick_image_answer_text = gr.Markdown(
                            show_copy_button=True,
                            height=600,
                            min_height=600,
                            max_height=600
                        )
                    with gr.Accordion(
                            label="Human è©•ä¾¡",
                            visible=True,
                            open=True
                    ) as tab_chat_document_llm_llama_4_maverick_human_evaluation_accordion:
                        with gr.Row():
                            tab_chat_document_llama_4_maverick_answer_human_eval_feedback_radio = gr.Radio(
                                show_label=False,
                                choices=[
                                    ("Good response", "good"),
                                    ("Neutral response", "neutral"),
                                    ("Bad response", "bad"),
                                ],
                                value="good",
                                container=False,
                                interactive=True,
                            )
                        with gr.Row():
                            with gr.Column(scale=11):
                                tab_chat_document_llama_4_maverick_answer_human_eval_feedback_text = gr.Textbox(
                                    show_label=False,
                                    container=False,
                                    lines=2,
                                    interactive=True,
                                    autoscroll=True,
                                    placeholder="å…·ä½“çš„ãªæ„è¦‹ã‚„æ„Ÿæƒ³ã‚’è‡ªç”±ã«æ›¸ã„ã¦ãã ã•ã„ã€‚",
                                )
                            with gr.Column(scale=1):
                                tab_chat_document_llama_4_maverick_answer_human_eval_feedback_send_button = gr.Button(
                                    value="é€ä¿¡",
                                    variant="primary",
                                )
                    with gr.Accordion(
                            label="LLM è©•ä¾¡çµæœ",
                            visible=False,
                            open=True
                    ) as tab_chat_document_llm_llama_4_maverick_evaluation_accordion:
                        tab_chat_document_llama_4_maverick_evaluation_text = gr.Markdown(
                            show_copy_button=True,
                            height=200,
                            min_height=200,
                            max_height=300
                        )
                with gr.Accordion(
                        label="Llama 4 Scout 17b ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
                        visible=False,
                        open=True
                ) as tab_chat_document_llm_llama_4_scout_accordion:
                    tab_chat_document_llama_4_scout_answer_text = gr.Markdown(
                        show_copy_button=True,
                        height=300,
                        min_height=300,
                        max_height=300
                    )
                    with gr.Accordion(
                            label="ç”»åƒå›ç­”",
                            visible=False,
                            open=True
                    ) as tab_chat_document_llm_llama_4_scout_image_accordion:
                        tab_chat_document_llama_4_scout_image_answer_text = gr.Markdown(
                            show_copy_button=True,
                            height=600,
                            min_height=600,
                            max_height=600
                        )
                    with gr.Accordion(
                            label="Human è©•ä¾¡",
                            visible=True,
                            open=True
                    ) as tab_chat_document_llm_llama_4_scout_human_evaluation_accordion:
                        with gr.Row():
                            tab_chat_document_llama_4_scout_answer_human_eval_feedback_radio = gr.Radio(
                                show_label=False,
                                choices=[
                                    ("Good response", "good"),
                                    ("Neutral response", "neutral"),
                                    ("Bad response", "bad"),
                                ],
                                value="good",
                                container=False,
                                interactive=True,
                            )
                        with gr.Row():
                            with gr.Column(scale=11):
                                tab_chat_document_llama_4_scout_answer_human_eval_feedback_text = gr.Textbox(
                                    show_label=False,
                                    container=False,
                                    lines=2,
                                    interactive=True,
                                    autoscroll=True,
                                    placeholder="å…·ä½“çš„ãªæ„è¦‹ã‚„æ„Ÿæƒ³ã‚’è‡ªç”±ã«æ›¸ã„ã¦ãã ã•ã„ã€‚",
                                )
                            with gr.Column(scale=1):
                                tab_chat_document_llama_4_scout_answer_human_eval_feedback_send_button = gr.Button(
                                    value="é€ä¿¡",
                                    variant="primary",
                                )
                    with gr.Accordion(
                            label="LLM è©•ä¾¡çµæœ",
                            visible=False,
                            open=True
                    ) as tab_chat_document_llm_llama_4_scout_evaluation_accordion:
                        tab_chat_document_llama_4_scout_evaluation_text = gr.Markdown(
                            show_copy_button=True,
                            height=200,
                            min_height=200,
                            max_height=300
                        )
                with gr.Accordion(
                        label="Llama 3.3 70b ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
                        visible=False,
                        open=True
                ) as tab_chat_document_llm_llama_3_3_70b_accordion:
                    tab_chat_document_llama_3_3_70b_answer_text = gr.Markdown(
                        show_copy_button=True,
                        height=300,
                        min_height=300,
                        max_height=300
                    )
                    with gr.Accordion(
                            label="Human è©•ä¾¡",
                            visible=True,
                            open=True
                    ) as tab_chat_document_llm_llama_3_3_70b_human_evaluation_accordion:
                        with gr.Row():
                            tab_chat_document_llama_3_3_70b_answer_human_eval_feedback_radio = gr.Radio(
                                show_label=False,
                                choices=[
                                    ("Good response", "good"),
                                    ("Neutral response", "neutral"),
                                    ("Bad response", "bad"),
                                ],
                                value="good",
                                container=False,
                                interactive=True,
                            )
                        with gr.Row():
                            with gr.Column(scale=11):
                                tab_chat_document_llama_3_3_70b_answer_human_eval_feedback_text = gr.Textbox(
                                    show_label=False,
                                    container=False,
                                    lines=2,
                                    interactive=True,
                                    autoscroll=True,
                                    placeholder="å…·ä½“çš„ãªæ„è¦‹ã‚„æ„Ÿæƒ³ã‚’è‡ªç”±ã«æ›¸ã„ã¦ãã ã•ã„ã€‚",
                                )
                            with gr.Column(scale=1):
                                tab_chat_document_llama_3_3_70b_answer_human_eval_feedback_send_button = gr.Button(
                                    value="é€ä¿¡",
                                    variant="primary",
                                )
                    with gr.Accordion(
                            label="LLM è©•ä¾¡çµæœ",
                            visible=False,
                            open=True
                    ) as tab_chat_document_llm_llama_3_3_70b_evaluation_accordion:
                        tab_chat_document_llama_3_3_70b_evaluation_text = gr.Markdown(
                            show_copy_button=True,
                            height=200,
                            min_height=200,
                            max_height=300
                        )
                with gr.Accordion(
                        label="Llama 3.2 90b Vision ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
                        visible=False,
                        open=True
                ) as tab_chat_document_llm_llama_3_2_90b_vision_accordion:
                    tab_chat_document_llama_3_2_90b_vision_answer_text = gr.Markdown(
                        show_copy_button=True,
                        height=300,
                        min_height=300,
                        max_height=300
                    )
                    with gr.Accordion(
                            label="ç”»åƒå›ç­”",
                            visible=False,
                            open=True
                    ) as tab_chat_document_llm_llama_3_2_90b_vision_image_accordion:
                        tab_chat_document_llama_3_2_90b_vision_image_answer_text = gr.Markdown(
                            show_copy_button=True,
                            height=600,
                            min_height=600,
                            max_height=600
                        )
                    with gr.Accordion(
                            label="Human è©•ä¾¡",
                            visible=True,
                            open=True
                    ) as tab_chat_document_llm_llama_3_2_90b_vision_human_evaluation_accordion:
                        with gr.Row():
                            tab_chat_document_llama_3_2_90b_vision_answer_human_eval_feedback_radio = gr.Radio(
                                show_label=False,
                                choices=[
                                    ("Good response", "good"),
                                    ("Neutral response", "neutral"),
                                    ("Bad response", "bad"),
                                ],
                                value="good",
                                container=False,
                                interactive=True,
                            )
                        with gr.Row():
                            with gr.Column(scale=11):
                                tab_chat_document_llama_3_2_90b_vision_answer_human_eval_feedback_text = gr.Textbox(
                                    show_label=False,
                                    container=False,
                                    lines=2,
                                    interactive=True,
                                    autoscroll=True,
                                    placeholder="å…·ä½“çš„ãªæ„è¦‹ã‚„æ„Ÿæƒ³ã‚’è‡ªç”±ã«æ›¸ã„ã¦ãã ã•ã„ã€‚",
                                )
                            with gr.Column(scale=1):
                                tab_chat_document_llama_3_2_90b_vision_answer_human_eval_feedback_send_button = gr.Button(
                                    value="é€ä¿¡",
                                    variant="primary",
                                )
                    with gr.Accordion(
                            label="LLM è©•ä¾¡çµæœ",
                            visible=False,
                            open=True
                    ) as tab_chat_document_llm_llama_3_2_90b_vision_evaluation_accordion:
                        tab_chat_document_llama_3_2_90b_vision_evaluation_text = gr.Markdown(
                            show_copy_button=True,
                            height=200,
                            min_height=200,
                            max_height=300
                        )
                with gr.Accordion(label="OpenAI gpt-4o ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
                                  visible=False,
                                  open=True) as tab_chat_document_llm_openai_gpt4o_accordion:
                    tab_chat_document_openai_gpt4o_answer_text = gr.Markdown(
                        show_copy_button=True,
                        height=300,
                        min_height=300,
                        max_height=300
                    )
                    with gr.Accordion(
                            label="ç”»åƒå›ç­”",
                            visible=False,
                            open=True
                    ) as tab_chat_document_llm_openai_gpt4o_image_accordion:
                        tab_chat_document_openai_gpt4o_image_answer_text = gr.Markdown(
                            show_copy_button=True,
                            height=600,
                            min_height=600,
                            max_height=600
                        )
                    with gr.Accordion(
                            label="Human è©•ä¾¡",
                            visible=True,
                            open=True
                    ) as tab_chat_document_llm_openai_gpt4o_human_evaluation_accordion:
                        with gr.Row():
                            tab_chat_document_openai_gpt4o_answer_human_eval_feedback_radio = gr.Radio(
                                show_label=False,
                                choices=[
                                    ("Good response", "good"),
                                    ("Neutral response", "neutral"),
                                    ("Bad response", "bad"),
                                ],
                                value="good",
                                container=False,
                                interactive=True,
                            )
                        with gr.Row():
                            with gr.Column(scale=11):
                                tab_chat_document_openai_gpt4o_answer_human_eval_feedback_text = gr.Textbox(
                                    show_label=False,
                                    container=False,
                                    lines=2,
                                    interactive=True,
                                    autoscroll=True,
                                    placeholder="å…·ä½“çš„ãªæ„è¦‹ã‚„æ„Ÿæƒ³ã‚’è‡ªç”±ã«æ›¸ã„ã¦ãã ã•ã„ã€‚",
                                )
                            with gr.Column(scale=1):
                                tab_chat_document_openai_gpt4o_answer_human_eval_feedback_send_button = gr.Button(
                                    value="é€ä¿¡",
                                    variant="primary",
                                )
                    with gr.Accordion(
                            label="LLM è©•ä¾¡çµæœ",
                            visible=False,
                            open=True
                    ) as tab_chat_document_llm_openai_gpt4o_evaluation_accordion:
                        tab_chat_document_openai_gpt4o_evaluation_text = gr.Markdown(
                            show_copy_button=True,
                            height=200,
                            min_height=200,
                            max_height=300
                        )
                with gr.Accordion(
                        label="OpenAI gpt-4 ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
                        visible=False,
                        open=True
                ) as tab_chat_document_llm_openai_gpt4_accordion:
                    tab_chat_document_openai_gpt4_answer_text = gr.Markdown(
                        show_copy_button=True,
                        height=300,
                        min_height=300,
                        max_height=300
                    )
                    with gr.Accordion(
                            label="Human è©•ä¾¡",
                            visible=True,
                            open=True
                    ) as tab_chat_document_llm_openai_gpt4_human_evaluation_accordion:
                        with gr.Row():
                            tab_chat_document_openai_gpt4_answer_human_eval_feedback_radio = gr.Radio(
                                show_label=False,
                                choices=[
                                    ("Good response", "good"),
                                    ("Neutral response", "neutral"),
                                    ("Bad response", "bad"),
                                ],
                                value="good",
                                container=False,
                                interactive=True,
                            )
                        with gr.Row():
                            with gr.Column(scale=11):
                                tab_chat_document_openai_gpt4_answer_human_eval_feedback_text = gr.Textbox(
                                    show_label=False,
                                    container=False,
                                    lines=2,
                                    interactive=True,
                                    autoscroll=True,
                                    placeholder="å…·ä½“çš„ãªæ„è¦‹ã‚„æ„Ÿæƒ³ã‚’è‡ªç”±ã«æ›¸ã„ã¦ãã ã•ã„ã€‚",
                                )
                            with gr.Column(scale=1):
                                tab_chat_document_openai_gpt4_answer_human_eval_feedback_send_button = gr.Button(
                                    value="é€ä¿¡",
                                    variant="primary",
                                )
                    with gr.Accordion(
                            label="LLM è©•ä¾¡çµæœ",
                            visible=False,
                            open=True
                    ) as tab_chat_document_llm_openai_gpt4_evaluation_accordion:
                        tab_chat_document_openai_gpt4_evaluation_text = gr.Markdown(
                            show_copy_button=True,
                            height=200,
                            min_height=200,
                            max_height=300
                        )
                with gr.Accordion(
                        label="Azure OpenAI gpt-4o ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
                        visible=False,
                        open=True
                ) as tab_chat_document_llm_azure_openai_gpt4o_accordion:
                    tab_chat_document_azure_openai_gpt4o_answer_text = gr.Markdown(
                        show_copy_button=True,
                        height=300,
                        min_height=300,
                        max_height=300
                    )
                    with gr.Accordion(
                            label="ç”»åƒå›ç­”",
                            visible=False,
                            open=True
                    ) as tab_chat_document_llm_azure_openai_gpt4o_image_accordion:
                        tab_chat_document_azure_openai_gpt4o_image_answer_text = gr.Markdown(
                            show_copy_button=True,
                            height=600,
                            min_height=600,
                            max_height=600
                        )
                    with gr.Accordion(
                            label="Human è©•ä¾¡",
                            visible=True,
                            open=True
                    ) as tab_chat_document_llm_azure_openai_gpt4o_human_evaluation_accordion:
                        with gr.Row():
                            tab_chat_document_azure_openai_gpt4o_answer_human_eval_feedback_radio = gr.Radio(
                                show_label=False,
                                choices=[
                                    ("Good response", "good"),
                                    ("Neutral response", "neutral"),
                                    ("Bad response", "bad"),
                                ],
                                value="good",
                                container=False,
                                interactive=True,
                            )
                        with gr.Row():
                            with gr.Column(scale=11):
                                tab_chat_document_azure_openai_gpt4o_answer_human_eval_feedback_text = gr.Textbox(
                                    show_label=False,
                                    container=False,
                                    lines=2,
                                    interactive=True,
                                    autoscroll=True,
                                    placeholder="å…·ä½“çš„ãªæ„è¦‹ã‚„æ„Ÿæƒ³ã‚’è‡ªç”±ã«æ›¸ã„ã¦ãã ã•ã„ã€‚",
                                )
                            with gr.Column(scale=1):
                                tab_chat_document_azure_openai_gpt4o_answer_human_eval_feedback_send_button = gr.Button(
                                    value="é€ä¿¡",
                                    variant="primary",
                                )
                    with gr.Accordion(
                            label="LLM è©•ä¾¡çµæœ",
                            visible=False,
                            open=True
                    ) as tab_chat_document_llm_azure_openai_gpt4o_evaluation_accordion:
                        tab_chat_document_azure_openai_gpt4o_evaluation_text = gr.Markdown(
                            show_copy_button=True,
                            height=200,
                            min_height=200,
                            max_height=300
                        )
                with gr.Accordion(
                        label="Azure OpenAI gpt-4 ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
                        visible=False,
                        open=True
                ) as tab_chat_document_llm_azure_openai_gpt4_accordion:
                    tab_chat_document_azure_openai_gpt4_answer_text = gr.Markdown(
                        show_copy_button=True,
                        height=300,
                        min_height=300,
                        max_height=300
                    )
                    with gr.Accordion(
                            label="Human è©•ä¾¡",
                            visible=True,
                            open=True
                    ) as tab_chat_document_llm_azure_openai_gpt4_human_evaluation_accordion:
                        with gr.Row():
                            tab_chat_document_azure_openai_gpt4_answer_human_eval_feedback_radio = gr.Radio(
                                show_label=False,
                                choices=[
                                    ("Good response", "good"),
                                    ("Neutral response", "neutral"),
                                    ("Bad response", "bad"),
                                ],
                                value="good",
                                container=False,
                                interactive=True,
                            )
                        with gr.Row():
                            with gr.Column(scale=11):
                                tab_chat_document_azure_openai_gpt4_answer_human_eval_feedback_text = gr.Textbox(
                                    show_label=False,
                                    container=False,
                                    lines=2,
                                    interactive=True,
                                    autoscroll=True,
                                    placeholder="å…·ä½“çš„ãªæ„è¦‹ã‚„æ„Ÿæƒ³ã‚’è‡ªç”±ã«æ›¸ã„ã¦ãã ã•ã„ã€‚",
                                )
                            with gr.Column(scale=1):
                                tab_chat_document_azure_openai_gpt4_answer_human_eval_feedback_send_button = gr.Button(
                                    value="é€ä¿¡",
                                    variant="primary",
                                )
                    with gr.Accordion(
                            label="LLM è©•ä¾¡çµæœ",
                            visible=False,
                            open=True
                    ) as tab_chat_document_llm_azure_openai_gpt4_evaluation_accordion:
                        tab_chat_document_azure_openai_gpt4_evaluation_text = gr.Markdown(
                            show_copy_button=True,
                            height=200,
                            min_height=200,
                            max_height=300
                        )

            with gr.TabItem(label="Step-5.è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã®å–å¾—") as tab_download_eval_result:
                with gr.Row():
                    tab_download_eval_result_generate_button = gr.Button(
                        value="è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ",
                        variant="primary",
                    )

                    tab_download_eval_result_download_button = gr.DownloadButton(
                        label="è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        variant="primary",
                    )

    gr.Markdown(
        value="### æœ¬ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã¯æ¤œè¨¼è©•ä¾¡ç”¨ã§ã™ã€‚æ—¥å¸¸åˆ©ç”¨ã®ãŸã‚ã®åŸºæœ¬æ©Ÿèƒ½ã¯å‚™ãˆã¦ã„ãªã„ç‚¹ã«ã¤ãã¾ã—ã¦ã”ç†è§£ã‚’ã‚ˆã‚ã—ããŠé¡˜ã„ç”³ã—ä¸Šã’ã¾ã™ã€‚",
        elem_classes="sub_Header")
    gr.Markdown(value="### Developed by Oracle Japan", elem_classes="sub_Header")
    tab_create_oci_clear_button.add(
        [
            tab_create_oci_cred_user_ocid_text,
            tab_create_oci_cred_tenancy_ocid_text,
            tab_create_oci_cred_fingerprint_text,
            tab_create_oci_cred_private_key_file
        ]
    )
    tab_create_oci_cred_button.click(
        create_oci_cred,
        inputs=[
            tab_create_oci_cred_user_ocid_text,
            tab_create_oci_cred_tenancy_ocid_text,
            tab_create_oci_cred_fingerprint_text,
            tab_create_oci_cred_private_key_file,
            tab_create_oci_cred_region_text,
        ],
        outputs=[
            tab_create_oci_cred_sql_accordion,
            tab_create_oci_cred_sql_text
        ]
    )
    tab_create_oci_cred_test_button.click(
        test_oci_cred,
        inputs=[
            tab_create_oci_cred_test_query_text
        ],
        outputs=[
            tab_create_oci_cred_test_vector_text
        ]
    )
    tab_create_cohere_cred_button.click(
        create_cohere_cred,
        inputs=[
            tab_create_cohere_cred_api_key_text
        ],
        outputs=[
            tab_create_cohere_cred_api_key_text
        ]
    )
    tab_create_openai_cred_button.click(
        create_openai_cred,
        inputs=[
            tab_create_openai_cred_base_url_text,
            tab_create_openai_cred_api_key_text
        ],
        outputs=[
            tab_create_openai_cred_base_url_text,
            tab_create_openai_cred_api_key_text
        ]
    )
    tab_create_azure_openai_cred_button.click(
        create_azure_openai_cred,
        inputs=[
            tab_create_azure_openai_cred_api_key_text,
            tab_create_azure_openai_cred_endpoint_gpt_4o_text,
            tab_create_azure_openai_cred_endpoint_gpt_4_text,
        ],
        outputs=[
            tab_create_azure_openai_cred_api_key_text,
            tab_create_azure_openai_cred_endpoint_gpt_4o_text,
            tab_create_azure_openai_cred_endpoint_gpt_4_text,
        ]
    )

    tab_create_langfuse_cred_button.click(
        create_langfuse_cred,
        inputs=[
            tab_create_langfuse_cred_secret_key_text,
            tab_create_langfuse_cred_public_key_text,
            tab_create_langfuse_cred_host_text
        ],
        outputs=[
            tab_create_langfuse_cred_secret_key_text,
            tab_create_langfuse_cred_public_key_text,
            tab_create_langfuse_cred_host_text
        ]
    )
    tab_chat_with_llm_answer_checkbox_group.change(
        set_chat_llm_answer,
        inputs=[
            tab_chat_with_llm_answer_checkbox_group
        ],
        outputs=[
            tab_chat_with_llm_xai_grok_3_accordion,
            tab_chat_with_llm_command_a_accordion,
            tab_chat_with_llm_llama_4_maverick_accordion,
            tab_chat_with_llm_llama_4_scout_accordion,
            tab_chat_with_llm_llama_3_3_70b_accordion,
            tab_chat_with_llm_llama_3_2_90b_vision_accordion,
            tab_chat_with_llm_openai_gpt4o_accordion,
            tab_chat_with_llm_openai_gpt4_accordion,
            tab_chat_with_llm_azure_openai_gpt4o_accordion,
            tab_chat_with_llm_azure_openai_gpt4_accordion
        ]
    )
    tab_chat_with_llm_clear_button.add(
        [
            tab_chat_with_llm_query_image,
            tab_chat_with_llm_query_text,
            tab_chat_with_llm_answer_checkbox_group,
            tab_chat_with_xai_grok_3_answer_text,
            tab_chat_with_command_a_answer_text,
            tab_chat_with_llama_4_maverick_answer_text,
            tab_chat_with_llama_4_scout_answer_text,
            tab_chat_with_llama_3_3_70b_answer_text,
            tab_chat_with_llama_3_2_90b_vision_answer_text,
            tab_chat_with_openai_gpt4o_answer_text,
            tab_chat_with_openai_gpt4_answer_text,
            tab_chat_with_azure_openai_gpt4o_answer_text,
            tab_chat_with_azure_openai_gpt4_answer_text
        ]
    )
    tab_chat_with_llm_chat_button.click(
        chat_stream,
        inputs=[
            tab_chat_with_llm_system_text,
            tab_chat_with_llm_query_image,
            tab_chat_with_llm_query_text,
            tab_chat_with_llm_answer_checkbox_group
        ],
        outputs=[
            tab_chat_with_xai_grok_3_answer_text,
            tab_chat_with_command_a_answer_text,
            tab_chat_with_llama_4_maverick_answer_text,
            tab_chat_with_llama_4_scout_answer_text,
            tab_chat_with_llama_3_3_70b_answer_text,
            tab_chat_with_llama_3_2_90b_vision_answer_text,
            tab_chat_with_openai_gpt4o_answer_text,
            tab_chat_with_openai_gpt4_answer_text,
            tab_chat_with_azure_openai_gpt4o_answer_text,
            tab_chat_with_azure_openai_gpt4_answer_text
        ]
    )

    tab_create_table_button.click(
        create_table,
        inputs=[],
        outputs=[
            tab_create_table_sql_accordion,
            tab_create_table_sql_text
        ]
    )

    tab_convert_document_convert_by_markitdown_button.click(
        convert_to_markdown_document,
        inputs=[
            tab_convert_document_convert_by_markitdown_file_text,
            tab_convert_document_convert_by_markitdown_use_llm_checkbox,
            tab_convert_document_convert_by_markitdown_llm_prompt_text,
        ],
        outputs=[
            tab_convert_document_convert_by_markitdown_file_text,
            tab_load_document_file_text,
        ],
    )

    tab_convert_document_convert_button.click(
        convert_excel_to_text_document,
        inputs=[
            tab_convert_document_convert_excel_to_text_file_text,
        ],
        outputs=[
            tab_convert_document_convert_excel_to_text_file_text,
            tab_load_document_file_text,
        ],
    )

    tab_load_document_load_button.click(
        load_document,
        inputs=[
            tab_load_document_file_text,
            tab_load_document_server_directory_text,
            tab_load_document_metadata_text,
        ],
        outputs=[
            tab_load_document_output_sql_text,
            tab_load_document_doc_id_text,
            tab_load_document_page_count_text,
            tab_load_document_page_content_text
        ],
    )
    tab_split_document.select(
        refresh_doc_list,
        inputs=[],
        outputs=[
            tab_split_document_doc_id_radio,
            tab_delete_document_doc_ids_checkbox_group,
            tab_chat_document_doc_id_checkbox_group
        ]
    ).then(
        reset_document_chunks_result_dataframe,
        inputs=[],
        outputs=[
            tab_split_document_chunks_result_dataframe,
        ]
    ).then(
        reset_document_chunks_result_detail,
        inputs=[],
        outputs=[
            tab_split_document_chunks_result_detail_chunk_id,
            tab_split_document_chunks_result_detail_chunk_data,
        ]
    )

    tab_split_document_split_button.click(
        split_document_by_unstructured,
        inputs=[
            tab_split_document_doc_id_radio,
            tab_split_document_chunks_by_radio,
            tab_split_document_chunks_max_slider,
            tab_split_document_chunks_overlap_slider,
            tab_split_document_chunks_split_by_radio,
            tab_split_document_chunks_split_by_custom_text,
            tab_split_document_chunks_language_radio,
            tab_split_document_chunks_normalize_radio,
            tab_split_document_chunks_normalize_options_checkbox_group
        ],
        outputs=[
            tab_split_document_output_sql_text,
            tab_split_document_chunks_count,
            tab_split_document_chunks_result_dataframe
        ]
    ).then(
        reset_document_chunks_result_detail,
        inputs=[],
        outputs=[
            tab_split_document_chunks_result_detail_chunk_id,
            tab_split_document_chunks_result_detail_chunk_data,
        ]
    )

    tab_split_document_chunks_result_dataframe.select(
        on_select_split_document_chunks_result,
        inputs=[
            tab_split_document_chunks_result_dataframe,
        ],
        outputs=[
            tab_split_document_chunks_result_detail_chunk_id,
            tab_split_document_chunks_result_detail_chunk_data,
        ]
    )

    tab_split_document_chunks_result_detail_update_button.click(
        update_document_chunks_result_detail,
        inputs=[
            tab_split_document_doc_id_radio,
            tab_split_document_chunks_result_dataframe,
            tab_split_document_chunks_result_detail_chunk_id,
            tab_split_document_chunks_result_detail_chunk_data,
        ],
        outputs=[
            tab_split_document_chunks_result_dataframe,
            tab_split_document_chunks_result_detail_chunk_id,
            tab_split_document_chunks_result_detail_chunk_data,
        ]
    )

    # tab_split_document_embed_save_button.click(
    #     split_document_by_unstructured,
    #     inputs=[
    #         tab_split_document_doc_id_radio,
    #         tab_split_document_chunks_by_radio,
    #         tab_split_document_chunks_max_slider,
    #         tab_split_document_chunks_overlap_slider,
    #         tab_split_document_chunks_split_by_radio,
    #         tab_split_document_chunks_split_by_custom_text,
    #         tab_split_document_chunks_language_radio,
    #         tab_split_document_chunks_normalize_radio,
    #         tab_split_document_chunks_normalize_options_checkbox_group
    #     ],
    #     outputs=[
    #         tab_split_document_output_sql_text,
    #         tab_split_document_chunks_count,
    #         tab_split_document_chunks_result_dataframe
    #     ],
    # ).then(
    tab_split_document_embed_save_button.click(
        embed_save_document_by_unstructured,
        inputs=[
            tab_split_document_doc_id_radio,
            tab_split_document_chunks_by_radio,
            tab_split_document_chunks_max_slider,
            tab_split_document_chunks_overlap_slider,
            tab_split_document_chunks_split_by_radio,
            tab_split_document_chunks_split_by_custom_text,
            tab_split_document_chunks_language_radio,
            tab_split_document_chunks_normalize_radio,
            tab_split_document_chunks_normalize_options_checkbox_group
        ],
        outputs=[
            tab_split_document_output_sql_text,
            tab_split_document_chunks_count,
            tab_split_document_chunks_result_dataframe
        ],
    )

    tab_delete_document.select(
        refresh_doc_list,
        inputs=[],
        outputs=[
            tab_split_document_doc_id_radio,
            tab_delete_document_doc_ids_checkbox_group,
            tab_chat_document_doc_id_checkbox_group
        ]
    )
    tab_delete_document_delete_button.click(
        delete_document,
        inputs=[
            tab_delete_document_server_directory_text,
            tab_delete_document_doc_ids_checkbox_group
        ],
        outputs=[
            tab_delete_document_delete_sql,
            tab_split_document_doc_id_radio,
            tab_delete_document_doc_ids_checkbox_group
        ]
    )
    tab_chat_document.select(
        refresh_doc_list,
        inputs=[],
        outputs=[
            tab_split_document_doc_id_radio,
            tab_delete_document_doc_ids_checkbox_group,
            tab_chat_document_doc_id_checkbox_group
        ]
    )
    tab_chat_document_doc_id_all_checkbox.change(
        lambda x: gr.CheckboxGroup(interactive=False, value=[]) if x else
        gr.CheckboxGroup(interactive=True, value=[]),
        tab_chat_document_doc_id_all_checkbox,
        tab_chat_document_doc_id_checkbox_group
    )
    tab_chat_document_llm_answer_checkbox_group.change(
        set_chat_llm_answer,
        inputs=[
            tab_chat_document_llm_answer_checkbox_group
        ],
        outputs=[
            tab_chat_document_llm_xai_grok_3_accordion,
            tab_chat_document_llm_command_a_accordion,
            tab_chat_document_llm_llama_4_maverick_accordion,
            tab_chat_document_llm_llama_4_scout_accordion,
            tab_chat_document_llm_llama_3_3_70b_accordion,
            tab_chat_document_llm_llama_3_2_90b_vision_accordion,
            tab_chat_document_llm_openai_gpt4o_accordion,
            tab_chat_document_llm_openai_gpt4_accordion,
            tab_chat_document_llm_azure_openai_gpt4o_accordion,
            tab_chat_document_llm_azure_openai_gpt4_accordion
        ]
    ).then(
        set_image_answer_visibility,
        inputs=[
            tab_chat_document_llm_answer_checkbox_group,
            tab_chat_document_use_image_checkbox
        ],
        outputs=[
            tab_chat_document_llm_llama_4_maverick_image_accordion,
            tab_chat_document_llm_llama_4_scout_image_accordion,
            tab_chat_document_llm_llama_3_2_90b_vision_image_accordion,
            tab_chat_document_llm_openai_gpt4o_image_accordion,
            tab_chat_document_llm_azure_openai_gpt4o_image_accordion
        ]
    )
    tab_chat_document_llm_evaluation_checkbox.change(
        lambda x: (
            gr.Textbox(visible=True, interactive=True),
            gr.Textbox(visible=True, interactive=True, value="")
        ) if x else (
            gr.Textbox(visible=False, interactive=False),
            gr.Textbox(visible=False, interactive=False, value="")
        ),
        tab_chat_document_llm_evaluation_checkbox,
        [
            tab_chat_document_system_message_text,
            tab_chat_document_standard_answer_text
        ]
    ).then(
        set_chat_llm_evaluation,
        inputs=[
            tab_chat_document_llm_evaluation_checkbox
        ],
        outputs=[
            tab_chat_document_llm_xai_grok_3_evaluation_accordion,
            tab_chat_document_llm_command_a_evaluation_accordion,
            tab_chat_document_llm_llama_4_maverick_evaluation_accordion,
            tab_chat_document_llm_llama_4_scout_evaluation_accordion,
            tab_chat_document_llm_llama_3_3_70b_evaluation_accordion,
            tab_chat_document_llm_llama_3_2_90b_vision_evaluation_accordion,
            tab_chat_document_llm_openai_gpt4o_evaluation_accordion,
            tab_chat_document_llm_openai_gpt4_evaluation_accordion,
            tab_chat_document_llm_azure_openai_gpt4o_evaluation_accordion,
            tab_chat_document_llm_azure_openai_gpt4_evaluation_accordion
        ]
    )

    tab_chat_document_generate_query_radio.change(
        lambda x: (gr.Textbox(value=""),
                   gr.Textbox(value=""),
                   gr.Textbox(value="")),
        inputs=[
            tab_chat_document_generate_query_radio
        ],
        outputs=[
            tab_chat_document_sub_query1_text,
            tab_chat_document_sub_query2_text,
            tab_chat_document_sub_query3_text
        ]
    )

    # ç”»åƒã‚’ä½¿ã£ã¦å›ç­”ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã®å¤‰æ›´ã‚¤ãƒ™ãƒ³ãƒˆ
    tab_chat_document_use_image_checkbox.change(
        lambda x: gr.Accordion(visible=True) if x else gr.Accordion(visible=False),  # ç”»åƒ Prompt è¨­å®šã®è¡¨ç¤º/éè¡¨ç¤ºã®ã¿åˆ¶å¾¡
        inputs=[tab_chat_document_use_image_checkbox],
        outputs=[
            tab_chat_document_image_prompt_accordion
        ]
    ).then(
        set_image_answer_visibility,
        inputs=[
            tab_chat_document_llm_answer_checkbox_group,
            tab_chat_document_use_image_checkbox
        ],
        outputs=[
            tab_chat_document_llm_llama_4_maverick_image_accordion,
            tab_chat_document_llm_llama_4_scout_image_accordion,
            tab_chat_document_llm_llama_3_2_90b_vision_image_accordion,
            tab_chat_document_llm_openai_gpt4o_image_accordion,
            tab_chat_document_llm_azure_openai_gpt4o_image_accordion
        ]
    )

    # tab_chat_document_chat_document_button.click(
    #     check_chat_document_input,
    #     inputs=[
    #         tab_chat_document_llm_answer_checkbox_group,
    #         tab_chat_document_llm_evaluation_checkbox,
    #         tab_chat_document_system_message_text,
    #         tab_chat_document_standard_answer_text,
    #     ],
    #     outputs=[]
    # ).then(
    tab_chat_document_chat_document_button.click(
        lambda: gr.DownloadButton(visible=False),
        outputs=[tab_chat_document_download_output_button]
    ).then(
        reset_all_llm_messages,
        inputs=[],
        outputs=[
            tab_chat_document_xai_grok_3_answer_text,
            tab_chat_document_command_a_answer_text,
            tab_chat_document_llama_4_maverick_answer_text,
            tab_chat_document_llama_4_scout_answer_text,
            tab_chat_document_llama_3_3_70b_answer_text,
            tab_chat_document_llama_3_2_90b_vision_answer_text,
            tab_chat_document_openai_gpt4o_answer_text,
            tab_chat_document_openai_gpt4_answer_text,
            tab_chat_document_azure_openai_gpt4o_answer_text,
            tab_chat_document_azure_openai_gpt4_answer_text,
        ]
    ).then(
        reset_image_answers,
        inputs=[],
        outputs=[
            tab_chat_document_llama_4_maverick_image_answer_text,
            tab_chat_document_llama_4_scout_image_answer_text,
            tab_chat_document_llama_3_2_90b_vision_image_answer_text,
            tab_chat_document_openai_gpt4o_image_answer_text,
            tab_chat_document_azure_openai_gpt4o_image_answer_text
        ]
    ).then(
        reset_llm_evaluations,
        inputs=[],
        outputs=[
            tab_chat_document_xai_grok_3_evaluation_text,
            tab_chat_document_command_a_evaluation_text,
            tab_chat_document_llama_4_maverick_evaluation_text,
            tab_chat_document_llama_4_scout_evaluation_text,
            tab_chat_document_llama_3_3_70b_evaluation_text,
            tab_chat_document_llama_3_2_90b_vision_evaluation_text,
            tab_chat_document_openai_gpt4o_evaluation_text,
            tab_chat_document_openai_gpt4_evaluation_text,
            tab_chat_document_azure_openai_gpt4o_evaluation_text,
            tab_chat_document_azure_openai_gpt4_evaluation_text
        ]
    ).then(
        reset_eval_by_human_result,
        inputs=[],
        outputs=[
            tab_chat_document_command_a_answer_human_eval_feedback_radio,
            tab_chat_document_command_a_answer_human_eval_feedback_text,
            tab_chat_document_llama_4_maverick_answer_human_eval_feedback_radio,
            tab_chat_document_llama_4_maverick_answer_human_eval_feedback_text,
            tab_chat_document_llama_4_scout_answer_human_eval_feedback_radio,
            tab_chat_document_llama_4_scout_answer_human_eval_feedback_text,
            tab_chat_document_llama_3_3_70b_answer_human_eval_feedback_radio,
            tab_chat_document_llama_3_3_70b_answer_human_eval_feedback_text,
            tab_chat_document_llama_3_2_90b_vision_answer_human_eval_feedback_radio,
            tab_chat_document_llama_3_2_90b_vision_answer_human_eval_feedback_text,
            tab_chat_document_openai_gpt4o_answer_human_eval_feedback_radio,
            tab_chat_document_openai_gpt4o_answer_human_eval_feedback_text,
            tab_chat_document_openai_gpt4_answer_human_eval_feedback_radio,
            tab_chat_document_openai_gpt4_answer_human_eval_feedback_text,
            tab_chat_document_azure_openai_gpt4o_answer_human_eval_feedback_radio,
            tab_chat_document_azure_openai_gpt4o_answer_human_eval_feedback_text,
            tab_chat_document_azure_openai_gpt4_answer_human_eval_feedback_radio,
            tab_chat_document_azure_openai_gpt4_answer_human_eval_feedback_text,
        ]
    ).then(
        generate_query,
        inputs=[
            tab_chat_document_query_text,
            tab_chat_document_generate_query_radio
        ],
        outputs=[
            tab_chat_document_sub_query1_text,
            tab_chat_document_sub_query2_text,
            tab_chat_document_sub_query3_text,
        ]
    ).then(
        search_document,
        inputs=[
            tab_chat_document_reranker_model_radio,
            tab_chat_document_reranker_top_k_slider,
            tab_chat_document_reranker_threshold_slider,
            tab_chat_document_threshold_value_slider,
            tab_chat_document_top_k_slider,
            tab_chat_document_doc_id_all_checkbox,
            tab_chat_document_doc_id_checkbox_group,
            tab_chat_document_text_search_checkbox,
            tab_chat_document_text_search_k_slider,
            tab_chat_document_document_metadata_text,
            tab_chat_document_query_text,
            tab_chat_document_sub_query1_text,
            tab_chat_document_sub_query2_text,
            tab_chat_document_sub_query3_text,
            tab_chat_document_partition_by_k_slider,
            tab_chat_document_answer_by_one_checkbox,
            tab_chat_document_extend_first_chunk_size,
            tab_chat_document_extend_around_chunk_size,
            tab_chat_document_use_image_checkbox
        ],
        outputs=[
            tab_chat_document_output_sql_text,
            tab_chat_document_searched_data_summary_text,
            tab_chat_document_searched_result_dataframe
        ]
    ).then(
        chat_document,
        inputs=[
            tab_chat_document_searched_result_dataframe,
            tab_chat_document_llm_answer_checkbox_group,
            tab_chat_document_include_citation_checkbox,
            tab_chat_document_include_current_time_checkbox,
            tab_chat_document_use_image_checkbox,
            tab_chat_document_query_text,
            tab_chat_document_doc_id_all_checkbox,
            tab_chat_document_doc_id_checkbox_group,
            tab_chat_document_rag_prompt_text,
        ],
        outputs=[
            tab_chat_document_xai_grok_3_answer_text,
            tab_chat_document_command_a_answer_text,
            tab_chat_document_llama_4_maverick_answer_text,
            tab_chat_document_llama_4_scout_answer_text,
            tab_chat_document_llama_3_3_70b_answer_text,
            tab_chat_document_llama_3_2_90b_vision_answer_text,
            tab_chat_document_openai_gpt4o_answer_text,
            tab_chat_document_openai_gpt4_answer_text,
            tab_chat_document_azure_openai_gpt4o_answer_text,
            tab_chat_document_azure_openai_gpt4_answer_text,
        ]
    ).then(
        append_citation,
        inputs=[
            tab_chat_document_searched_result_dataframe,
            tab_chat_document_llm_answer_checkbox_group,
            tab_chat_document_include_citation_checkbox,
            tab_chat_document_query_text,
            tab_chat_document_doc_id_all_checkbox,
            tab_chat_document_doc_id_checkbox_group,
            tab_chat_document_xai_grok_3_answer_text,
            tab_chat_document_command_a_answer_text,
            tab_chat_document_llama_4_maverick_answer_text,
            tab_chat_document_llama_4_scout_answer_text,
            tab_chat_document_llama_3_3_70b_answer_text,
            tab_chat_document_llama_3_2_90b_vision_answer_text,
            tab_chat_document_openai_gpt4o_answer_text,
            tab_chat_document_openai_gpt4_answer_text,
            tab_chat_document_azure_openai_gpt4o_answer_text,
            tab_chat_document_azure_openai_gpt4_answer_text,
        ],
        outputs=[
            tab_chat_document_xai_grok_3_answer_text,
            tab_chat_document_command_a_answer_text,
            tab_chat_document_llama_4_maverick_answer_text,
            tab_chat_document_llama_4_scout_answer_text,
            tab_chat_document_llama_3_3_70b_answer_text,
            tab_chat_document_llama_3_2_90b_vision_answer_text,
            tab_chat_document_openai_gpt4o_answer_text,
            tab_chat_document_openai_gpt4_answer_text,
            tab_chat_document_azure_openai_gpt4o_answer_text,
            tab_chat_document_azure_openai_gpt4_answer_text,
        ]
    ).then(
        process_image_answers_streaming,
        inputs=[
            tab_chat_document_searched_result_dataframe,
            tab_chat_document_use_image_checkbox,
            tab_chat_document_single_image_processing_checkbox,
            tab_chat_document_llm_answer_checkbox_group,
            tab_chat_document_query_text,
            tab_chat_document_llama_4_maverick_image_answer_text,
            tab_chat_document_llama_4_scout_image_answer_text,
            tab_chat_document_llama_3_2_90b_vision_image_answer_text,
            tab_chat_document_openai_gpt4o_image_answer_text,
            tab_chat_document_azure_openai_gpt4o_image_answer_text,
            tab_chat_document_image_prompt_text
        ],
        outputs=[
            tab_chat_document_llama_4_maverick_image_answer_text,
            tab_chat_document_llama_4_scout_image_answer_text,
            tab_chat_document_llama_3_2_90b_vision_image_answer_text,
            tab_chat_document_openai_gpt4o_image_answer_text,
            tab_chat_document_azure_openai_gpt4o_image_answer_text
        ]
    ).then(
        eval_by_ragas,
        inputs=[
            tab_chat_document_query_text,
            tab_chat_document_doc_id_all_checkbox,
            tab_chat_document_doc_id_checkbox_group,
            tab_chat_document_searched_result_dataframe,
            tab_chat_document_llm_answer_checkbox_group,
            tab_chat_document_llm_evaluation_checkbox,
            tab_chat_document_system_message_text,
            tab_chat_document_standard_answer_text,
            tab_chat_document_xai_grok_3_answer_text,
            tab_chat_document_command_a_answer_text,
            tab_chat_document_llama_4_maverick_answer_text,
            tab_chat_document_llama_4_scout_answer_text,
            tab_chat_document_llama_3_3_70b_answer_text,
            tab_chat_document_llama_3_2_90b_vision_answer_text,
            tab_chat_document_openai_gpt4o_answer_text,
            tab_chat_document_openai_gpt4_answer_text,
            tab_chat_document_azure_openai_gpt4o_answer_text,
            tab_chat_document_azure_openai_gpt4_answer_text,
        ],
        outputs=[
            tab_chat_document_xai_grok_3_evaluation_text,
            tab_chat_document_command_a_evaluation_text,
            tab_chat_document_llama_4_maverick_evaluation_text,
            tab_chat_document_llama_4_scout_evaluation_text,
            tab_chat_document_llama_3_3_70b_evaluation_text,
            tab_chat_document_llama_3_2_90b_vision_evaluation_text,
            tab_chat_document_openai_gpt4o_evaluation_text,
            tab_chat_document_openai_gpt4_evaluation_text,
            tab_chat_document_azure_openai_gpt4o_evaluation_text,
            tab_chat_document_azure_openai_gpt4_evaluation_text,
        ]
    ).then(
        generate_download_file,
        inputs=[
            tab_chat_document_searched_result_dataframe,
            tab_chat_document_llm_answer_checkbox_group,
            tab_chat_document_include_citation_checkbox,
            tab_chat_document_llm_evaluation_checkbox,
            tab_chat_document_query_text,
            tab_chat_document_doc_id_all_checkbox,
            tab_chat_document_doc_id_checkbox_group,
            tab_chat_document_standard_answer_text,
            tab_chat_document_xai_grok_3_answer_text,
            tab_chat_document_command_a_answer_text,
            tab_chat_document_llama_4_maverick_answer_text,
            tab_chat_document_llama_4_scout_answer_text,
            tab_chat_document_llama_3_3_70b_answer_text,
            tab_chat_document_llama_3_2_90b_vision_answer_text,
            tab_chat_document_openai_gpt4o_answer_text,
            tab_chat_document_openai_gpt4_answer_text,
            tab_chat_document_azure_openai_gpt4o_answer_text,
            tab_chat_document_azure_openai_gpt4_answer_text,
            tab_chat_document_xai_grok_3_evaluation_text,
            tab_chat_document_command_a_evaluation_text,
            tab_chat_document_llama_4_maverick_evaluation_text,
            tab_chat_document_llama_4_scout_evaluation_text,
            tab_chat_document_llama_3_3_70b_evaluation_text,
            tab_chat_document_llama_3_2_90b_vision_evaluation_text,
            tab_chat_document_openai_gpt4o_evaluation_text,
            tab_chat_document_openai_gpt4_evaluation_text,
            tab_chat_document_azure_openai_gpt4o_evaluation_text,
            tab_chat_document_azure_openai_gpt4_evaluation_text,
        ],
        outputs=[
            tab_chat_document_download_output_button
        ]
    ).then(
        set_query_id_state,
        inputs=[],
        outputs=[
            query_id_state,
        ]
    ).then(
        insert_query_result,
        inputs=[
            tab_chat_document_searched_result_dataframe,
            query_id_state,
            tab_chat_document_query_text,
            tab_chat_document_doc_id_all_checkbox,
            tab_chat_document_doc_id_checkbox_group,
            tab_chat_document_output_sql_text,
            tab_chat_document_llm_answer_checkbox_group,
            tab_chat_document_llm_evaluation_checkbox,
            tab_chat_document_standard_answer_text,
            tab_chat_document_xai_grok_3_answer_text,
            tab_chat_document_command_a_answer_text,
            tab_chat_document_llama_4_maverick_answer_text,
            tab_chat_document_llama_4_scout_answer_text,
            tab_chat_document_llama_3_3_70b_answer_text,
            tab_chat_document_llama_3_2_90b_vision_answer_text,
            tab_chat_document_openai_gpt4o_answer_text,
            tab_chat_document_openai_gpt4_answer_text,
            tab_chat_document_azure_openai_gpt4o_answer_text,
            tab_chat_document_azure_openai_gpt4_answer_text,
            tab_chat_document_xai_grok_3_evaluation_text,
            tab_chat_document_command_a_evaluation_text,
            tab_chat_document_llama_4_maverick_evaluation_text,
            tab_chat_document_llama_4_scout_evaluation_text,
            tab_chat_document_llama_3_3_70b_evaluation_text,
            tab_chat_document_llama_3_2_90b_vision_evaluation_text,
            tab_chat_document_openai_gpt4o_evaluation_text,
            tab_chat_document_openai_gpt4_evaluation_text,
            tab_chat_document_azure_openai_gpt4o_evaluation_text,
            tab_chat_document_azure_openai_gpt4_evaluation_text,
        ],
        outputs=[]
    )

    tab_chat_document_xai_grok_3_answer_human_eval_feedback_send_button.click(
        eval_by_human,
        inputs=[
            query_id_state,
            gr.State(value="xai/grok-3"),
            tab_chat_document_xai_grok_3_answer_human_eval_feedback_radio,
            tab_chat_document_xai_grok_3_answer_human_eval_feedback_text,
        ],
        outputs=[
            tab_chat_document_xai_grok_3_answer_human_eval_feedback_radio,
            tab_chat_document_xai_grok_3_answer_human_eval_feedback_text,
        ]
    )

    tab_chat_document_command_a_answer_human_eval_feedback_send_button.click(
        eval_by_human,
        inputs=[
            query_id_state,
            gr.State(value="cohere/command-a"),
            tab_chat_document_command_a_answer_human_eval_feedback_radio,
            tab_chat_document_command_a_answer_human_eval_feedback_text,
        ],
        outputs=[
            tab_chat_document_command_a_answer_human_eval_feedback_radio,
            tab_chat_document_command_a_answer_human_eval_feedback_text,
        ]
    )


    tab_chat_document_llama_4_maverick_answer_human_eval_feedback_send_button.click(
        eval_by_human,
        inputs=[
            query_id_state,
            gr.State(value="meta/llama-4-maverick-17b-128e-instruct-fp8"),
            tab_chat_document_llama_4_maverick_answer_human_eval_feedback_radio,
            tab_chat_document_llama_4_maverick_answer_human_eval_feedback_text,
        ],
        outputs=[
            tab_chat_document_llama_4_maverick_answer_human_eval_feedback_radio,
            tab_chat_document_llama_4_maverick_answer_human_eval_feedback_text,
        ]
    )

    tab_chat_document_llama_4_scout_answer_human_eval_feedback_send_button.click(
        eval_by_human,
        inputs=[
            query_id_state,
            gr.State(value="meta/llama-4-scout-17b-16e-instruct"),
            tab_chat_document_llama_4_scout_answer_human_eval_feedback_radio,
            tab_chat_document_llama_4_scout_answer_human_eval_feedback_text,
        ],
        outputs=[
            tab_chat_document_llama_4_scout_answer_human_eval_feedback_radio,
            tab_chat_document_llama_4_scout_answer_human_eval_feedback_text,
        ]
    )

    tab_chat_document_llama_3_3_70b_answer_human_eval_feedback_send_button.click(
        eval_by_human,
        inputs=[
            query_id_state,
            gr.State(value="meta/llama-3-3-70b"),
            tab_chat_document_llama_3_3_70b_answer_human_eval_feedback_radio,
            tab_chat_document_llama_3_3_70b_answer_human_eval_feedback_text,
        ],
        outputs=[
            tab_chat_document_llama_3_3_70b_answer_human_eval_feedback_radio,
            tab_chat_document_llama_3_3_70b_answer_human_eval_feedback_text,
        ]
    )

    tab_chat_document_llama_3_2_90b_vision_answer_human_eval_feedback_send_button.click(
        eval_by_human,
        inputs=[
            query_id_state,
            gr.State(value="meta/llama-3-2-90b-vision"),
            tab_chat_document_llama_3_2_90b_vision_answer_human_eval_feedback_radio,
            tab_chat_document_llama_3_2_90b_vision_answer_human_eval_feedback_text,
        ],
        outputs=[
            tab_chat_document_llama_3_2_90b_vision_answer_human_eval_feedback_radio,
            tab_chat_document_llama_3_2_90b_vision_answer_human_eval_feedback_text,
        ]
    )

    tab_chat_document_openai_gpt4o_answer_human_eval_feedback_send_button.click(
        eval_by_human,
        inputs=[
            query_id_state,
            gr.State(value="openai/gpt-4o"),
            tab_chat_document_openai_gpt4o_answer_human_eval_feedback_radio,
            tab_chat_document_openai_gpt4o_answer_human_eval_feedback_text,
        ],
        outputs=[
            tab_chat_document_openai_gpt4o_answer_human_eval_feedback_radio,
            tab_chat_document_openai_gpt4o_answer_human_eval_feedback_text,
        ]
    )

    tab_chat_document_openai_gpt4_answer_human_eval_feedback_send_button.click(
        eval_by_human,
        inputs=[
            query_id_state,
            gr.State(value="openai/gpt-4"),
            tab_chat_document_openai_gpt4_answer_human_eval_feedback_radio,
            tab_chat_document_openai_gpt4_answer_human_eval_feedback_text,
        ],
        outputs=[
            tab_chat_document_openai_gpt4_answer_human_eval_feedback_radio,
            tab_chat_document_openai_gpt4_answer_human_eval_feedback_text,
        ]
    )

    tab_chat_document_azure_openai_gpt4o_answer_human_eval_feedback_send_button.click(
        eval_by_human,
        inputs=[
            query_id_state,
            gr.State(value="azure_openai/gpt-4o"),
            tab_chat_document_azure_openai_gpt4o_answer_human_eval_feedback_radio,
            tab_chat_document_azure_openai_gpt4o_answer_human_eval_feedback_text,
        ],
        outputs=[
            tab_chat_document_azure_openai_gpt4o_answer_human_eval_feedback_radio,
            tab_chat_document_azure_openai_gpt4o_answer_human_eval_feedback_text,
        ]
    )

    tab_chat_document_azure_openai_gpt4_answer_human_eval_feedback_send_button.click(
        eval_by_human,
        inputs=[
            query_id_state,
            gr.State(value="azure_openai/gpt-4"),
            tab_chat_document_azure_openai_gpt4_answer_human_eval_feedback_radio,
            tab_chat_document_azure_openai_gpt4_answer_human_eval_feedback_text,
        ],
        outputs=[
            tab_chat_document_azure_openai_gpt4_answer_human_eval_feedback_radio,
            tab_chat_document_azure_openai_gpt4_answer_human_eval_feedback_text,
        ]
    )

    tab_download_eval_result_generate_button.click(
        generate_eval_result_file,
        inputs=[],
        outputs=[
            tab_download_eval_result_download_button,
        ]
    )


    # RAG Prompt è¨­å®šã®ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼
    def save_rag_prompt(prompt_text):
        """RAG promptã‚’ä¿å­˜ã™ã‚‹"""
        try:
            update_langgpt_rag_prompt(prompt_text)
            return gr.Info("PromptãŒä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")
        except Exception as e:
            return gr.Warning(f"Promptã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")


    def reset_rag_prompt():
        """RAG promptã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«æˆ»ã™"""
        default_prompt = get_langgpt_rag_prompt("{{context}}", "{{query_text}}", False, False)
        update_langgpt_rag_prompt(default_prompt)
        return default_prompt


    tab_chat_document_rag_prompt_save_button.click(
        save_rag_prompt,
        inputs=[tab_chat_document_rag_prompt_text],
        outputs=[]
    )

    tab_chat_document_rag_prompt_reset_button.click(
        reset_rag_prompt,
        inputs=[],
        outputs=[tab_chat_document_rag_prompt_text]
    )


    # ç”»åƒ Prompt è¨­å®šã®ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼
    def save_image_prompt(prompt_text):
        """ç”»åƒ promptã‚’ä¿å­˜ã™ã‚‹"""
        try:
            update_image_qa_prompt(prompt_text)
            return gr.Info("ç”»åƒ PromptãŒä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")
        except Exception as e:
            return gr.Warning(f"ç”»åƒ Promptã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")


    def reset_image_prompt():
        """ç”»åƒ promptã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«æˆ»ã™"""
        default_prompt = get_image_qa_prompt("{{query_text}}")
        update_image_qa_prompt(default_prompt)
        return default_prompt


    # tab_chat_document_image_prompt_save_button.click(
    #     save_image_prompt,
    #     inputs=[tab_chat_document_image_prompt_text],
    #     outputs=[]
    # )

    tab_chat_document_image_prompt_reset_button.click(
        reset_image_prompt,
        inputs=[],
        outputs=[tab_chat_document_image_prompt_text]
    )

app.queue()
if __name__ == "__main__":
    # ãƒªã‚½ãƒ¼ã‚¹è­¦å‘Šè¿½è·¡ã‚’æœ‰åŠ¹åŒ–
    enable_resource_warnings()

    parser = argparse.ArgumentParser()
    parser.add_argument("--host", type=str, default="127.0.0.1")
    parser.add_argument("--port", type=int, default=8080)
    args = parser.parse_args()
    app.launch(
        server_name=args.host,
        server_port=args.port,
        max_threads=200,
        show_api=False,
        # auth=do_auth,
    )
